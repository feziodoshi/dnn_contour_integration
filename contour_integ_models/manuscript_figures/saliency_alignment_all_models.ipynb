{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "\n",
    "\n",
    "# Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Pytorch related\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils import data as dt\n",
    "from torchinfo import summary\n",
    "import torchvision.models as pretrained_models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils import model_zoo\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "# Numpy, Matplotlib, Pandas, Sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from sklearn import manifold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "# python utilities\n",
    "from itertools import combinations\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import copy\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import networkx as nx\n",
    "import os\n",
    "from IPython.display import Image\n",
    "from IPython.core.debugger import set_trace\n",
    "import collections\n",
    "from functools import partial\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "\n",
    "from PIL import Image, ImageStat\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "# Extra imports\n",
    "from lib.feature_extractor import FeatureExtractor\n",
    "from lib.custom_dataset import Contour_Dataset\n",
    "from lib.build_fe_ft_models import *\n",
    "from lib.misc_functions import *\n",
    "from lib.field_stim_functions import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "img_dim=512\n",
    "\n",
    "batch_size=32\n",
    "num_workers=8\n",
    "device = torch.device('cuda:'+'3')\n",
    "# device = 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all the values from the config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Visual Diet config \n",
      "\n",
      "{'root_directory': '/home/jovyan/work/Datasets/contour_integration/model-training/config_0/', 'get_B': [0, 15, 30, 45, 60, 75], 'get_D': [32], 'get_A': [0], 'get_numElements': [12]}\n",
      "\n",
      " Saved Model config \n",
      "\n",
      "{'saved_model_directory_or_frozen_broad': '../../model_weights/contour_model_weights/alexnet_regimagenet_categ_frozen_broad/', 'saved_model_directory_or_finetune_broad': '../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/', 'saved_model_directory_random_frozen_broad': '../../model_weights/contour_model_weights/alexnet-random-nodata-notask_frozen_broad/'}\n"
     ]
    }
   ],
   "source": [
    "from visualdiet_savedmodel_config import *\n",
    "\n",
    "print('\\n Visual Diet config \\n')\n",
    "print(visual_diet_config)\n",
    "\n",
    "\n",
    "print('\\n Saved Model config \\n')\n",
    "print(saved_model_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory=visual_diet_config['root_directory']\n",
    "get_B=visual_diet_config['get_B']\n",
    "get_D=visual_diet_config['get_D']\n",
    "get_A=visual_diet_config['get_A']\n",
    "get_numElements=visual_diet_config['get_numElements']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_file_paths = glob.glob(os.path.join(saved_directory,'*.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize images using parameters from the training image set\n",
    "data_transform = transforms.Compose([       \n",
    " transforms.Resize(img_dim),                   \n",
    " transforms.CenterCrop((img_dim,img_dim)),         \n",
    " transforms.ToTensor(),                    \n",
    " transforms.Normalize(                      \n",
    " mean=[0.485, 0.456, 0.406],                \n",
    " std=[0.229, 0.224, 0.225]                  \n",
    " )])\n",
    "\n",
    "data_transform_without_norm = transforms.Compose([       \n",
    " transforms.Resize(img_dim),                   \n",
    " transforms.CenterCrop((img_dim,img_dim)),         \n",
    " transforms.ToTensor()                    \n",
    " ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diff_means(arr1, arr2):\n",
    "    mean1 = np.mean(arr1)\n",
    "    mean2 = np.mean(arr2)\n",
    "    \n",
    "    return (mean1 - mean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dprime_means(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Compute d' based on the difference of means divided by the pooled standard deviation.\n",
    "    \"\"\"\n",
    "    mean1 = np.mean(arr1)\n",
    "    mean2 = np.mean(arr2)\n",
    "    \n",
    "    var1 = np.var(arr1, ddof=1)  # ddof=1 for sample variance\n",
    "    var2 = np.var(arr2, ddof=1)  # ddof=1 for sample variance\n",
    "    \n",
    "    # Calculate the pooled standard deviation\n",
    "    pooled_std = np.sqrt((var1 + var2) / 2)\n",
    "    \n",
    "    # Compute d'\n",
    "    d_prime = (mean1 - mean2) / pooled_std\n",
    "    \n",
    "    return d_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aprime_means(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Compute A' as the area under the ROC curve based on two input arrays.\n",
    "    \"\"\"\n",
    "    # Combine the arrays and create corresponding labels\n",
    "    combined = np.concatenate((arr1, arr2))\n",
    "    labels = np.concatenate((np.ones(len(arr1)), np.zeros(len(arr2))))\n",
    "    \n",
    "    # Compute AUC\n",
    "    a_prime = roc_auc_score(labels, combined)\n",
    "    \n",
    "    return a_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69286fe19e8f419cacd7ec0f89c917aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=15, description='beta_val', max=91), IntSlider(value=4, description='imgâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(beta_val=(0,91,1),img_num=(0,20,1))\n",
    "def get_data(beta_val=15,img_num=4):\n",
    "    \n",
    "    \n",
    "    ### Show the image\n",
    "    val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=[beta_val],get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "    val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    val_dataset_without_norm = Contour_Dataset(root=root_directory,transform=data_transform_without_norm,train=False,get_B=[beta_val],get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "    val_loader_without_norm = torch.utils.data.DataLoader(dataset=val_dataset_without_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    a, b, d, alpha, nel, labels, record =next(iter(val_loader_norm))\n",
    "    prep_img=torch.unsqueeze(a[img_num],0)\n",
    "    print('Beta Val: ',b[img_num])\n",
    "    print('Contour Present: ',labels[img_num])\n",
    "    prep_img=prep_img.to(device)\n",
    "    prep_img=Variable(prep_img,requires_grad=True)\n",
    "    ## Plot the orginal image\n",
    "    original_img=np.transpose(prep_img[0].detach().cpu().numpy(),(1,2,0))\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(original_img)\n",
    "    \n",
    "    ## Plot the orginal unnormalized image\n",
    "    plt.figure(figsize=(10,8))\n",
    "    a_without_norm,b, d, alpha, nel, labels, record=next(iter(val_loader_without_norm))\n",
    "    plt.imshow(np.transpose(torch.unsqueeze(a_without_norm[img_num],0)[0].numpy(),(1,2,0)))\n",
    "    \n",
    "    \n",
    "    ## Plot the image with contour\n",
    "    print(record[img_num])\n",
    "    image_recorder_dict=torch.load(record[img_num])\n",
    "    img_contour, img_control, img_contour_background, img_control_background=image_renderer(image_recorder_dict)\n",
    "    display(blend(show_path(image_recorder_dict['path_points'], image_recorder_dict['path_centers'], imHeight=image_recorder_dict['image_height'], imWidth=image_recorder_dict['image_width']),img_contour_background,0.7))\n",
    "    \n",
    "    display(img_contour_background)\n",
    "    display(img_contour)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Analysis (Saliency) over 1 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @interact(beta_val=(0,91,1),img_num=(0,batch_size,1))\n",
    "# def get_data(beta_val=15,img_num=4):\n",
    "    \n",
    "    \n",
    "#     ### Show the image\n",
    "#     val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=[beta_val],get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "#     val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "#     val_dataset_without_norm = Contour_Dataset(root=root_directory,transform=data_transform_without_norm,train=False,get_B=[beta_val],get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "#     val_loader_without_norm = torch.utils.data.DataLoader(dataset=val_dataset_without_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#     a, b, d, alpha, nel, labels, record =next(iter(val_loader_norm))\n",
    "#     prep_img=torch.unsqueeze(a[img_num],0)\n",
    "#     print('Beta Val: ',b[img_num])\n",
    "#     print('Contour Present: ',labels[img_num])\n",
    "#     prep_img=prep_img.to(device)\n",
    "#     prep_img=Variable(prep_img,requires_grad=True)\n",
    "    \n",
    "#     ## Plot the orginal image - this is the normalized one, the one that will actually be used\n",
    "#     # original_img=np.transpose(prep_img[0].detach().cpu().numpy(),(1,2,0))\n",
    "#     # plt.figure(figsize=(10,8))\n",
    "#     # plt.imshow(original_img)\n",
    "#     # plt.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "#     ## Plot the orginal unnormalized image\n",
    "#     plt.figure(figsize=(10,8))\n",
    "#     a_without_norm,b, d, alpha, nel, labels, record=next(iter(val_loader_without_norm))\n",
    "#     plt.imshow(np.transpose(torch.unsqueeze(a_without_norm[img_num],0)[0].numpy(),(1,2,0)))\n",
    "#     plt.axis('off')\n",
    "#     # plt.savefig('./dev/ccn_figures/poster/original_image.png', bbox_inches='tight',dpi=300) \n",
    "#     # plt.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     @interact(file=model_file_paths)\n",
    "#     def vis_image(file,show_vis=False):\n",
    "        \n",
    "\n",
    "#         if(show_vis):    \n",
    "#             checkpoint=torch.load(file)\n",
    "            \n",
    "#             ####################################################################################################\n",
    "#             ## Showing the Model Stats\n",
    "#             print('BASE MODEL IS',checkpoint['training_config']['base_model_name'])\n",
    "#             print('Final Val Accuracy is',checkpoint['metrics']['val_acc'][-1])\n",
    "\n",
    "#             plt.figure(figsize=(12,4))\n",
    "\n",
    "\n",
    "#             plt.subplot(1,2,1)\n",
    "#             plt.plot(np.arange(len(checkpoint['metrics']['train_acc'])),checkpoint['metrics']['train_acc'],label='train')\n",
    "#             plt.plot(np.arange(len(checkpoint['metrics']['val_acc'])),checkpoint['metrics']['val_acc'],label='val')\n",
    "#             plt.axhline(y=0.5,linestyle='--',color='r',label='chance')\n",
    "\n",
    "#             plt.xlabel('Epochs',fontsize=15,labelpad=15)\n",
    "#             plt.ylabel('Accuracy',fontsize=15,labelpad=15)\n",
    "\n",
    "#             plt.gca().spines['top'].set_visible(False)\n",
    "#             plt.gca().spines['right'].set_visible(False)\n",
    "#             plt.yticks(fontsize=15)\n",
    "#             plt.xticks(fontsize=15)   \n",
    "#             plt.ylim(0.4,1.01)\n",
    "#             plt.subplot(1,2,2)\n",
    "#             plt.plot(np.arange(len(checkpoint['metrics']['train_loss'])),checkpoint['metrics']['train_loss'],label='train')\n",
    "#             plt.plot(np.arange(len(checkpoint['metrics']['val_loss'])),checkpoint['metrics']['val_loss'],label='val')\n",
    "\n",
    "#             plt.xlabel('Epochs',fontsize=15,labelpad=15)\n",
    "#             plt.ylabel('Loss',fontsize=15,labelpad=15)\n",
    "\n",
    "#             plt.gca().spines['top'].set_visible(False)\n",
    "#             plt.gca().spines['right'].set_visible(False)\n",
    "#             plt.yticks(fontsize=15)\n",
    "#             plt.xticks(fontsize=15)   \n",
    "#             plt.legend()\n",
    "#             plt.show()\n",
    "#             ####################################################################################################\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "#             ####################################################################################################\n",
    "#             ## Loading the model and setting up the Guided Spliced Model for Attention Maps\n",
    "#             loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "#             loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "#             GBP = Spliced_GuidedBackprop(loaded_spliced_model,device)\n",
    "#             ####################################################################################################\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             ####################################################################################################\n",
    "#             ## Getting the Saliency Maps\n",
    "#             # Get gradients\n",
    "#             guided_grads = GBP.generate_gradients(prep_img, 1)\n",
    "\n",
    "            \n",
    "#             # Gradients\n",
    "#             guided_grads_vis = guided_grads - guided_grads.min()\n",
    "#             guided_grads_vis /= guided_grads_vis.max()\n",
    "\n",
    "#             plt.figure(figsize=(10,8))\n",
    "#             plt.imshow(np.transpose(guided_grads_vis,(1,2,0)))\n",
    "#             plt.axis('off')\n",
    "#             plt.show()\n",
    "\n",
    "\n",
    "#             # Grayscale Gradients\n",
    "#             grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
    "\n",
    "\n",
    "#             grayscale_guided_grads_vis = grayscale_guided_grads - grayscale_guided_grads.min()\n",
    "#             grayscale_guided_grads_vis /= grayscale_guided_grads_vis.max()\n",
    "\n",
    "#             plt.figure(figsize=(10,8))\n",
    "#             plt.imshow(np.squeeze(np.transpose(grayscale_guided_grads_vis,(1,2,0))),cmap='gray')\n",
    "#             plt.axis('off')\n",
    "#             plt.show()\n",
    "            \n",
    "#             # plt.savefig('./dev/ccn_figures/poster/saliency_image.png',bbox_inches='tight', dpi=300) \n",
    "#             # plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             ## Compute saliency maps\n",
    "#             print(grayscale_guided_grads_vis.shape)\n",
    "#             viz_image=grayscale_guided_grads_vis[0]\n",
    "#             ####################################################################################################\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             ####################################################################################################\n",
    "#             ## Compute saliency scores\n",
    "#             mask_img_path_fg, mask_img_path_bg, list_gauss=mask_renderer(torch.load(record[img_num]),140)\n",
    "#             mask_img_path_fg=np.array(mask_img_path_fg)\n",
    "#             mask_img_path_bg=np.array(mask_img_path_bg)\n",
    "            \n",
    "#             plt.figure(figsize=(20,8))\n",
    "#             plt.subplot(1,2,1)\n",
    "#             plt.imshow(mask_img_path_fg,cmap='gray')\n",
    "            \n",
    "#             plt.subplot(1,2,2)\n",
    "#             plt.imshow(mask_img_path_bg,cmap='gray')\n",
    "#             plt.axis('off')\n",
    "#             plt.show()\n",
    "            \n",
    "            \n",
    "#             ## Plotting Gradients for contour and background elements\n",
    "#             sns.distplot(viz_image[np.where(mask_img_path_fg==255)],label='distribution of fg')\n",
    "#             sns.distplot(viz_image[np.where(mask_img_path_bg==255)],label='distribution of bg')\n",
    "#             plt.title('Distribution of gradients')\n",
    "#             plt.legend()\n",
    "#             plt.gca().spines['top'].set_visible(False)\n",
    "#             plt.gca().spines['right'].set_visible(False)\n",
    "#             plt.show()\n",
    "    \n",
    "\n",
    "            \n",
    "            \n",
    "#             saliency_score_diffmean=compute_diff_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "#             saliency_score_dprime=compute_dprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "#             saliency_score_aprime=compute_aprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "            \n",
    "            \n",
    "#             print('saliency_score_diffmean: \\t', saliency_score_diffmean)\n",
    "#             print('saliency_score_diffmean: \\t', saliency_score_dprime)\n",
    "#             print('saliency_score_diffmean: \\t',saliency_score_aprime)\n",
    "#             ####################################################################################################\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Analysis (Saliency) over all images from the validation set (also you will have to pre select the model). \n",
    "\n",
    "This analysis takes roughly 25 minutes to complete on 600 validation images when run on the contour model reading out from avgpool layer of Alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=get_B,get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "# # val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "# val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=1, num_workers=num_workers, shuffle=False)\n",
    "# print(len(val_dataset_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the model from the saved model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,file in enumerate(model_file_paths):\n",
    "#     print(i,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_file=model_file_paths[0]\n",
    "# print(selected_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Saliency for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_saliency(prep_img,prep_recorder,file):\n",
    "#     prep_img=Variable(prep_img,requires_grad=True).to(device)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     ####################################################################################################\n",
    "#     ## Loading the model and setting up the Guided Spliced Model for Attention Maps   \n",
    "#     ## Model\n",
    "#     checkpoint=torch.load(selected_file)\n",
    "    \n",
    "#     loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "\n",
    "#     loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "#     GBP = Spliced_GuidedBackprop(loaded_spliced_model,device)\n",
    "#     ####################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     ####################################################################################################\n",
    "#     ## Getting the Saliency Maps\n",
    "#     # Get gradients\n",
    "#     guided_grads = GBP.generate_gradients(prep_img, 1)\n",
    "#     # Gradients\n",
    "#     guided_grads_vis = guided_grads - guided_grads.min()\n",
    "#     guided_grads_vis /= guided_grads_vis.max()\n",
    "\n",
    "#     # Grayscale Gradients\n",
    "#     grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
    "#     grayscale_guided_grads_vis = grayscale_guided_grads - grayscale_guided_grads.min()\n",
    "#     grayscale_guided_grads_vis /= grayscale_guided_grads_vis.max()\n",
    "    \n",
    "    \n",
    "#     viz_image=grayscale_guided_grads_vis[0]\n",
    "#     ####################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     ####################################################################################################\n",
    "#     ## Compute saliency scores\n",
    "#     mask_img_path_fg, mask_img_path_bg, list_gauss=mask_renderer(torch.load(prep_recorder),140)\n",
    "#     mask_img_path_fg=np.array(mask_img_path_fg)\n",
    "#     mask_img_path_bg=np.array(mask_img_path_bg)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     saliency_score_diffmean=compute_diff_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "#     saliency_score_dprime=compute_dprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "#     saliency_score_aprime=compute_aprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "#     ####################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     ####################################################################################################\n",
    "#     ## Making the run more memory efficient\n",
    "#     loaded_spliced_model=loaded_spliced_model.to('cpu')\n",
    "#     del GBP\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "#     ####################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "#     return saliency_score_diffmean,saliency_score_dprime,saliency_score_aprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_saliency_score_diff=[]\n",
    "# all_saliency_score_dprime=[]\n",
    "# all_saliency_score_aprime=[]\n",
    "\n",
    "# all_betas=[]\n",
    "# all_labels=[]\n",
    "\n",
    "# for (a, b, d, alpha, nel, labels, record) in tqdm(val_loader_norm):\n",
    "#     for img_num in tqdm(range(a.shape[0]),disable=True):\n",
    "        \n",
    "#         prep_img=torch.unsqueeze(a[img_num],0)\n",
    "#         prep_recorder=record[img_num]\n",
    "        \n",
    "#         saliency_score_diffmean,saliency_score_dprime,saliency_score_aprime=compute_saliency(prep_img,prep_recorder,file)\n",
    "        \n",
    "#         all_saliency_score_diff.append(saliency_score_diffmean)\n",
    "#         all_saliency_score_dprime.append(saliency_score_dprime)\n",
    "#         all_saliency_score_aprime.append(saliency_score_aprime)\n",
    "        \n",
    "        \n",
    "        \n",
    "#     all_betas.append(b)\n",
    "#     all_labels.append(labels)\n",
    "\n",
    "# all_betas=torch.cat(all_betas).numpy()\n",
    "# all_labels=torch.cat(all_labels).numpy()\n",
    "# all_saliency_score_diff=np.array(all_saliency_score_diff)\n",
    "# all_saliency_score_dprime=np.array(all_saliency_score_dprime)\n",
    "# all_saliency_score_aprime=np.array(all_saliency_score_aprime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contour_present_pos=np.where(all_labels==1)[0]\n",
    "# contour_absent_pos=np.where(all_labels==0)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment Sensitivity via Diff. of Means Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a figure\n",
    "# fig = plt.figure(figsize=(16,10))\n",
    "# # Create a GridSpec object\n",
    "# gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "# # First subplot: Left column, spanning both rows\n",
    "# ax1 = plt.subplot(gs[:, 0])\n",
    "\n",
    "# # Second subplot: Top-right corner\n",
    "# ax2 = plt.subplot(gs[0, 1])\n",
    "\n",
    "# # Third subplot: Bottom-right corner\n",
    "# ax3 = plt.subplot(gs[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # First subplot: Left column, spanning both rows\n",
    "# # Scatter Plot with Connecting Lines\n",
    "# for x1, x2 in zip(all_saliency_score_diff[contour_absent_pos], all_saliency_score_diff[contour_present_pos]):\n",
    "#     ax1.plot([1, 4], [x1, x2], 'ko-', markersize=8,alpha=0.09)\n",
    "# ax1.set_xticks([1, 4], ['Contour absent', 'Contour present'])\n",
    "# ax1.set_xlim(0,5)\n",
    "# ax1.set_ylabel('saliency_score_diffmean')\n",
    "# ax1.set_title('Contour Sensitivity using Diff. Means')\n",
    "# ax1.spines['top'].set_visible(False)\n",
    "# ax1.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Second subplot: Top-right corner\n",
    "# sns.distplot(all_saliency_score_diff[contour_present_pos], ax=ax2, label='contour_present',color='green')\n",
    "# sns.distplot(all_saliency_score_diff[contour_absent_pos], ax=ax2, label='contour_absent',color='gray')\n",
    "# ax2.set_title('Contour Sensitivity using Diff. Means (Histogram)')\n",
    "# ax2.legend()\n",
    "# ax2.spines['top'].set_visible(False)\n",
    "# ax2.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Third subplot: Bottom-right corner\n",
    "# sns.distplot(all_saliency_score_diff[contour_present_pos] - all_saliency_score_diff[contour_absent_pos], ax=ax3, label='difference',color='k')\n",
    "# ax3.set_title('Alignment Sensitivity using Diff. Means')\n",
    "# ax3.spines['top'].set_visible(False)\n",
    "# ax3.spines['right'].set_visible(False)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# # Perform the paired t-test\n",
    "# t_statistic, p_value = stats.ttest_rel(all_saliency_score_diff[contour_present_pos], all_saliency_score_diff[contour_absent_pos])\n",
    "# print(f\"t-statistic: {t_statistic:.4f}\")\n",
    "# print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment Sensitivity via DPrime Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a figure\n",
    "# fig = plt.figure(figsize=(16,10))\n",
    "# # Create a GridSpec object\n",
    "# gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "# # First subplot: Left column, spanning both rows\n",
    "# ax1 = plt.subplot(gs[:, 0])\n",
    "\n",
    "# # Second subplot: Top-right corner\n",
    "# ax2 = plt.subplot(gs[0, 1])\n",
    "\n",
    "# # Third subplot: Bottom-right corner\n",
    "# ax3 = plt.subplot(gs[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # First subplot: Left column, spanning both rows\n",
    "# # Scatter Plot with Connecting Lines\n",
    "# for x1, x2 in zip(all_saliency_score_dprime[contour_absent_pos], all_saliency_score_dprime[contour_present_pos]):\n",
    "#     ax1.plot([1, 4], [x1, x2], 'ko-', markersize=8,alpha=0.09)\n",
    "# ax1.set_xticks([1, 4], ['Contour absent', 'Contour present'])\n",
    "# ax1.set_xlim(0,5)\n",
    "# ax1.set_ylabel('saliency_score_dprime')\n",
    "# ax1.set_title('Contour Sensitivity using DPrime')\n",
    "# ax1.spines['top'].set_visible(False)\n",
    "# ax1.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Second subplot: Top-right corner\n",
    "# sns.distplot(all_saliency_score_dprime[contour_present_pos], ax=ax2, label='contour_present',color='green')\n",
    "# sns.distplot(all_saliency_score_dprime[contour_absent_pos], ax=ax2, label='contour_absent',color='gray')\n",
    "# ax2.set_title('Contour Sensitivity using DPrime (Histogram)')\n",
    "# ax2.legend()\n",
    "# ax2.spines['top'].set_visible(False)\n",
    "# ax2.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Third subplot: Bottom-right corner\n",
    "# sns.distplot(all_saliency_score_dprime[contour_present_pos] - all_saliency_score_dprime[contour_absent_pos], ax=ax3, label='difference',color='k')\n",
    "# ax3.set_title('Alignment Sensitivity using DPrime')\n",
    "# ax3.spines['top'].set_visible(False)\n",
    "# ax3.spines['right'].set_visible(False)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# # Perform the paired t-test\n",
    "# t_statistic, p_value = stats.ttest_rel(all_saliency_score_dprime[contour_present_pos], all_saliency_score_dprime[contour_absent_pos])\n",
    "# print(f\"t-statistic: {t_statistic:.4f}\")\n",
    "# print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment Sensitivity via APrime Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a figure\n",
    "# fig = plt.figure(figsize=(16,10))\n",
    "# # Create a GridSpec object\n",
    "# gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "# # First subplot: Left column, spanning both rows\n",
    "# ax1 = plt.subplot(gs[:, 0])\n",
    "\n",
    "# # Second subplot: Top-right corner\n",
    "# ax2 = plt.subplot(gs[0, 1])\n",
    "\n",
    "# # Third subplot: Bottom-right corner\n",
    "# ax3 = plt.subplot(gs[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # First subplot: Left column, spanning both rows\n",
    "# # Scatter Plot with Connecting Lines\n",
    "# for x1, x2 in zip(all_saliency_score_aprime[contour_absent_pos], all_saliency_score_aprime[contour_present_pos]):\n",
    "#     ax1.plot([1, 4], [x1, x2], 'ko-', markersize=8,alpha=0.09)\n",
    "# ax1.set_xticks([1, 4], ['Contour absent', 'Contour present'])\n",
    "# ax1.set_xlim(0,5)\n",
    "# ax1.set_ylabel('saliency_score_aprime')\n",
    "# ax1.set_title('Contour Sensitivity using APrime')\n",
    "# ax1.spines['top'].set_visible(False)\n",
    "# ax1.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Second subplot: Top-right corner\n",
    "# sns.distplot(all_saliency_score_aprime[contour_present_pos], ax=ax2, label='contour_present',color='green')\n",
    "# sns.distplot(all_saliency_score_aprime[contour_absent_pos], ax=ax2, label='contour_absent',color='gray')\n",
    "# ax2.set_title('Contour Sensitivity using APrime (Histogram)')\n",
    "# ax2.legend()\n",
    "# ax2.spines['top'].set_visible(False)\n",
    "# ax2.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Third subplot: Bottom-right corner\n",
    "# sns.distplot(all_saliency_score_aprime[contour_present_pos] - all_saliency_score_aprime[contour_absent_pos], ax=ax3, label='difference',color='k')\n",
    "# ax3.set_title('Alignment Sensitivity using APrime')\n",
    "# ax3.spines['top'].set_visible(False)\n",
    "# ax3.spines['right'].set_visible(False)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# # Perform the paired t-test\n",
    "# t_statistic, p_value = stats.ttest_rel(all_saliency_score_aprime[contour_present_pos], all_saliency_score_aprime[contour_absent_pos])\n",
    "# print(f\"t-statistic: {t_statistic:.4f}\")\n",
    "# print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta-wise saliency score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_beta_vals=np.unique(all_betas)\n",
    "# unique_beta_saliency_mean=[]\n",
    "# unique_beta_saliency_std=[]\n",
    "\n",
    "\n",
    "\n",
    "# for i in unique_beta_vals:\n",
    "    \n",
    "#     unique_beta_saliency_mean.append(np.mean(all_saliency_score_aprime[np.where(all_betas==i)[0]]))\n",
    "#     unique_beta_saliency_std.append(np.std(all_saliency_score_aprime[np.where(all_betas==i)[0]],ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(unique_beta_saliency_mean)\n",
    "# print(unique_beta_saliency_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ALREADY COMPUTED OVERALL\n",
    "# height=unique_beta_saliency_mean\n",
    "# error_bar=unique_beta_saliency_std\n",
    "\n",
    "# bars=unique_beta_vals\n",
    "# y_pos = np.arange(len(bars)) / 3.5\n",
    "# color=['green'] * len(unique_beta_vals)\n",
    "\n",
    "# plt.figure(figsize=(12,4))\n",
    "\n",
    "# plt.gca().bar(y_pos, height, yerr=error_bar, align='center', width=0.09, color=color, alpha=0.2, ecolor='gray', capsize=4)\n",
    "# plt.xticks(y_pos, bars,fontsize=10)\n",
    "# plt.yticks(fontsize=10)\n",
    "# plt.ylim(0.0,1.0)\n",
    "# plt.ylabel('Saliency Score',fontsize=15,labelpad=20)\n",
    "# plt.xlabel('Beta Values',fontsize=15,labelpad=20)\n",
    "# plt.gca().spines['top'].set_visible(False)\n",
    "# plt.gca().spines['right'].set_visible(False)\n",
    "# plt.gca().spines['left'].set_visible(True)\n",
    "# plt.gca().spines['bottom'].set_visible(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) NEW-All models, All images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location sensitivity and Alignment sensitivity\n",
    "\n",
    "This analysis takes roughly 8 hours minutes to complete on 600 validation images when run on all contour readout models (with object recognition alexnet finetuned backbones and trained on braod contour curvatures) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the dataset and set of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=get_B,get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "print(len(val_dataset_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_paths=sorted(glob.glob(os.path.join(saved_model_config['saved_model_directory_or_finetune_broad'],'*.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune.pt\n",
      "1 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_classifier-0_mode_finetune.pt\n",
      "2 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_classifier-1_mode_finetune.pt\n",
      "3 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_classifier-2_mode_finetune.pt\n",
      "4 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_classifier-3_mode_finetune.pt\n",
      "5 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_classifier-4_mode_finetune.pt\n",
      "6 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_classifier-5_mode_finetune.pt\n",
      "7 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_classifier-6_mode_finetune.pt\n",
      "8 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-0_mode_finetune.pt\n",
      "9 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-10_mode_finetune.pt\n",
      "10 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-11_mode_finetune.pt\n",
      "11 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-12_mode_finetune.pt\n",
      "12 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-1_mode_finetune.pt\n",
      "13 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-2_mode_finetune.pt\n",
      "14 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-3_mode_finetune.pt\n",
      "15 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-4_mode_finetune.pt\n",
      "16 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-5_mode_finetune.pt\n",
      "17 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-6_mode_finetune.pt\n",
      "18 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-7_mode_finetune.pt\n",
      "19 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-8_mode_finetune.pt\n",
      "20 ../../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-9_mode_finetune.pt\n"
     ]
    }
   ],
   "source": [
    "for i,file in enumerate(model_file_paths):\n",
    "    print(i,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Saliency for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency(prep_img,prep_recorder,file):\n",
    "    prep_img=Variable(prep_img,requires_grad=True).to(device)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Loading the model and setting up the Guided Spliced Model for Attention Maps   \n",
    "    ## Model\n",
    "    checkpoint=torch.load(selected_file)\n",
    "    \n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    GBP = Spliced_GuidedBackprop(loaded_spliced_model,device)\n",
    "    ####################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ####################################################################################################\n",
    "    ## Getting the Saliency Maps\n",
    "    # Get gradients\n",
    "    guided_grads = GBP.generate_gradients(prep_img, 1)\n",
    "    # Gradients\n",
    "    guided_grads_vis = guided_grads - guided_grads.min()\n",
    "    guided_grads_vis /= guided_grads_vis.max()\n",
    "\n",
    "    # Grayscale Gradients\n",
    "    grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
    "    grayscale_guided_grads_vis = grayscale_guided_grads - grayscale_guided_grads.min()\n",
    "    grayscale_guided_grads_vis /= grayscale_guided_grads_vis.max()\n",
    "    \n",
    "    \n",
    "    viz_image=grayscale_guided_grads_vis[0]\n",
    "    ####################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Compute saliency scores\n",
    "    mask_img_path_fg, mask_img_path_bg, list_gauss=mask_renderer(torch.load(prep_recorder),140)\n",
    "    mask_img_path_fg=np.array(mask_img_path_fg)\n",
    "    mask_img_path_bg=np.array(mask_img_path_bg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    saliency_score_diffmean=compute_diff_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    saliency_score_dprime=compute_dprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    saliency_score_aprime=compute_aprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Making the run more memory efficient\n",
    "    loaded_spliced_model=loaded_spliced_model.to('cpu')\n",
    "    del GBP\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    return saliency_score_diffmean,saliency_score_dprime,saliency_score_aprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_saliency(prep_img,prep_recorder,file):\n",
    "#     prep_img=Variable(prep_img,requires_grad=True).to(device)\n",
    "\n",
    "\n",
    "#     # Model\n",
    "#     checkpoint=torch.load(file)\n",
    "    \n",
    "#     # loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device='cpu')\n",
    "#     loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "\n",
    "#     loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "#     GBP = Spliced_GuidedBackprop(loaded_spliced_model,device)\n",
    "\n",
    "\n",
    "\n",
    "#     # Get gradients\n",
    "#     guided_grads = GBP.generate_gradients(prep_img, 1)\n",
    "#     # Gradients\n",
    "#     guided_grads_vis = guided_grads - guided_grads.min()\n",
    "#     guided_grads_vis /= guided_grads_vis.max()\n",
    "\n",
    "#     # Grayscale Gradients\n",
    "#     grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
    "#     grayscale_guided_grads_vis = grayscale_guided_grads - grayscale_guided_grads.min()\n",
    "#     grayscale_guided_grads_vis /= grayscale_guided_grads_vis.max()\n",
    "\n",
    "\n",
    "#     # Saliency Score\n",
    "#     viz_image=grayscale_guided_grads_vis[0]\n",
    "\n",
    "#     mask_img_path_fg, mask_img_path_bg, list_gauss=mask_renderer(torch.load(prep_recorder),140)\n",
    "#     mask_img_path_fg=np.array(mask_img_path_fg)\n",
    "#     mask_img_path_bg=np.array(mask_img_path_bg)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     saliency_score_diffmean=compute_diff_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "#     saliency_score_dprime=compute_dprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "#     saliency_score_aprime=compute_aprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    \n",
    "    \n",
    "#     loaded_spliced_model=loaded_spliced_model.to('cpu')\n",
    "#     del GBP\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "    \n",
    "#     return saliency_score_diffmean,saliency_score_dprime,saliency_score_aprime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the loop - All models, All images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saliency_score_diff={}\n",
    "model_saliency_score_dprime={}\n",
    "model_saliency_score_aprime={}\n",
    "\n",
    "\n",
    "for file in tqdm(model_file_paths):\n",
    "    print(file)\n",
    "    checkpoint=torch.load(file)\n",
    "    \n",
    "    model_saliency_score_diff[checkpoint['training_config']['layer_name']]=[]\n",
    "    model_saliency_score_dprime[checkpoint['training_config']['layer_name']]=[]\n",
    "    model_saliency_score_aprime[checkpoint['training_config']['layer_name']]=[]\n",
    "    \n",
    "    for (a, b, d, alpha, nel, labels, record) in tqdm(val_loader_norm,disable=True):\n",
    "        \n",
    "        \n",
    "        for img_num in tqdm(range(a.shape[0]),disable=True):\n",
    "            prep_img=torch.unsqueeze(a[img_num],0)\n",
    "            prep_recorder=record[img_num]\n",
    "            \n",
    "            saliency_score_diffmean,saliency_score_dprime,saliency_score_aprime=compute_saliency(prep_img,prep_recorder,file)\n",
    "\n",
    "            model_saliency_score_diff[checkpoint['training_config']['layer_name']].append(saliency_score_diffmean)\n",
    "            model_saliency_score_dprime[checkpoint['training_config']['layer_name']].append(saliency_score_dprime)\n",
    "            model_saliency_score_aprime[checkpoint['training_config']['layer_name']].append(saliency_score_aprime)\n",
    "            \n",
    "        \n",
    "        \n",
    "    model_saliency_score_diff[checkpoint['training_config']['layer_name']]=np.array(model_saliency_score_diff[checkpoint['training_config']['layer_name']])\n",
    "    model_saliency_score_dprime[checkpoint['training_config']['layer_name']]=np.array(model_saliency_score_dprime[checkpoint['training_config']['layer_name']])\n",
    "    model_saliency_score_aprime[checkpoint['training_config']['layer_name']]=np.array(model_saliency_score_aprime[checkpoint['training_config']['layer_name']])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_betas=[]\n",
    "all_labels=[]\n",
    "\n",
    "for (a, b, d, alpha, nel, labels, record) in tqdm(val_loader_norm,disable=True):\n",
    "    all_betas.append(b)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "    \n",
    "all_betas=torch.cat(all_betas).numpy()\n",
    "all_labels=torch.cat(all_labels).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saliency_score_diff.keys()\n",
    "model_saliency_score_dprime.keys()\n",
    "model_saliency_score_aprime.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the scores for all models from the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_present_pos=np.where(all_labels==1)[0]\n",
    "contour_absent_pos=np.where(all_labels==0)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,4))\n",
    "# plt.subplot(1,2,1)\n",
    "# sns.distplot(model_saliency_score_aprime['avgpool'][contour_present_pos],label='contour_present',color='green')\n",
    "# sns.distplot(model_saliency_score_aprime['avgpool'][contour_absent_pos],label='contour_absent',color='gray')\n",
    "# plt.title('saliency_score_aprime')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# sns.distplot(model_saliency_score_aprime['avgpool'][contour_present_pos] - model_saliency_score_aprime['avgpool'][contour_absent_pos],label='difference',color='k')\n",
    "# plt.title('Alignment Sensitivity')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8,10))\n",
    "# # Scatter Plot with Connecting Lines\n",
    "# for x1, x2 in zip(model_saliency_score_aprime['avgpool'][contour_absent_pos], model_saliency_score_aprime['avgpool'][contour_present_pos]):\n",
    "#     plt.plot([x1, x2], [1, 2], 'ko-', markersize=8,alpha=0.09)\n",
    "    \n",
    "\n",
    "# plt.yticks([1, 2], ['Contour absent', 'Contour present'])\n",
    "\n",
    "# plt.gca().spines['top'].set_visible(False)\n",
    "# plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualzing all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foreground_sensitivity_allmodels(sal_dict,list_layers,pos):\n",
    "    mean_sal=[]\n",
    "    std_sal=[]\n",
    "    confint_sal=[]\n",
    "    confint_lower=[]\n",
    "    confint_upper=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(list_layers)):\n",
    "        layer=list_layers[i]\n",
    "        \n",
    "        \n",
    "        if layer in sal_dict.keys():\n",
    "            \n",
    "            \n",
    "            mean_sal.append(np.mean(sal_dict[layer][pos]))\n",
    "            std_sal.append(np.std(sal_dict[layer][pos],ddof=1))\n",
    "            \n",
    "            \n",
    "            \n",
    "            confint_sal.append(1.96*(np.std(sal_dict[layer][pos],ddof=1) / np.sqrt(len(sal_dict[layer][pos]))))\n",
    "            confint_lower.append(np.percentile(sal_dict[layer][pos], 2.5))\n",
    "            confint_upper.append(np.percentile(sal_dict[layer][pos], 97.5))\n",
    "            \n",
    "    return mean_sal,std_sal,confint_sal,confint_lower,confint_upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alignment_sensitivity_allmodels(sal_dict,list_layers,present_pos,absent_pos):\n",
    "    \n",
    "    mean_sal=[]\n",
    "    std_sal=[]\n",
    "    confint_sal=[]\n",
    "    confint_lower=[]\n",
    "    confint_upper=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(list_layers)):\n",
    "        layer=list_layers[i]\n",
    "        \n",
    "        \n",
    "        if layer in sal_dict.keys():\n",
    "            \n",
    "            \n",
    "            diff_values=sal_dict[layer][present_pos] - sal_dict[layer][absent_pos]\n",
    "            \n",
    "            mean_sal.append(np.mean(diff_values))\n",
    "            std_sal.append(np.std(diff_values))\n",
    "            \n",
    "            \n",
    "            \n",
    "            confint_sal.append(1.96*(np.std(diff_values) / np.sqrt(len(diff_values))))\n",
    "            confint_lower.append(np.percentile(diff_values, 2.5))\n",
    "            confint_upper.append(np.percentile(diff_values, 97.5))\n",
    "            \n",
    "    return mean_sal,std_sal,confint_sal,confint_lower,confint_upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_layers=['features_0',\n",
    " 'features_1',\n",
    " 'features_2',\n",
    " 'features_3',\n",
    " 'features_4',\n",
    " 'features_5',\n",
    " 'features_6',\n",
    " 'features_7',\n",
    " 'features_8',\n",
    " 'features_9',\n",
    " 'features_10',\n",
    " 'features_11',\n",
    " 'features_12',\n",
    " 'avgpool',\n",
    " 'classifier_0',\n",
    " 'classifier_1',\n",
    " 'classifier_2',\n",
    " 'classifier_3',\n",
    " 'classifier_4',\n",
    " 'classifier_5',\n",
    " 'classifier_6']\n",
    "\n",
    "for i in range(len(list_layers)):\n",
    "    list_layers[i]=list_layers[i].replace('_','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saliency_score_aprime['avgpool'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_present_pos=np.where(all_labels==1)[0]\n",
    "contour_absent_pos=np.where(all_labels==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sal,std_sal,confint_sal,confint_lower,confint_upper=get_foreground_sensitivity_allmodels(model_saliency_score_diff,list_layers,contour_present_pos)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(22,10))\n",
    "color_or_finetune=(78/255,121/255,180/255)\n",
    "spacing_x=2\n",
    "error_bar_width=3\n",
    "error_bar_cap=4\n",
    "\n",
    "plt.errorbar(np.arange(len(mean_sal)) * spacing_x, mean_sal, yerr=confint_sal, \n",
    "             capsize=error_bar_cap,elinewidth=error_bar_width,fmt='o',marker='o', color=color_or_finetune, markersize=10, \n",
    "             label='or finetune', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(mean_sal)) * spacing_x,list_layers,rotation=90,fontsize=15)\n",
    "plt.yticks(fontsize=15) \n",
    "\n",
    "\n",
    "plt.ylabel('Diff of means',fontsize=15)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title('Foreground Sensitivity in Contour Images',fontsize=15)\n",
    "\n",
    "plt.savefig('./dev/manuscript_figures/foreground_sensitivity_allmodel_diffmean.png', format='png', dpi=600)\n",
    "\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sal,std_sal,confint_sal,confint_lower,confint_upper=get_foreground_sensitivity_allmodels(model_saliency_score_aprime,list_layers,contour_present_pos)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(22,10))\n",
    "color_or_finetune=(78/255,121/255,180/255)\n",
    "spacing_x=2\n",
    "error_bar_width=3\n",
    "error_bar_cap=4\n",
    "\n",
    "plt.errorbar(np.arange(len(mean_sal)) * spacing_x, mean_sal, yerr=confint_sal, \n",
    "             capsize=error_bar_cap,elinewidth=error_bar_width,fmt='o',marker='o', color=color_or_finetune, markersize=10, \n",
    "             label='or finetune', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(mean_sal)) * spacing_x,list_layers,rotation=90,fontsize=15)\n",
    "plt.yticks(fontsize=15) \n",
    "\n",
    "\n",
    "plt.ylabel('Aprime',fontsize=15)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title('Foreground Sensitivity in Contour Images',fontsize=15)\n",
    "\n",
    "plt.savefig('./dev/manuscript_figures/foreground_sensitivity_allmodel_aprime.png', format='png', dpi=600)\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sal,std_sal,confint_sal,confint_lower,confint_upper=get_alignment_sensitivity_allmodels(model_saliency_score_aprime,list_layers,contour_present_pos,contour_absent_pos)\n",
    "\n",
    "plt.figure(figsize=(22,10))\n",
    "color_or_finetune=(78/255,121/255,180/255)\n",
    "spacing_x=2\n",
    "error_bar_width=3\n",
    "error_bar_cap=4\n",
    "\n",
    "plt.errorbar(np.arange(len(mean_sal)) * spacing_x, mean_sal, yerr=confint_sal, \n",
    "             capsize=error_bar_cap,elinewidth=error_bar_width,fmt='o',marker='o', color=color_or_finetune, markersize=10, \n",
    "             label='or finetune', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(mean_sal)) * spacing_x,list_layers,rotation=90,fontsize=15)\n",
    "plt.yticks(fontsize=15) \n",
    "\n",
    "\n",
    "plt.ylabel('Aprime',fontsize=15)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title('Alignment Sensitivity in Contour Images',fontsize=15)\n",
    "\n",
    "plt.savefig('./dev/manuscript_figures/alignment_sensitivity_allmodel_aprime.png', format='png', dpi=600)\n",
    "plt.show()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
