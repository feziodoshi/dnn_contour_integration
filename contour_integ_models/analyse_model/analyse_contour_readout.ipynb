{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "\n",
    "\n",
    "# Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Pytorch related\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils import data as dt\n",
    "from torchinfo import summary\n",
    "import torchvision.models as pretrained_models\n",
    "from alexnet_pytorch import AlexNet\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils import model_zoo\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "# Numpy, Matplotlib, Pandas, Sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from sklearn import manifold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "# python utilities\n",
    "from itertools import combinations\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import copy\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import networkx as nx\n",
    "import os\n",
    "from IPython.display import Image\n",
    "from IPython.core.debugger import set_trace\n",
    "import collections\n",
    "from functools import partial\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "\n",
    "from PIL import Image, ImageStat\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "# Extra imports\n",
    "from lib.feature_extractor import FeatureExtractor\n",
    "from lib.custom_dataset import Contour_Dataset\n",
    "from lib.build_fe_ft_models import *\n",
    "from lib.misc_functions import *\n",
    "from lib.field_stim_functions import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dim=512\n",
    "\n",
    "batch_size=32\n",
    "num_workers=8\n",
    "device = torch.device('cuda:'+'3')\n",
    "# device = 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all the values from the config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdiet_savedmodel_config import *\n",
    "\n",
    "print('\\n Visual Diet config \\n')\n",
    "print(visual_diet_config)\n",
    "\n",
    "\n",
    "print('\\n Saved Model config \\n')\n",
    "print(saved_model_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_directory = saved_model_config['saved_model_directory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory=visual_diet_config['root_directory']\n",
    "get_B=visual_diet_config['get_B']\n",
    "get_D=visual_diet_config['get_D']\n",
    "get_A=visual_diet_config['get_A']\n",
    "get_numElements=visual_diet_config['get_numElements']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_paths = glob.glob(os.path.join(saved_directory,'*.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize images using parameters from the training image set\n",
    "data_transform = transforms.Compose([       \n",
    " transforms.Resize(img_dim),                   \n",
    " transforms.CenterCrop((img_dim,img_dim)),         \n",
    " transforms.ToTensor(),                    \n",
    " transforms.Normalize(                      \n",
    " mean=[0.485, 0.456, 0.406],                \n",
    " std=[0.229, 0.224, 0.225]                  \n",
    " )])\n",
    "\n",
    "data_transform_without_norm = transforms.Compose([       \n",
    " transforms.Resize(img_dim),                   \n",
    " transforms.CenterCrop((img_dim,img_dim)),         \n",
    " transforms.ToTensor()                    \n",
    " ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diff_means(arr1, arr2):\n",
    "    mean1 = np.mean(arr1)\n",
    "    mean2 = np.mean(arr2)\n",
    "    \n",
    "    return (mean1 - mean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dprime_means(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Compute d' based on the difference of means divided by the pooled standard deviation.\n",
    "    \"\"\"\n",
    "    mean1 = np.mean(arr1)\n",
    "    mean2 = np.mean(arr2)\n",
    "    \n",
    "    var1 = np.var(arr1, ddof=1)  # ddof=1 for sample variance\n",
    "    var2 = np.var(arr2, ddof=1)  # ddof=1 for sample variance\n",
    "    \n",
    "    # Calculate the pooled standard deviation\n",
    "    pooled_std = np.sqrt((var1 + var2) / 2)\n",
    "    \n",
    "    # Compute d'\n",
    "    d_prime = (mean1 - mean2) / pooled_std\n",
    "    \n",
    "    return d_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aprime_means(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Compute A' as the area under the ROC curve based on two input arrays.\n",
    "    \"\"\"\n",
    "    # Combine the arrays and create corresponding labels\n",
    "    combined = np.concatenate((arr1, arr2))\n",
    "    labels = np.concatenate((np.ones(len(arr1)), np.zeros(len(arr2))))\n",
    "    \n",
    "    # Compute AUC\n",
    "    a_prime = roc_auc_score(labels, combined)\n",
    "    \n",
    "    return a_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(beta_val=(0,91,1),img_num=(0,20,1))\n",
    "def get_data(beta_val=15,img_num=4):\n",
    "    \n",
    "    \n",
    "    ### Show the image\n",
    "    val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=[beta_val],get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "    val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    val_dataset_without_norm = Contour_Dataset(root=root_directory,transform=data_transform_without_norm,train=False,get_B=[beta_val],get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "    val_loader_without_norm = torch.utils.data.DataLoader(dataset=val_dataset_without_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    a, b, d, alpha, nel, labels, record =next(iter(val_loader_norm))\n",
    "    prep_img=torch.unsqueeze(a[img_num],0)\n",
    "    print('Beta Val: ',b[img_num])\n",
    "    print('Contour Present: ',labels[img_num])\n",
    "    prep_img=prep_img.to(device)\n",
    "    prep_img=Variable(prep_img,requires_grad=True)\n",
    "    ## Plot the orginal image\n",
    "    original_img=np.transpose(prep_img[0].detach().cpu().numpy(),(1,2,0))\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(original_img)\n",
    "    \n",
    "    ## Plot the orginal unnormalized image\n",
    "    plt.figure(figsize=(10,8))\n",
    "    a_without_norm,b, d, alpha, nel, labels, record=next(iter(val_loader_without_norm))\n",
    "    plt.imshow(np.transpose(torch.unsqueeze(a_without_norm[img_num],0)[0].numpy(),(1,2,0)))\n",
    "    \n",
    "    \n",
    "    ## Plot the image with contour\n",
    "    print(record[img_num])\n",
    "    image_recorder_dict=torch.load(record[img_num])\n",
    "    img_contour, img_control, img_contour_background, img_control_background=image_renderer(image_recorder_dict)\n",
    "    display(blend(show_path(image_recorder_dict['path_points'], image_recorder_dict['path_centers'], imHeight=image_recorder_dict['image_height'], imWidth=image_recorder_dict['image_width']),img_contour_background,0.7))\n",
    "    \n",
    "    display(img_contour_background)\n",
    "    display(img_contour)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Analysis (Saliency) over 1 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(beta_val=(0,91,1),img_num=(0,batch_size,1))\n",
    "def get_data(beta_val=15,img_num=4):\n",
    "    \n",
    "    \n",
    "    ### Show the image\n",
    "    val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=[beta_val],get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "    val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    val_dataset_without_norm = Contour_Dataset(root=root_directory,transform=data_transform_without_norm,train=False,get_B=[beta_val],get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "    val_loader_without_norm = torch.utils.data.DataLoader(dataset=val_dataset_without_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    a, b, d, alpha, nel, labels, record =next(iter(val_loader_norm))\n",
    "    prep_img=torch.unsqueeze(a[img_num],0)\n",
    "    print('Beta Val: ',b[img_num])\n",
    "    print('Contour Present: ',labels[img_num])\n",
    "    prep_img=prep_img.to(device)\n",
    "    prep_img=Variable(prep_img,requires_grad=True)\n",
    "    \n",
    "    ## Plot the orginal image - this is the normalized one, the one that will actually be used\n",
    "    # original_img=np.transpose(prep_img[0].detach().cpu().numpy(),(1,2,0))\n",
    "    # plt.figure(figsize=(10,8))\n",
    "    # plt.imshow(original_img)\n",
    "    # plt.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Plot the orginal unnormalized image\n",
    "    plt.figure(figsize=(10,8))\n",
    "    a_without_norm,b, d, alpha, nel, labels, record=next(iter(val_loader_without_norm))\n",
    "    plt.imshow(np.transpose(torch.unsqueeze(a_without_norm[img_num],0)[0].numpy(),(1,2,0)))\n",
    "    plt.axis('off')\n",
    "    # plt.savefig('./dev/ccn_figures/poster/original_image.png', bbox_inches='tight',dpi=300) \n",
    "    # plt.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    @interact(file=model_file_paths)\n",
    "    def vis_image(file,show_vis=False):\n",
    "        \n",
    "\n",
    "        if(show_vis):    \n",
    "            checkpoint=torch.load(file)\n",
    "            \n",
    "            ####################################################################################################\n",
    "            ## Showing the Model Stats\n",
    "            print('BASE MODEL IS',checkpoint['training_config']['base_model_name'])\n",
    "            print('Final Val Accuracy is',checkpoint['metrics']['val_acc'][-1])\n",
    "\n",
    "            plt.figure(figsize=(12,4))\n",
    "\n",
    "\n",
    "            plt.subplot(1,2,1)\n",
    "            plt.plot(np.arange(len(checkpoint['metrics']['train_acc'])),checkpoint['metrics']['train_acc'],label='train')\n",
    "            plt.plot(np.arange(len(checkpoint['metrics']['val_acc'])),checkpoint['metrics']['val_acc'],label='val')\n",
    "            plt.axhline(y=0.5,linestyle='--',color='r',label='chance')\n",
    "\n",
    "            plt.xlabel('Epochs',fontsize=15,labelpad=15)\n",
    "            plt.ylabel('Accuracy',fontsize=15,labelpad=15)\n",
    "\n",
    "            plt.gca().spines['top'].set_visible(False)\n",
    "            plt.gca().spines['right'].set_visible(False)\n",
    "            plt.yticks(fontsize=15)\n",
    "            plt.xticks(fontsize=15)   \n",
    "            plt.ylim(0.4,1.01)\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.plot(np.arange(len(checkpoint['metrics']['train_loss'])),checkpoint['metrics']['train_loss'],label='train')\n",
    "            plt.plot(np.arange(len(checkpoint['metrics']['val_loss'])),checkpoint['metrics']['val_loss'],label='val')\n",
    "\n",
    "            plt.xlabel('Epochs',fontsize=15,labelpad=15)\n",
    "            plt.ylabel('Loss',fontsize=15,labelpad=15)\n",
    "\n",
    "            plt.gca().spines['top'].set_visible(False)\n",
    "            plt.gca().spines['right'].set_visible(False)\n",
    "            plt.yticks(fontsize=15)\n",
    "            plt.xticks(fontsize=15)   \n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            ####################################################################################################\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            ####################################################################################################\n",
    "            ## Loading the model and setting up the Guided Spliced Model for Attention Maps\n",
    "            loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "            loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "            GBP = Spliced_GuidedBackprop(loaded_spliced_model,device)\n",
    "            ####################################################################################################\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ####################################################################################################\n",
    "            ## Getting the Saliency Maps\n",
    "            # Get gradients\n",
    "            guided_grads = GBP.generate_gradients(prep_img, 1)\n",
    "\n",
    "            \n",
    "            # Gradients\n",
    "            guided_grads_vis = guided_grads - guided_grads.min()\n",
    "            guided_grads_vis /= guided_grads_vis.max()\n",
    "\n",
    "            plt.figure(figsize=(10,8))\n",
    "            plt.imshow(np.transpose(guided_grads_vis,(1,2,0)))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            # Grayscale Gradients\n",
    "            grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
    "\n",
    "\n",
    "            grayscale_guided_grads_vis = grayscale_guided_grads - grayscale_guided_grads.min()\n",
    "            grayscale_guided_grads_vis /= grayscale_guided_grads_vis.max()\n",
    "\n",
    "            plt.figure(figsize=(10,8))\n",
    "            plt.imshow(np.squeeze(np.transpose(grayscale_guided_grads_vis,(1,2,0))),cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            # plt.savefig('./dev/ccn_figures/poster/saliency_image.png',bbox_inches='tight', dpi=300) \n",
    "            # plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ## Compute saliency maps\n",
    "            print(grayscale_guided_grads_vis.shape)\n",
    "            viz_image=grayscale_guided_grads_vis[0]\n",
    "            ####################################################################################################\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ####################################################################################################\n",
    "            ## Compute saliency scores\n",
    "            mask_img_path_fg, mask_img_path_bg, list_gauss=mask_renderer(torch.load(record[img_num]),140)\n",
    "            mask_img_path_fg=np.array(mask_img_path_fg)\n",
    "            mask_img_path_bg=np.array(mask_img_path_bg)\n",
    "            \n",
    "            plt.figure(figsize=(20,8))\n",
    "            plt.subplot(1,2,1)\n",
    "            plt.imshow(mask_img_path_fg,cmap='gray')\n",
    "            \n",
    "            plt.subplot(1,2,2)\n",
    "            plt.imshow(mask_img_path_bg,cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            ## Plotting Gradients for contour and background elements\n",
    "            sns.distplot(viz_image[np.where(mask_img_path_fg==255)],label='distribution of fg')\n",
    "            sns.distplot(viz_image[np.where(mask_img_path_bg==255)],label='distribution of bg')\n",
    "            plt.title('Distribution of gradients')\n",
    "            plt.legend()\n",
    "            plt.gca().spines['top'].set_visible(False)\n",
    "            plt.gca().spines['right'].set_visible(False)\n",
    "            plt.show()\n",
    "    \n",
    "\n",
    "            \n",
    "            \n",
    "            saliency_score_diffmean=compute_diff_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "            saliency_score_dprime=compute_dprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "            saliency_score_aprime=compute_aprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "            \n",
    "            \n",
    "            print('saliency_score_diffmean: \\t', saliency_score_diffmean)\n",
    "            print('saliency_score_diffmean: \\t', saliency_score_dprime)\n",
    "            print('saliency_score_diffmean: \\t',saliency_score_aprime)\n",
    "            ####################################################################################################\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Analysis (Saliency) over all images from the validation set (also you will have to pre select the model). \n",
    "\n",
    "This analysis takes roughly 25 minutes to complete on 600 validation images when run on the contour model reading out from avgpool layer of Alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=get_B,get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "# val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=1, num_workers=num_workers, shuffle=False)\n",
    "print(len(val_dataset_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the model from the saved model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,file in enumerate(model_file_paths):\n",
    "    print(i,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_file=model_file_paths[0]\n",
    "print(selected_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Saliency for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency(prep_img,prep_recorder,file):\n",
    "    prep_img=Variable(prep_img,requires_grad=True).to(device)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Loading the model and setting up the Guided Spliced Model for Attention Maps   \n",
    "    ## Model\n",
    "    checkpoint=torch.load(selected_file)\n",
    "    \n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    GBP = Spliced_GuidedBackprop(loaded_spliced_model,device)\n",
    "    ####################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ####################################################################################################\n",
    "    ## Getting the Saliency Maps\n",
    "    # Get gradients\n",
    "    guided_grads = GBP.generate_gradients(prep_img, 1)\n",
    "    # Gradients\n",
    "    guided_grads_vis = guided_grads - guided_grads.min()\n",
    "    guided_grads_vis /= guided_grads_vis.max()\n",
    "\n",
    "    # Grayscale Gradients\n",
    "    grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
    "    grayscale_guided_grads_vis = grayscale_guided_grads - grayscale_guided_grads.min()\n",
    "    grayscale_guided_grads_vis /= grayscale_guided_grads_vis.max()\n",
    "    \n",
    "    \n",
    "    viz_image=grayscale_guided_grads_vis[0]\n",
    "    ####################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Compute saliency scores\n",
    "    mask_img_path_fg, mask_img_path_bg, list_gauss=mask_renderer(torch.load(prep_recorder),140)\n",
    "    mask_img_path_fg=np.array(mask_img_path_fg)\n",
    "    mask_img_path_bg=np.array(mask_img_path_bg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    saliency_score_diffmean=compute_diff_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    saliency_score_dprime=compute_dprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    saliency_score_aprime=compute_aprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Making the run more memory efficient\n",
    "    loaded_spliced_model=loaded_spliced_model.to('cpu')\n",
    "    del GBP\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    return saliency_score_diffmean,saliency_score_dprime,saliency_score_aprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_saliency_score_diff=[]\n",
    "all_saliency_score_dprime=[]\n",
    "all_saliency_score_aprime=[]\n",
    "\n",
    "all_betas=[]\n",
    "all_labels=[]\n",
    "\n",
    "for (a, b, d, alpha, nel, labels, record) in tqdm(val_loader_norm):\n",
    "    for img_num in tqdm(range(a.shape[0]),disable=True):\n",
    "        \n",
    "        prep_img=torch.unsqueeze(a[img_num],0)\n",
    "        prep_recorder=record[img_num]\n",
    "        \n",
    "        saliency_score_diffmean,saliency_score_dprime,saliency_score_aprime=compute_saliency(prep_img,prep_recorder,selected_file)\n",
    "        \n",
    "        all_saliency_score_diff.append(saliency_score_diffmean)\n",
    "        all_saliency_score_dprime.append(saliency_score_dprime)\n",
    "        all_saliency_score_aprime.append(saliency_score_aprime)\n",
    "        \n",
    "        \n",
    "        \n",
    "    all_betas.append(b)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "all_betas=torch.cat(all_betas).numpy()\n",
    "all_labels=torch.cat(all_labels).numpy()\n",
    "all_saliency_score_diff=np.array(all_saliency_score_diff)\n",
    "all_saliency_score_dprime=np.array(all_saliency_score_dprime)\n",
    "all_saliency_score_aprime=np.array(all_saliency_score_aprime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_present_pos=np.where(all_labels==1)[0]\n",
    "contour_absent_pos=np.where(all_labels==0)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment Sensitivity via Diff. of Means Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "# Create a GridSpec object\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "ax1 = plt.subplot(gs[:, 0])\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "ax3 = plt.subplot(gs[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "# Scatter Plot with Connecting Lines\n",
    "for x1, x2 in zip(all_saliency_score_diff[contour_absent_pos], all_saliency_score_diff[contour_present_pos]):\n",
    "    ax1.plot([1, 4], [x1, x2], 'ko-', markersize=8,alpha=0.09)\n",
    "ax1.set_xticks([1, 4], ['Contour absent', 'Contour present'])\n",
    "ax1.set_xlim(0,5)\n",
    "ax1.set_ylabel('saliency_score_diffmean')\n",
    "ax1.set_title('Contour Sensitivity using Diff. Means')\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "sns.distplot(all_saliency_score_diff[contour_present_pos], ax=ax2, label='contour_present',color='green')\n",
    "sns.distplot(all_saliency_score_diff[contour_absent_pos], ax=ax2, label='contour_absent',color='gray')\n",
    "ax2.set_title('Contour Sensitivity using Diff. Means (Histogram)')\n",
    "ax2.legend()\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "sns.distplot(all_saliency_score_diff[contour_present_pos] - all_saliency_score_diff[contour_absent_pos], ax=ax3, label='difference',color='k')\n",
    "ax3.set_title('Alignment Sensitivity using Diff. Means')\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Perform the paired t-test\n",
    "t_statistic, p_value = stats.ttest_rel(all_saliency_score_diff[contour_present_pos], all_saliency_score_diff[contour_absent_pos])\n",
    "print(f\"t-statistic: {t_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment Sensitivity via DPrime Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "# Create a GridSpec object\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "ax1 = plt.subplot(gs[:, 0])\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "ax3 = plt.subplot(gs[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "# Scatter Plot with Connecting Lines\n",
    "for x1, x2 in zip(all_saliency_score_dprime[contour_absent_pos], all_saliency_score_dprime[contour_present_pos]):\n",
    "    ax1.plot([1, 4], [x1, x2], 'ko-', markersize=8,alpha=0.09)\n",
    "ax1.set_xticks([1, 4], ['Contour absent', 'Contour present'])\n",
    "ax1.set_xlim(0,5)\n",
    "ax1.set_ylabel('saliency_score_dprime')\n",
    "ax1.set_title('Contour Sensitivity using DPrime')\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "sns.distplot(all_saliency_score_dprime[contour_present_pos], ax=ax2, label='contour_present',color='green')\n",
    "sns.distplot(all_saliency_score_dprime[contour_absent_pos], ax=ax2, label='contour_absent',color='gray')\n",
    "ax2.set_title('Contour Sensitivity using DPrime (Histogram)')\n",
    "ax2.legend()\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "sns.distplot(all_saliency_score_dprime[contour_present_pos] - all_saliency_score_dprime[contour_absent_pos], ax=ax3, label='difference',color='k')\n",
    "ax3.set_title('Alignment Sensitivity using DPrime')\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Perform the paired t-test\n",
    "t_statistic, p_value = stats.ttest_rel(all_saliency_score_dprime[contour_present_pos], all_saliency_score_dprime[contour_absent_pos])\n",
    "print(f\"t-statistic: {t_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment Sensitivity via APrime Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "# Create a GridSpec object\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "ax1 = plt.subplot(gs[:, 0])\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "ax3 = plt.subplot(gs[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "# Scatter Plot with Connecting Lines\n",
    "for x1, x2 in zip(all_saliency_score_aprime[contour_absent_pos], all_saliency_score_aprime[contour_present_pos]):\n",
    "    ax1.plot([1, 4], [x1, x2], 'ko-', markersize=8,alpha=0.09)\n",
    "ax1.set_xticks([1, 4], ['Contour absent', 'Contour present'])\n",
    "ax1.set_xlim(0,5)\n",
    "ax1.set_ylabel('saliency_score_aprime')\n",
    "ax1.set_title('Contour Sensitivity using APrime')\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "sns.distplot(all_saliency_score_aprime[contour_present_pos], ax=ax2, label='contour_present',color='green')\n",
    "sns.distplot(all_saliency_score_aprime[contour_absent_pos], ax=ax2, label='contour_absent',color='gray')\n",
    "ax2.set_title('Contour Sensitivity using APrime (Histogram)')\n",
    "ax2.legend()\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "sns.distplot(all_saliency_score_aprime[contour_present_pos] - all_saliency_score_aprime[contour_absent_pos], ax=ax3, label='difference',color='k')\n",
    "ax3.set_title('Alignment Sensitivity using APrime')\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Perform the paired t-test\n",
    "t_statistic, p_value = stats.ttest_rel(all_saliency_score_aprime[contour_present_pos], all_saliency_score_aprime[contour_absent_pos])\n",
    "print(f\"t-statistic: {t_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
