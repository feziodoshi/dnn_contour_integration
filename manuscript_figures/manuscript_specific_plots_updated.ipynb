{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated sept16\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "sys.path.insert(1,'../contour_integ_models/')\n",
    "\n",
    "\n",
    "# Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Pytorch related\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils import data as dt\n",
    "# from torchinfo import summary\n",
    "import torchvision.models as pretrained_models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils import model_zoo\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "# Numpy, Matplotlib, Pandas, Sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from sklearn import manifold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "# python utilities\n",
    "from itertools import combinations\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import copy\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "from IPython.display import Image\n",
    "from IPython.core.debugger import set_trace\n",
    "import collections\n",
    "from functools import partial\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "\n",
    "from PIL import Image, ImageStat\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "# Extra imports\n",
    "from lib.feature_extractor import FeatureExtractor\n",
    "from lib.custom_dataset import Contour_Dataset, Psychophysics_Dataset\n",
    "from lib.build_fe_ft_models import *\n",
    "from lib.misc_functions import *\n",
    "from lib.field_stim_functions import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "img_dim=512\n",
    "\n",
    "batch_size=32\n",
    "num_workers=8\n",
    "device = torch.device('cuda:'+'0')\n",
    "# device = 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all the values from the config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Visual Diet config \n",
      "\n",
      "{'root_directory': '/n/alvarez_lab_tier1/Users/fdoshi/Datasets/contour_dataset/home/jovyan/work/Datasets/contour_integration/model-training/config_0', 'get_B': [0, 15, 30, 45, 60, 75], 'get_D': [32], 'get_A': [0], 'get_numElements': [12]}\n",
      "\n",
      " Psychophysics Visual Diet config \n",
      "\n",
      "{'root_directory': '/home/jovyan/work/Datasets/contour_integration/model-psychophysics/experiment_1/', 'get_B': [15, 30, 45, 60, 75], 'get_D': [32], 'get_A': [0], 'get_numElements': [12]}\n"
     ]
    }
   ],
   "source": [
    "from visualdiet_savedmodel_config import *\n",
    "\n",
    "print('\\n Visual Diet config \\n')\n",
    "print(visual_diet_config)\n",
    "\n",
    "\n",
    "print('\\n Psychophysics Visual Diet config \\n')\n",
    "print(psychophysics_visual_diet_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory=visual_diet_config['root_directory']\n",
    "get_B=visual_diet_config['get_B']\n",
    "get_D=visual_diet_config['get_D']\n",
    "get_A=visual_diet_config['get_A']\n",
    "get_numElements=visual_diet_config['get_numElements']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize images using parameters from the training image set\n",
    "data_transform = transforms.Compose([       \n",
    " transforms.Resize(img_dim),                   \n",
    " transforms.CenterCrop((img_dim,img_dim)),         \n",
    " transforms.ToTensor(),                    \n",
    " transforms.Normalize(                      \n",
    " mean=[0.485, 0.456, 0.406],                \n",
    " std=[0.229, 0.224, 0.225]                  \n",
    " )])\n",
    "\n",
    "data_transform_without_norm = transforms.Compose([       \n",
    " transforms.Resize(img_dim),                   \n",
    " transforms.CenterCrop((img_dim,img_dim)),         \n",
    " transforms.ToTensor()                    \n",
    " ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diff_means(arr1, arr2):\n",
    "    mean1 = np.mean(arr1)\n",
    "    mean2 = np.mean(arr2)\n",
    "    \n",
    "    return (mean1 - mean2)\n",
    "\n",
    "def compute_dprime_means(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Compute d' based on the difference of means divided by the pooled standard deviation.\n",
    "    \"\"\"\n",
    "    mean1 = np.mean(arr1)\n",
    "    mean2 = np.mean(arr2)\n",
    "    \n",
    "    var1 = np.var(arr1, ddof=1)  # ddof=1 for sample variance\n",
    "    var2 = np.var(arr2, ddof=1)  # ddof=1 for sample variance\n",
    "    \n",
    "    # Calculate the pooled standard deviation\n",
    "    pooled_std = np.sqrt((var1 + var2) / 2)\n",
    "    \n",
    "    # Compute d'\n",
    "    d_prime = (mean1 - mean2) / pooled_std\n",
    "    \n",
    "    return d_prime\n",
    "\n",
    "def compute_aprime_means(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Compute A' as the area under the ROC curve based on two input arrays.\n",
    "    \"\"\"\n",
    "    # Combine the arrays and create corresponding labels\n",
    "    combined = np.concatenate((arr1, arr2))\n",
    "    labels = np.concatenate((np.ones(len(arr1)), np.zeros(len(arr2))))\n",
    "    \n",
    "    # Compute AUC\n",
    "    a_prime = roc_auc_score(labels, combined)\n",
    "    \n",
    "    return a_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_acc(files):\n",
    "    all_models_train_acc={}\n",
    "    all_models_val_acc={}\n",
    "\n",
    "    for f in files:\n",
    "        # checkpoint=torch.load(f)\n",
    "        # checkpoint = torch.load(f, map_location=torch.device('cuda:0'))\n",
    "        checkpoint = torch.load(f, map_location='cpu')\n",
    "        \n",
    "        all_models_train_acc[checkpoint['training_config']['layer_name']]=checkpoint['metrics']['train_acc'][-1]\n",
    "        all_models_val_acc[checkpoint['training_config']['layer_name']]=checkpoint['metrics']['val_acc'][-1]\n",
    "\n",
    "\n",
    "\n",
    "    myKeys = list(all_models_train_acc.keys())\n",
    "    myKeys.sort()\n",
    "    all_models_train_acc = {i: all_models_train_acc[i] for i in myKeys}\n",
    "\n",
    "\n",
    "\n",
    "    myKeys = list(all_models_val_acc.keys())\n",
    "    myKeys.sort()\n",
    "    all_models_val_acc = {i: all_models_val_acc[i] for i in myKeys}\n",
    "    \n",
    "    \n",
    "    return all_models_train_acc,all_models_val_acc\n",
    "\n",
    "def get_list_acc(acc_dict,list_layers):\n",
    "    acc=[]\n",
    "    for i in range(len(list_layers)):\n",
    "        # layer=list_layers[i].replace('_','.')\n",
    "        layer=list_layers[i]\n",
    "        if layer in acc_dict.keys():\n",
    "            acc.append(acc_dict[layer])\n",
    "            \n",
    "    return acc\n",
    "\n",
    "\n",
    "def get_prediction(model_file_paths):\n",
    "    model_valacc_scores={}\n",
    "    for file in tqdm(model_file_paths):\n",
    "\n",
    "        # Model\n",
    "        # checkpoint=torch.load(file)\n",
    "        # loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device='cpu')\n",
    "        \n",
    "        checkpoint=torch.load(file, map_location='cpu')\n",
    "        loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "        loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "        \n",
    "        change_train_eval_mode(loaded_spliced_model,loaded_spliced_model.fine_tune,train_eval_mode='eval')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        model_valacc_scores[checkpoint['training_config']['layer_name']]=np.array([])\n",
    "        for (inputs, b, d, alpha, nel, labels, record) in tqdm(val_loader_norm,disable=True):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs= loaded_spliced_model.forward(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            model_valacc_scores[checkpoint['training_config']['layer_name']]=np.concatenate((model_valacc_scores[checkpoint['training_config']['layer_name']],(preds == labels).float().cpu().numpy()))\n",
    "    \n",
    "    \n",
    "    return model_valacc_scores\n",
    "\n",
    "def get_list_acc_mean_std_ci(sal_dict,list_layers):\n",
    "    mean_sal=[]\n",
    "    std_sal=[]\n",
    "    confint_sal=[]\n",
    "    \n",
    "    \n",
    "    confint_lower=[]\n",
    "    confint_upper=[]\n",
    "    \n",
    "    for i in range(len(list_layers)):\n",
    "        # layer=list_layers[i].replace('_','.')\n",
    "        layer=list_layers[i]\n",
    "        if layer in sal_dict.keys():\n",
    "            mean_sal.append(np.mean(sal_dict[layer]))\n",
    "            \n",
    "            std_sal.append(np.std(sal_dict[layer],ddof=1))\n",
    "            \n",
    "            confint_sal.append(1.96*(np.std(sal_dict[layer],ddof=1) / np.sqrt(len(sal_dict[layer]))))\n",
    "            \n",
    "            confint_lower.append(np.percentile(sal_dict[layer], 2.5))\n",
    "            confint_upper.append(np.percentile(sal_dict[layer], 97.5))\n",
    "            \n",
    "    return mean_sal,std_sal,confint_sal,confint_lower,confint_upper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1 style=\"color:red\">All Figures</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"color:red\">Figure : Fig. 2A</h1>\n",
    "\n",
    "## Description: Contour Readout Accuracies for Finetune, Frozen and Random Alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the dataset and set of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=get_B,get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the accuracy data for all models over the validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_regular_finetune_files = glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/*finetune.pt\")\n",
    "sup_regular_finetune_acc={'train':get_train_val_acc(sup_regular_finetune_files)[0],'val':get_train_val_acc(sup_regular_finetune_files)[1]}\n",
    "sup_regular_finetune_predictions=get_prediction(sup_regular_finetune_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_regular_frozen_files =  glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_frozen_broad/*frozen.pt\")\n",
    "sup_regular_frozen_acc={'train':get_train_val_acc(sup_regular_frozen_files)[0],'val':get_train_val_acc(sup_regular_frozen_files)[1]}\n",
    "sup_regular_frozen_predictions=get_prediction(sup_regular_frozen_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_regular_frozen_files =  glob.glob(\"../model_weights/contour_model_weights/alexnet-random-nodata-notask_frozen_broad/*frozen.pt\")\n",
    "random_regular_frozen_acc={'train':get_train_val_acc(random_regular_frozen_files)[0],'val':get_train_val_acc(random_regular_frozen_files)[1]}\n",
    "random_regular_frozen_predictions=get_prediction(random_regular_frozen_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting all the accuracy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_layers=['features_0',\n",
    " 'features_1',\n",
    " 'features_2',\n",
    " 'features_3',\n",
    " 'features_4',\n",
    " 'features_5',\n",
    " 'features_6',\n",
    " 'features_7',\n",
    " 'features_8',\n",
    " 'features_9',\n",
    " 'features_10',\n",
    " 'features_11',\n",
    " 'features_12',\n",
    " 'avgpool',\n",
    " 'classifier_0',\n",
    " 'classifier_1',\n",
    " 'classifier_2',\n",
    " 'classifier_3',\n",
    " 'classifier_4',\n",
    " 'classifier_5',\n",
    " 'classifier_6']\n",
    "\n",
    "for i in range(len(list_layers)):\n",
    "    list_layers[i]=list_layers[i].replace('_','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "\n",
    "\n",
    "spacing_x=2\n",
    "jitter_stylized=0.4\n",
    "\n",
    "color_or_finetune=(78/255,121/255,180/255)\n",
    "color_or_frozen=(209/255,111/255,28/255)\n",
    "color_stylized_frozen=(231/255,168/255,34/255)\n",
    "\n",
    "error_bar_width=3\n",
    "error_bar_cap=4\n",
    "\n",
    "\n",
    "\n",
    "mean_sal,std_sal,confint_sal,confint_lower,confint_upper=get_list_acc_mean_std_ci(sup_regular_finetune_predictions,list_layers)\n",
    "plt.errorbar(np.arange(len(mean_sal)) * spacing_x, mean_sal, \n",
    "             yerr=confint_sal, \n",
    "             capsize=error_bar_cap,elinewidth=error_bar_width,fmt='o', marker='o', color=color_or_finetune, markersize=10, \n",
    "             label='or finetune', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mean_sal,std_sal,confint_sal,confint_lower,confint_upper=get_list_acc_mean_std_ci(sup_regular_frozen_predictions,list_layers)\n",
    "plt.errorbar(np.arange(len(mean_sal)) * spacing_x, mean_sal, \n",
    "             yerr=confint_sal, \n",
    "             capsize=error_bar_cap,elinewidth=error_bar_width,fmt='s', marker='o', color=color_or_frozen, markersize=10, \n",
    "             label='or frozen', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "mean_sal,std_sal,confint_sal,confint_lower,confint_upper=get_list_acc_mean_std_ci(random_regular_frozen_predictions,list_layers)\n",
    "plt.errorbar(np.arange(len(mean_sal)) * spacing_x, mean_sal, \n",
    "             yerr=confint_sal, \n",
    "             capsize=error_bar_cap,elinewidth=error_bar_width,fmt='d', marker='o', color='gray', markersize=10, \n",
    "             label='random frozen', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "plt.axhline(y=0.5,linestyle='--',alpha=0.3,color='r',label='chance')\n",
    "\n",
    "\n",
    "# for i in range(len(get_list_acc(sup_regular_finetune_acc['val'],list_layers))):\n",
    "#     plt.axvline(i*2,linestyle='--',alpha=0.1,color='k')\n",
    "\n",
    "plt.xticks(np.arange(len(get_list_acc(sup_regular_finetune_acc['val'],list_layers))) * spacing_x,list_layers,rotation=90,fontsize=15)\n",
    "plt.yticks(fontsize=15) \n",
    "plt.ylim(0.4,1.01)\n",
    "plt.ylabel('Validation Accuracy',fontsize=15)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"color:red\">Figure : Fig. 2B and SFig. 2A </h1>\n",
    "\n",
    "## Description: Location Sensitivity for avgpool and conv2 finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(model_file, beta_val=15,img_num=4):\n",
    "    \n",
    "    \n",
    "    ### Show the image\n",
    "    val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=[beta_val],get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "    val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    val_dataset_without_norm = Contour_Dataset(root=root_directory,transform=data_transform_without_norm,train=False,get_B=[beta_val],get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "    val_loader_without_norm = torch.utils.data.DataLoader(dataset=val_dataset_without_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    a, b, d, alpha, nel, labels, record =next(iter(val_loader_norm))\n",
    "    prep_img=torch.unsqueeze(a[img_num],0)\n",
    "    print('Beta Val: ',b[img_num])\n",
    "    print('Contour Present: ',labels[img_num])\n",
    "    prep_img=prep_img.to(device)\n",
    "    prep_img=Variable(prep_img,requires_grad=True)\n",
    "    \n",
    "    ## Plot the orginal image - this is the normalized one, the one that will actually be used\n",
    "    # original_img=np.transpose(prep_img[0].detach().cpu().numpy(),(1,2,0))\n",
    "    # plt.figure(figsize=(10,8))\n",
    "    # plt.imshow(original_img)\n",
    "    # plt.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Plot the orginal unnormalized image\n",
    "    plt.figure(figsize=(10,8))\n",
    "    a_without_norm,b, d, alpha, nel, labels, record=next(iter(val_loader_without_norm))\n",
    "    plt.imshow(np.transpose(torch.unsqueeze(a_without_norm[img_num],0)[0].numpy(),(1,2,0)))\n",
    "    plt.title('Original Image - visualizing unnormalized image here')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    # plt.savefig('./dev/ccn_figures/poster/original_image.png', bbox_inches='tight',dpi=300) \n",
    "    # plt.close()\n",
    "    \n",
    "\n",
    "    checkpoint=torch.load(model_file)\n",
    "\n",
    "    ####################################################################################################\n",
    "    ## Showing the Model Stats\n",
    "    print('Base Model is',checkpoint['training_config']['base_model_name'])\n",
    "    print('Layer Readout is',checkpoint['training_config']['layer_name'])\n",
    "    print('Final Val Accuracy is',checkpoint['metrics']['val_acc'][-1])\n",
    "    ####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ####################################################################################################\n",
    "    ## Loading the model and setting up the Guided Spliced Model for Attention Maps\n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    GBP = Spliced_GuidedBackprop(loaded_spliced_model,device)\n",
    "    ####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ####################################################################################################\n",
    "    ## Getting the Saliency Maps\n",
    "    # Get gradients\n",
    "    guided_grads = GBP.generate_gradients(prep_img, 1)\n",
    "\n",
    "\n",
    "    # Gradients\n",
    "    guided_grads_vis = guided_grads - guided_grads.min()\n",
    "    guided_grads_vis /= guided_grads_vis.max()\n",
    "\n",
    "#     plt.figure(figsize=(20,16))\n",
    "    \n",
    "    \n",
    "#     plt.subplot(2,2,3)\n",
    "#     plt.imshow(np.transpose(guided_grads_vis,(1,2,0)))\n",
    "#     plt.axis('off')\n",
    "\n",
    "\n",
    "    # Grayscale Gradients\n",
    "    grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
    "\n",
    "\n",
    "    grayscale_guided_grads_vis = grayscale_guided_grads - grayscale_guided_grads.min()\n",
    "    grayscale_guided_grads_vis /= grayscale_guided_grads_vis.max()\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(20,16))\n",
    "    \n",
    "    \n",
    "    plt.subplot(2,2,3)\n",
    "    plt.imshow(np.transpose(guided_grads_vis,(1,2,0)))\n",
    "    plt.title('Saliency Map overlayed on image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2,2,4)\n",
    "    plt.imshow(np.squeeze(np.transpose(grayscale_guided_grads_vis,(1,2,0))),cmap='gray')\n",
    "    plt.title('Saliency Map')\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "    ## Compute saliency maps\n",
    "    print(grayscale_guided_grads_vis.shape)\n",
    "    viz_image=grayscale_guided_grads_vis[0]\n",
    "    ####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ####################################################################################################\n",
    "    ## Compute saliency scores\n",
    "    mask_img_path_fg, mask_img_path_bg, list_gauss=mask_renderer(torch.load(record[img_num]),140)\n",
    "    mask_img_path_fg=np.array(mask_img_path_fg)\n",
    "    mask_img_path_bg=np.array(mask_img_path_bg)\n",
    "\n",
    "    \n",
    "    plt.subplot(2,2,1)\n",
    "    plt.imshow(mask_img_path_fg,cmap='gray')\n",
    "    plt.title('Contour Element Locations')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.imshow(mask_img_path_bg,cmap='gray')\n",
    "    plt.title('Background Element Locations')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    saliency_score_diffmean=compute_diff_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    saliency_score_dprime=compute_dprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    saliency_score_aprime=compute_aprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "\n",
    "\n",
    "    print('saliency_score_diffmean: \\t', saliency_score_diffmean)\n",
    "    print('saliency_score_diffmean: \\t', saliency_score_dprime)\n",
    "    print('saliency_score_diffmean: \\t',saliency_score_aprime)\n",
    "    ####################################################################################################\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(model_file='../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-3_mode_finetune.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(model_file='../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"color:red\"> Figure : Fig. 2D and SFig. 2C-Inset  </h1>\n",
    "\n",
    "## Description: Alignment Sensitivity for avgpool finetuned model\n",
    "This analysis takes roughly **25 minutes** to complete on 600 validation images when run on the contour model reading out from avgpool layer of Alexnet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=get_B,get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "# val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=1, num_workers=num_workers, shuffle=False)\n",
    "print(len(val_dataset_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the model from the saved model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_file='../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune.pt'\n",
    "print(selected_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Saliency for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency(prep_img,prep_recorder,file):\n",
    "    prep_img=Variable(prep_img,requires_grad=True).to(device)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Loading the model and setting up the Guided Spliced Model for Attention Maps   \n",
    "    ## Model\n",
    "    checkpoint=torch.load(selected_file)\n",
    "    \n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    GBP = Spliced_GuidedBackprop(loaded_spliced_model,device)\n",
    "    ####################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ####################################################################################################\n",
    "    ## Getting the Saliency Maps\n",
    "    # Get gradients\n",
    "    guided_grads = GBP.generate_gradients(prep_img, 1)\n",
    "    # Gradients\n",
    "    guided_grads_vis = guided_grads - guided_grads.min()\n",
    "    guided_grads_vis /= guided_grads_vis.max()\n",
    "\n",
    "    # Grayscale Gradients\n",
    "    grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
    "    grayscale_guided_grads_vis = grayscale_guided_grads - grayscale_guided_grads.min()\n",
    "    grayscale_guided_grads_vis /= grayscale_guided_grads_vis.max()\n",
    "    \n",
    "    \n",
    "    viz_image=grayscale_guided_grads_vis[0]\n",
    "    ####################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Compute saliency scores\n",
    "    mask_img_path_fg, mask_img_path_bg, list_gauss=mask_renderer(torch.load(prep_recorder),140)\n",
    "    mask_img_path_fg=np.array(mask_img_path_fg)\n",
    "    mask_img_path_bg=np.array(mask_img_path_bg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    saliency_score_diffmean=compute_diff_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    saliency_score_dprime=compute_dprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    saliency_score_aprime=compute_aprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Making the run more memory efficient\n",
    "    loaded_spliced_model=loaded_spliced_model.to('cpu')\n",
    "    del GBP\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    return saliency_score_diffmean,saliency_score_dprime,saliency_score_aprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_saliency_score_diff=[]\n",
    "all_saliency_score_dprime=[]\n",
    "all_saliency_score_aprime=[]\n",
    "\n",
    "all_betas=[]\n",
    "all_labels=[]\n",
    "\n",
    "for (a, b, d, alpha, nel, labels, record) in tqdm(val_loader_norm):\n",
    "    for img_num in tqdm(range(a.shape[0]),disable=True):\n",
    "        \n",
    "        prep_img=torch.unsqueeze(a[img_num],0)\n",
    "        prep_recorder=record[img_num]\n",
    "        \n",
    "        saliency_score_diffmean,saliency_score_dprime,saliency_score_aprime=compute_saliency(prep_img,prep_recorder,selected_file)\n",
    "        \n",
    "        all_saliency_score_diff.append(saliency_score_diffmean)\n",
    "        all_saliency_score_dprime.append(saliency_score_dprime)\n",
    "        all_saliency_score_aprime.append(saliency_score_aprime)\n",
    "        \n",
    "        \n",
    "        \n",
    "    all_betas.append(b)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "all_betas=torch.cat(all_betas).numpy()\n",
    "all_labels=torch.cat(all_labels).numpy()\n",
    "all_saliency_score_diff=np.array(all_saliency_score_diff)\n",
    "all_saliency_score_dprime=np.array(all_saliency_score_dprime)\n",
    "all_saliency_score_aprime=np.array(all_saliency_score_aprime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_present_pos=np.where(all_labels==1)[0]\n",
    "contour_absent_pos=np.where(all_labels==0)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment Sensitivity via Diff. of Means Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "# Create a GridSpec object\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "ax1 = plt.subplot(gs[:, 0])\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "ax3 = plt.subplot(gs[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "# Scatter Plot with Connecting Lines\n",
    "for x1, x2 in zip(all_saliency_score_diff[contour_absent_pos], all_saliency_score_diff[contour_present_pos]):\n",
    "    ax1.plot([1, 4], [x1, x2], 'ko-', markersize=8,alpha=0.09)\n",
    "ax1.set_xticks([1, 4], ['Contour absent', 'Contour present'])\n",
    "ax1.set_xlim(0,5)\n",
    "ax1.set_ylabel('saliency_score_diffmean')\n",
    "ax1.set_title('Contour Sensitivity using Diff. Means')\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "sns.distplot(all_saliency_score_diff[contour_present_pos], ax=ax2, label='contour_present',color='green')\n",
    "sns.distplot(all_saliency_score_diff[contour_absent_pos], ax=ax2, label='contour_absent',color='gray')\n",
    "ax2.set_title('Contour Sensitivity using Diff. Means (Histogram)')\n",
    "ax2.legend()\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "sns.distplot(all_saliency_score_diff[contour_present_pos] - all_saliency_score_diff[contour_absent_pos], ax=ax3, label='difference',color='k')\n",
    "ax3.set_title('Alignment Sensitivity using Diff. Means')\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Perform the paired t-test\n",
    "t_statistic, p_value = stats.ttest_rel(all_saliency_score_diff[contour_present_pos], all_saliency_score_diff[contour_absent_pos])\n",
    "print(f\"t-statistic: {t_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment Sensitivity via DPrime Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "# Create a GridSpec object\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "ax1 = plt.subplot(gs[:, 0])\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "ax3 = plt.subplot(gs[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "# Scatter Plot with Connecting Lines\n",
    "for x1, x2 in zip(all_saliency_score_dprime[contour_absent_pos], all_saliency_score_dprime[contour_present_pos]):\n",
    "    ax1.plot([1, 4], [x1, x2], 'ko-', markersize=8,alpha=0.09)\n",
    "ax1.set_xticks([1, 4], ['Contour absent', 'Contour present'])\n",
    "ax1.set_xlim(0,5)\n",
    "ax1.set_ylabel('saliency_score_dprime')\n",
    "ax1.set_title('Contour Sensitivity using DPrime')\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "sns.distplot(all_saliency_score_dprime[contour_present_pos], ax=ax2, label='contour_present',color='green')\n",
    "sns.distplot(all_saliency_score_dprime[contour_absent_pos], ax=ax2, label='contour_absent',color='gray')\n",
    "ax2.set_title('Contour Sensitivity using DPrime (Histogram)')\n",
    "ax2.legend()\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "sns.distplot(all_saliency_score_dprime[contour_present_pos] - all_saliency_score_dprime[contour_absent_pos], ax=ax3, label='difference',color='k')\n",
    "ax3.set_title('Alignment Sensitivity using DPrime')\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Perform the paired t-test\n",
    "t_statistic, p_value = stats.ttest_rel(all_saliency_score_dprime[contour_present_pos], all_saliency_score_dprime[contour_absent_pos])\n",
    "print(f\"t-statistic: {t_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment Sensitivity via APrime Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "# Create a GridSpec object\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "ax1 = plt.subplot(gs[:, 0])\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "ax3 = plt.subplot(gs[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "# Scatter Plot with Connecting Lines\n",
    "for x1, x2 in zip(all_saliency_score_aprime[contour_absent_pos], all_saliency_score_aprime[contour_present_pos]):\n",
    "    ax1.plot([1, 4], [x1, x2], 'ko-', markersize=8,alpha=0.09)\n",
    "ax1.set_xticks([1, 4], ['Contour absent', 'Contour present'])\n",
    "ax1.set_xlim(0,5)\n",
    "ax1.set_ylabel('saliency_score_aprime')\n",
    "ax1.set_title('Contour Sensitivity using APrime')\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "sns.distplot(all_saliency_score_aprime[contour_present_pos], ax=ax2, label='contour_present',color='green')\n",
    "sns.distplot(all_saliency_score_aprime[contour_absent_pos], ax=ax2, label='contour_absent',color='gray')\n",
    "ax2.set_title('Contour Sensitivity using APrime (Histogram)')\n",
    "ax2.legend()\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "sns.distplot(all_saliency_score_aprime[contour_present_pos] - all_saliency_score_aprime[contour_absent_pos], ax=ax3, label='difference',color='k')\n",
    "ax3.set_title('Alignment Sensitivity using APrime')\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Perform the paired t-test\n",
    "t_statistic, p_value = stats.ttest_rel(all_saliency_score_aprime[contour_present_pos], all_saliency_score_aprime[contour_absent_pos])\n",
    "print(f\"t-statistic: {t_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"color:red\"> Figure : SFig. 2B and SFig. 2C   </h1>\n",
    "\n",
    "## Description: Location and Alignment Sensitivity for all layers of finetuned alexnet over broad curvatures\n",
    "This analysis takes roughly **8 hours** to complete on 600 validation images when run on all contour readout models (with object recognition alexnet finetuned backbones and trained on braod contour curvatures) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the dataset and set of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=get_B,get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "print(len(val_dataset_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_paths=sorted(glob.glob('../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/*.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,file in enumerate(model_file_paths):\n",
    "    print(i,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Saliency for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency(prep_img,prep_recorder,file):\n",
    "    prep_img=Variable(prep_img,requires_grad=True).to(device)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Loading the model and setting up the Guided Spliced Model for Attention Maps   \n",
    "    ## Model\n",
    "    checkpoint=torch.load(selected_file)\n",
    "    \n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    GBP = Spliced_GuidedBackprop(loaded_spliced_model,device)\n",
    "    ####################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ####################################################################################################\n",
    "    ## Getting the Saliency Maps\n",
    "    # Get gradients\n",
    "    guided_grads = GBP.generate_gradients(prep_img, 1)\n",
    "    # Gradients\n",
    "    guided_grads_vis = guided_grads - guided_grads.min()\n",
    "    guided_grads_vis /= guided_grads_vis.max()\n",
    "\n",
    "    # Grayscale Gradients\n",
    "    grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
    "    grayscale_guided_grads_vis = grayscale_guided_grads - grayscale_guided_grads.min()\n",
    "    grayscale_guided_grads_vis /= grayscale_guided_grads_vis.max()\n",
    "    \n",
    "    \n",
    "    viz_image=grayscale_guided_grads_vis[0]\n",
    "    ####################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Compute saliency scores\n",
    "    mask_img_path_fg, mask_img_path_bg, list_gauss=mask_renderer(torch.load(prep_recorder),140)\n",
    "    mask_img_path_fg=np.array(mask_img_path_fg)\n",
    "    mask_img_path_bg=np.array(mask_img_path_bg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    saliency_score_diffmean=compute_diff_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    saliency_score_dprime=compute_dprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    saliency_score_aprime=compute_aprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Making the run more memory efficient\n",
    "    loaded_spliced_model=loaded_spliced_model.to('cpu')\n",
    "    del GBP\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    return saliency_score_diffmean,saliency_score_dprime,saliency_score_aprime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the loop - All models, All images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saliency_score_diff={}\n",
    "model_saliency_score_dprime={}\n",
    "model_saliency_score_aprime={}\n",
    "\n",
    "\n",
    "for file in tqdm(model_file_paths):\n",
    "    print(file)\n",
    "    checkpoint=torch.load(file)\n",
    "    \n",
    "    model_saliency_score_diff[checkpoint['training_config']['layer_name']]=[]\n",
    "    model_saliency_score_dprime[checkpoint['training_config']['layer_name']]=[]\n",
    "    model_saliency_score_aprime[checkpoint['training_config']['layer_name']]=[]\n",
    "    \n",
    "    for (a, b, d, alpha, nel, labels, record) in tqdm(val_loader_norm,disable=True):\n",
    "        \n",
    "        \n",
    "        for img_num in tqdm(range(a.shape[0]),disable=True):\n",
    "            prep_img=torch.unsqueeze(a[img_num],0)\n",
    "            prep_recorder=record[img_num]\n",
    "            \n",
    "            saliency_score_diffmean,saliency_score_dprime,saliency_score_aprime=compute_saliency(prep_img,prep_recorder,file)\n",
    "\n",
    "            model_saliency_score_diff[checkpoint['training_config']['layer_name']].append(saliency_score_diffmean)\n",
    "            model_saliency_score_dprime[checkpoint['training_config']['layer_name']].append(saliency_score_dprime)\n",
    "            model_saliency_score_aprime[checkpoint['training_config']['layer_name']].append(saliency_score_aprime)\n",
    "            \n",
    "        \n",
    "        \n",
    "    model_saliency_score_diff[checkpoint['training_config']['layer_name']]=np.array(model_saliency_score_diff[checkpoint['training_config']['layer_name']])\n",
    "    model_saliency_score_dprime[checkpoint['training_config']['layer_name']]=np.array(model_saliency_score_dprime[checkpoint['training_config']['layer_name']])\n",
    "    model_saliency_score_aprime[checkpoint['training_config']['layer_name']]=np.array(model_saliency_score_aprime[checkpoint['training_config']['layer_name']])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_betas=[]\n",
    "all_labels=[]\n",
    "\n",
    "for (a, b, d, alpha, nel, labels, record) in tqdm(val_loader_norm,disable=True):\n",
    "    all_betas.append(b)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "    \n",
    "all_betas=torch.cat(all_betas).numpy()\n",
    "all_labels=torch.cat(all_labels).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_saliency_score_diff.keys())\n",
    "print(model_saliency_score_dprime.keys())\n",
    "print(model_saliency_score_aprime.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_present_pos=np.where(all_labels==1)[0]\n",
    "contour_absent_pos=np.where(all_labels==0)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting for all readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foreground_sensitivity_allmodels(sal_dict,list_layers,pos):\n",
    "    mean_sal=[]\n",
    "    std_sal=[]\n",
    "    confint_sal=[]\n",
    "    confint_lower=[]\n",
    "    confint_upper=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(list_layers)):\n",
    "        layer=list_layers[i]\n",
    "        \n",
    "        \n",
    "        if layer in sal_dict.keys():\n",
    "            \n",
    "            \n",
    "            mean_sal.append(np.mean(sal_dict[layer][pos]))\n",
    "            std_sal.append(np.std(sal_dict[layer][pos],ddof=1))\n",
    "            \n",
    "            \n",
    "            \n",
    "            confint_sal.append(1.96*(np.std(sal_dict[layer][pos],ddof=1) / np.sqrt(len(sal_dict[layer][pos]))))\n",
    "            confint_lower.append(np.percentile(sal_dict[layer][pos], 2.5))\n",
    "            confint_upper.append(np.percentile(sal_dict[layer][pos], 97.5))\n",
    "            \n",
    "    return mean_sal,std_sal,confint_sal,confint_lower,confint_upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alignment_sensitivity_allmodels(sal_dict,list_layers,present_pos,absent_pos):\n",
    "    \n",
    "    mean_sal=[]\n",
    "    std_sal=[]\n",
    "    confint_sal=[]\n",
    "    confint_lower=[]\n",
    "    confint_upper=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(list_layers)):\n",
    "        layer=list_layers[i]\n",
    "        \n",
    "        \n",
    "        if layer in sal_dict.keys():\n",
    "            \n",
    "            \n",
    "            diff_values=sal_dict[layer][present_pos] - sal_dict[layer][absent_pos]\n",
    "            \n",
    "            mean_sal.append(np.mean(diff_values))\n",
    "            std_sal.append(np.std(diff_values))\n",
    "            \n",
    "            \n",
    "            \n",
    "            confint_sal.append(1.96*(np.std(diff_values) / np.sqrt(len(diff_values))))\n",
    "            confint_lower.append(np.percentile(diff_values, 2.5))\n",
    "            confint_upper.append(np.percentile(diff_values, 97.5))\n",
    "            \n",
    "    return mean_sal,std_sal,confint_sal,confint_lower,confint_upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_layers=['features_0',\n",
    " 'features_1',\n",
    " 'features_2',\n",
    " 'features_3',\n",
    " 'features_4',\n",
    " 'features_5',\n",
    " 'features_6',\n",
    " 'features_7',\n",
    " 'features_8',\n",
    " 'features_9',\n",
    " 'features_10',\n",
    " 'features_11',\n",
    " 'features_12',\n",
    " 'avgpool',\n",
    " 'classifier_0',\n",
    " 'classifier_1',\n",
    " 'classifier_2',\n",
    " 'classifier_3',\n",
    " 'classifier_4',\n",
    " 'classifier_5',\n",
    " 'classifier_6']\n",
    "\n",
    "for i in range(len(list_layers)):\n",
    "    list_layers[i]=list_layers[i].replace('_','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_present_pos=np.where(all_labels==1)[0]\n",
    "contour_absent_pos=np.where(all_labels==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sal,std_sal,confint_sal,confint_lower,confint_upper=get_foreground_sensitivity_allmodels(model_saliency_score_diff,list_layers,contour_present_pos)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(22,10))\n",
    "color_or_finetune=(78/255,121/255,180/255)\n",
    "spacing_x=2\n",
    "error_bar_width=3\n",
    "error_bar_cap=4\n",
    "\n",
    "plt.errorbar(np.arange(len(mean_sal)) * spacing_x, mean_sal, yerr=confint_sal, \n",
    "             capsize=error_bar_cap,elinewidth=error_bar_width,fmt='o',marker='o', color=color_or_finetune, markersize=10, \n",
    "             label='or finetune', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(mean_sal)) * spacing_x,list_layers,rotation=90,fontsize=15)\n",
    "plt.yticks(fontsize=15) \n",
    "\n",
    "\n",
    "plt.ylabel('Diff of means',fontsize=15)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title('Location Sensitivity in Contour Images using Diff. of Means measure',fontsize=15)\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sal,std_sal,confint_sal,confint_lower,confint_upper=get_foreground_sensitivity_allmodels(model_saliency_score_aprime,list_layers,contour_present_pos)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(22,10))\n",
    "color_or_finetune=(78/255,121/255,180/255)\n",
    "spacing_x=2\n",
    "error_bar_width=3\n",
    "error_bar_cap=4\n",
    "\n",
    "plt.errorbar(np.arange(len(mean_sal)) * spacing_x, mean_sal, yerr=confint_sal, \n",
    "             capsize=error_bar_cap,elinewidth=error_bar_width,fmt='o',marker='o', color=color_or_finetune, markersize=10, \n",
    "             label='or finetune', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(mean_sal)) * spacing_x,list_layers,rotation=90,fontsize=15)\n",
    "plt.yticks(fontsize=15) \n",
    "\n",
    "\n",
    "plt.ylabel('Aprime',fontsize=15)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title('Location Sensitivity in Contour Images using APrime measure',fontsize=15)\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sal,std_sal,confint_sal,confint_lower,confint_upper=get_alignment_sensitivity_allmodels(model_saliency_score_aprime,list_layers,contour_present_pos,contour_absent_pos)\n",
    "\n",
    "plt.figure(figsize=(22,10))\n",
    "color_or_finetune=(78/255,121/255,180/255)\n",
    "spacing_x=2\n",
    "error_bar_width=3\n",
    "error_bar_cap=4\n",
    "\n",
    "plt.errorbar(np.arange(len(mean_sal)) * spacing_x, mean_sal, yerr=confint_sal, \n",
    "             capsize=error_bar_cap,elinewidth=error_bar_width,fmt='o',marker='o', color=color_or_finetune, markersize=10, \n",
    "             label='or finetune', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(mean_sal)) * spacing_x,list_layers,rotation=90,fontsize=15)\n",
    "plt.yticks(fontsize=15) \n",
    "\n",
    "\n",
    "plt.ylabel('Difference b/w misaligned and aligned pairs',fontsize=15)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title('Alignment Sensitivity in Contour Images using APrime measure',fontsize=15)\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"color:red\"> Figure : Fig. 3 and SFig. 3   </h1>\n",
    "\n",
    "## Description: Performance of PinholeNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=get_B,get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=1, num_workers=num_workers, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinholenet_get_prediction(model_file_paths):\n",
    "    model_valacc_scores={}\n",
    "    for file in tqdm(model_file_paths):\n",
    "\n",
    "        # Model\n",
    "        checkpoint=torch.load(file)\n",
    "        # loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device='cpu')\n",
    "        loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "        loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "        \n",
    "        change_train_eval_mode(loaded_spliced_model,loaded_spliced_model.fine_tune,train_eval_mode='eval')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        model_valacc_scores[file.split('/')[-1]]=np.array([])\n",
    "        for (inputs, b, d, alpha, nel, labels, record) in tqdm(val_loader_norm,disable=True):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs= loaded_spliced_model.forward(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            model_valacc_scores[file.split('/')[-1]]=np.concatenate((model_valacc_scores[file.split('/')[-1]],(preds == labels).float().cpu().numpy()))\n",
    "    \n",
    "    \n",
    "    return model_valacc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinholenet_get_list_acc_mean_std_ci(sal_dict):\n",
    "    mean_sal=[]\n",
    "    std_sal=[]\n",
    "    confint_sal=[]\n",
    "    \n",
    "    \n",
    "    confint_lower=[]\n",
    "    confint_upper=[]\n",
    "    \n",
    "    list_keys=[]\n",
    "    \n",
    "    for key in sal_dict.keys():\n",
    "        \n",
    "        \n",
    "        mean_sal.append(np.mean(sal_dict[key]))\n",
    "\n",
    "        std_sal.append(np.std(sal_dict[key],ddof=1))\n",
    "\n",
    "        confint_sal.append(1.96*(np.std(sal_dict[key],ddof=1) / np.sqrt(len(sal_dict[key]))))\n",
    "\n",
    "        confint_lower.append(np.percentile(sal_dict[key], 2.5))\n",
    "        confint_upper.append(np.percentile(sal_dict[key], 97.5))\n",
    "        \n",
    "        list_keys.append(key)\n",
    "            \n",
    "    return mean_sal,std_sal,confint_sal,confint_lower,confint_upper, list_keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting all the accuracy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagnet_conv5_finetune_files = ['../model_weights/contour_model_weights/alexnet-bagnet11_regimagenet_categ_finetune_broad/model_alexnet-bagnet11-regim-categ_layer_features-9_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet17_regimagenet_categ_finetune_broad/model_alexnet-bagnet17-regim-categ_layer_features-9_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet31_regimagenet_categ_finetune_broad/model_alexnet-bagnet31-regim-categ_layer_features-9_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet33_regimagenet_categ_finetune_broad/model_alexnet-bagnet33-regim-categ_layer_features-9_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-12_mode_finetune.pt']\n",
    "bagnet_conv5_finetune_predictions=pinholenet_get_prediction(bagnet_conv5_finetune_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagnet_ap_finetune_files = ['../model_weights/contour_model_weights/alexnet-bagnet11_regimagenet_categ_finetune_broad/model_alexnet-bagnet11-regim-categ_layer_avgpool_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet17_regimagenet_categ_finetune_broad/model_alexnet-bagnet17-regim-categ_layer_avgpool_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet31_regimagenet_categ_finetune_broad/model_alexnet-bagnet31-regim-categ_layer_avgpool_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet33_regimagenet_categ_finetune_broad/model_alexnet-bagnet33-regim-categ_layer_avgpool_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune.pt']\n",
    "bagnet_ap_finetune_predictions=pinholenet_get_prediction(bagnet_ap_finetune_files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bagnet_fc1_finetune_files = ['../model_weights/contour_model_weights/alexnet-bagnet11_regimagenet_categ_finetune_broad/model_alexnet-bagnet11-regim-categ_layer_classifier-2_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet17_regimagenet_categ_finetune_broad/model_alexnet-bagnet17-regim-categ_layer_classifier-2_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet31_regimagenet_categ_finetune_broad/model_alexnet-bagnet31-regim-categ_layer_classifier-2_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet33_regimagenet_categ_finetune_broad/model_alexnet-bagnet33-regim-categ_layer_classifier-2_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_classifier-2_mode_finetune.pt']\n",
    "bagnet_fc1_finetune_predictions=pinholenet_get_prediction(bagnet_fc1_finetune_files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bagnet_fc2_finetune_files = ['../model_weights/contour_model_weights/alexnet-bagnet11_regimagenet_categ_finetune_broad/model_alexnet-bagnet11-regim-categ_layer_classifier-5_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet17_regimagenet_categ_finetune_broad/model_alexnet-bagnet17-regim-categ_layer_classifier-5_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet31_regimagenet_categ_finetune_broad/model_alexnet-bagnet31-regim-categ_layer_classifier-5_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet33_regimagenet_categ_finetune_broad/model_alexnet-bagnet33-regim-categ_layer_classifier-5_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_classifier-5_mode_finetune.pt']\n",
    "bagnet_fc2_finetune_predictions=pinholenet_get_prediction(bagnet_fc2_finetune_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagnet_conv5_mean_sal, _, bagnet_conv5_confint_sal, _, _, _ = pinholenet_get_list_acc_mean_std_ci(bagnet_conv5_finetune_predictions)\n",
    "\n",
    "\n",
    "bagnet_ap_mean_sal, _, bagnet_ap_confint_sal, _, _, _ = pinholenet_get_list_acc_mean_std_ci(bagnet_ap_finetune_predictions)\n",
    "\n",
    "\n",
    "bagnet_fc1_mean_sal, _, bagnet_fc1_confint_sal, _, _, _ = pinholenet_get_list_acc_mean_std_ci(bagnet_fc1_finetune_predictions)\n",
    "\n",
    "\n",
    "bagnet_fc2_mean_sal, _, bagnet_fc2_confint_sal, _, _, _ = pinholenet_get_list_acc_mean_std_ci(bagnet_fc2_finetune_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_shades_barplot=[(0.8229757785467128, 0.8898269896193771, 0.9527566320645905),\n",
    "                      (0.6965013456362937, 0.8248366013071895, 0.9092656670511342),\n",
    "                      (0.5168627450980392, 0.7357477893118032, 0.8601922337562476),\n",
    "                      (0.3363783160322953, 0.6255132641291811, 0.8067358708189158),\n",
    "                      (0.8275, 0.8275, 0.8275)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig. 3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'model_name': ['P 11*11', 'P 17*17', 'P 31*31', 'P 33*33','Standard Alexnet',\n",
    "                  'P 11*11', 'P 17*17', 'P 31*31', 'P 33*33','Standard Alexnet'],\n",
    "    \n",
    "    'layer_name': ['C5', 'C5', 'C5', 'C5', 'C5',\n",
    "                   'FC2','FC2','FC2','FC2','FC2'],\n",
    "    \n",
    "    'accuracy': [*bagnet_conv5_mean_sal,\n",
    "                *bagnet_fc2_mean_sal],\n",
    "    \n",
    "    'error': [*bagnet_conv5_confint_sal,\n",
    "                *bagnet_fc2_confint_sal],\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sample colors with RGB values\n",
    "color_dict = {\n",
    "    'P 11*11': (0.8229757785467128, 0.8898269896193771, 0.9527566320645905),\n",
    "    'P 17*17': (0.6965013456362937, 0.8248366013071895, 0.9092656670511342),\n",
    "    'P 31*31': (0.5168627450980392, 0.7357477893118032, 0.8601922337562476),\n",
    "    'P 33*33': (0.3363783160322953, 0.6255132641291811, 0.8067358708189158),\n",
    "    'Standard Alexnet':(0.827, 0.827, 0.827)\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "barplot = sns.barplot(data=df, x='layer_name', y='accuracy', hue='model_name', palette=color_dict, ci=None)\n",
    "\n",
    "# Adjust the ylim to make sure error bars are visible\n",
    "plt.ylim(0.4, 1.0)\n",
    "\n",
    "# Since we have grouped data, we need to adjust the way we access the error values.\n",
    "# We'll use groupby and get_group to get the correct subsets.\n",
    "for name, group in df.groupby(['layer_name', 'model_name']):\n",
    "    layer_name, model_name = name\n",
    "    group_data = df[(df['layer_name'] == layer_name) & (df['model_name'] == model_name)]\n",
    "    \n",
    "    # Get the positions of the bars for the current group\n",
    "    positions = [p.get_x() + p.get_width() / 2 for p in barplot.patches if p.get_height() == group_data['accuracy'].values[0]]\n",
    "    \n",
    "    # Plot the error bars for the current group\n",
    "    plt.errorbar(positions, group_data['accuracy'], yerr=group_data['error'], fmt='none', c='k', capsize=5)\n",
    "\n",
    "plt.title('Model Accuracy by Layer')\n",
    "plt.xlabel('Layer Name')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Remove the legend if not needed\n",
    "plt.legend(title='Model Name').remove()\n",
    "\n",
    "# Spine visibility settings as before\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "# plt.savefig('./dev/manuscript_figures/f3_bagnet_comparison.png', format='png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFig. 3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnetbase=torch.load('../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-12_mode_finetune.pt')['metrics']['val_acc']\n",
    "\n",
    "alexnetepoch50=torch.load('../model_weights/contour_model_weights/alexnet-epoch50_regimagenet_categ_finetune_broad/model_alexnet-epoch50-regim-categ_layer_features-12_mode_finetune.pt')['metrics']['val_acc']\n",
    "alexnetepoch100=torch.load('../model_weights/contour_model_weights/alexnet-epoch100_regimagenet_categ_finetune_broad/model_alexnet-epoch100-regim-categ_layer_features-12_mode_finetune.pt')['metrics']['val_acc']\n",
    "\n",
    "alexnet33=torch.load('../model_weights/contour_model_weights/alexnet-bagnet33_regimagenet_categ_finetune_broad/model_alexnet-bagnet33-regim-categ_layer_features-9_mode_finetune.pt')['metrics']['val_acc']\n",
    "alexnet31=torch.load('../model_weights/contour_model_weights/alexnet-bagnet31_regimagenet_categ_finetune_broad/model_alexnet-bagnet31-regim-categ_layer_features-9_mode_finetune.pt')['metrics']['val_acc']\n",
    "\n",
    "alexnet17=torch.load('../model_weights/contour_model_weights/alexnet-bagnet17_regimagenet_categ_finetune_broad/model_alexnet-bagnet17-regim-categ_layer_features-9_mode_finetune.pt')['metrics']['val_acc']\n",
    "alexnet11=torch.load('../model_weights/contour_model_weights/alexnet-bagnet11_regimagenet_categ_finetune_broad/model_alexnet-bagnet11-regim-categ_layer_features-9_mode_finetune.pt')['metrics']['val_acc']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_colors = ['#1f78b4', '#33a02c', '#e31a1c', '#ff7f00', '#6a3d9a']\n",
    "\n",
    "color_shades_barplot=[(0.8229757785467128, 0.8898269896193771, 0.9527566320645905),\n",
    "                      (0.6965013456362937, 0.8248366013071895, 0.9092656670511342),\n",
    "                      (0.5168627450980392, 0.7357477893118032, 0.8601922337562476),\n",
    "                      (0.3363783160322953, 0.6255132641291811, 0.8067358708189158),\n",
    "                      (0.8275, 0.8275, 0.8275)]\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "\n",
    "\n",
    "l1,=plt.plot(np.arange(len(alexnetbase)),alexnetbase,linewidth=3.0,color='lightgray',label='alexnet torchvision baseline (90 epochs)')\n",
    "# plt.plot(np.arange(len(alexnetepoch100)),alexnetepoch100,linewidth=1.0,color='k',label='alexnet-epoch100')\n",
    "plt.plot(np.arange(len(alexnetepoch50)),alexnetepoch50,linestyle='--',linewidth=3.0,color='k',label='alexnet-epoch50')\n",
    "\n",
    "\n",
    "plt.plot(np.arange(len(alexnet11)),alexnet11,linewidth=3.0,color=(0.8229757785467128, 0.8898269896193771, 0.9527566320645905),label='pinholenet-11')\n",
    "plt.plot(np.arange(len(alexnet17)),alexnet17,linewidth=3.0,color=(0.6965013456362937, 0.8248366013071895, 0.9092656670511342),label='pinholenet-17')\n",
    "\n",
    "plt.plot(np.arange(len(alexnet31)),alexnet31,linewidth=3.0,color=(0.5168627450980392, 0.7357477893118032, 0.8601922337562476),label='pinholenet-31')\n",
    "plt.plot(np.arange(len(alexnet33)),alexnet33,linewidth=3.0,color=(0.3363783160322953, 0.6255132641291811, 0.8067358708189158),label='pinholenet-33')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plt.axhline(y=0.5,linestyle='--',color='r',label='chance',alpha=0.1)\n",
    "\n",
    "plt.xlabel('Epochs',fontsize=15,labelpad=15)\n",
    "plt.ylabel('Contour Readout Accuracy',fontsize=15,labelpad=15)\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)   \n",
    "plt.ylim(0.4,1.01)\n",
    "\n",
    "    \n",
    "# plt.legend(fontsize=12,frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFig. 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_base_rf=[19,67,99,131,195,323,512,512]\n",
    "pinhole_11_rf=[7,9,11,11,11,101,512,512]\n",
    "pinhole_17_rf=[9,13,17,17,17,101,512,512]\n",
    "pinhole_31_rf=[11,19,23,27,31,111,512,512]\n",
    "pinhole_33_rf=[13,25,29,33,33,111,512,512]\n",
    "\n",
    "\n",
    "x_lab=['C1','C2','C3','C4','C5','AP','F1','F2']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(np.arange(len(alexnet_base_rf)),alexnet_base_rf,linewidth=3.0,color='lightgray',label='standarad alexnet')\n",
    "\n",
    "plt.plot(np.arange(len(pinhole_11_rf)),pinhole_11_rf,linewidth=3.0,color=(0.8229757785467128, 0.8898269896193771, 0.9527566320645905),label='pinholenet-11')\n",
    "plt.plot(np.arange(len(pinhole_17_rf)),pinhole_17_rf,linewidth=3.0,color=(0.6965013456362937, 0.8248366013071895, 0.9092656670511342),label='pinholenet-17')\n",
    "plt.plot(np.arange(len(pinhole_31_rf)),pinhole_31_rf,linewidth=3.0,color=(0.5168627450980392, 0.7357477893118032, 0.8601922337562476),label='pinholenet-31')\n",
    "plt.plot(np.arange(len(pinhole_33_rf)),pinhole_33_rf,linewidth=3.0,color=(0.3363783160322953, 0.6255132641291811, 0.8067358708189158),label='pinholenet-33')\n",
    "\n",
    "\n",
    "for i in range(len(np.arange(len(alexnet_base_rf)))):\n",
    "    plt.axvline(np.arange(len(alexnet_base_rf))[i],linestyle='--',alpha=0.09,color='k')\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(alexnet_base_rf)),x_lab,fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "plt.xlabel('Layer',fontsize=15,labelpad=15)\n",
    "plt.ylabel('Receptive Field Size',fontsize=15,labelpad=15)\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "plt.legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFig. 3E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'model_name': ['P 11*11', 'P 17*17', 'P 31*31', 'P 33*33','Standard Alexnet',\n",
    "                  'P 11*11', 'P 17*17', 'P 31*31', 'P 33*33','Standard Alexnet',\n",
    "                  'P 11*11', 'P 17*17', 'P 31*31', 'P 33*33','Standard Alexnet'],\n",
    "    \n",
    "    'layer_name': ['AP', 'AP', 'AP', 'AP', 'AP',\n",
    "                   'FC1','FC1','FC1','FC1','FC1',\n",
    "                   'FC2','FC2','FC2','FC2','FC2'],\n",
    "    \n",
    "    'accuracy': [*bagnet_ap_mean_sal,\n",
    "                *bagnet_fc1_mean_sal,\n",
    "                *bagnet_fc2_mean_sal]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sample colors with RGB values\n",
    "color_dict = {\n",
    "    'P 11*11': (0.8229757785467128, 0.8898269896193771, 0.9527566320645905),\n",
    "    'P 17*17': (0.6965013456362937, 0.8248366013071895, 0.9092656670511342),\n",
    "    'P 31*31': (0.5168627450980392, 0.7357477893118032, 0.8601922337562476),\n",
    "    'P 33*33': (0.3363783160322953, 0.6255132641291811, 0.8067358708189158),\n",
    "    'Standard Alexnet':(0.827, 0.827, 0.827)\n",
    "}\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.barplot(data=df, x='layer_name', y='accuracy', hue='model_name', palette=color_dict, ci=None)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Model Accuracy by Layer')\n",
    "plt.xlabel('Layer Name')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(title='Model Name').remove()\n",
    "\n",
    "\n",
    "plt.ylim(0.5, 1.0) \n",
    "\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "# plt.savefig('./dev/manuscript_figures/pinholenet_contour_training_classifier_head.png', bbox_inches='tight', format='png', dpi=600)\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"color:red\"> Figure : Fig. 4   </h1>\n",
    "\n",
    "## Description: Human Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_variables(filename):\n",
    "    \"\"\"\n",
    "    Load multiple Python variables from a file.\n",
    "    \n",
    "    Parameters:\n",
    "    - filename (str): the name of the file to load from.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing the loaded variables.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent_correct(all_participants_filename,all_participants_correct,all_participants_beta):\n",
    "    unique_images=np.unique(all_participants_filename)\n",
    "\n",
    "    accuracy_image=[]\n",
    "    beta_image=[]\n",
    "\n",
    "    for img in unique_images:\n",
    "        accuracy_image.append(np.mean(all_participants_correct[np.where(all_participants_filename==img)]))\n",
    "        beta_image.append(np.mean(all_participants_beta[np.where(all_participants_filename==img)]))\n",
    "        \n",
    "    return np.array(accuracy_image),np.array(beta_image),unique_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(data, num_samples,num_simulations=300):\n",
    "    \"\"\"Creates a bootstrap sample from data with replacement.\n",
    "\n",
    "    Args:\n",
    "    data (np.array): The numpy array to sample from.\n",
    "    num_samples (int): The number of samples to draw.\n",
    "\n",
    "    Returns:\n",
    "    np.array: The bootstrap sample as a numpy array.\n",
    "    \"\"\"\n",
    "    bootstrapped_data=[]\n",
    "    for sim in range(num_simulations):\n",
    "        bootstrapped_data.append(np.random.choice(data, size=num_samples))\n",
    "    bootstrapped_data=np.array(bootstrapped_data)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return bootstrapped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_variables('../contour_integ_behavior/contour_exp1/analysis_data/analysis_data.pkl')\n",
    "\n",
    "all_participants_filename=data['all_participants_filename']\n",
    "all_participants_correct=data['all_participants_correct']\n",
    "all_participants_beta=data['all_participants_beta']\n",
    "\n",
    "print(all_participants_beta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Contour Signal at level of individual trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_img_signal,beta_image,unique_images=get_percent_correct(all_participants_filename,all_participants_correct,all_participants_beta)\n",
    "unique_beta_vals = np.unique(beta_image)\n",
    "\n",
    "print(human_img_signal.shape)\n",
    "print(unique_beta_vals)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.bar(range(len(human_img_signal)), human_img_signal,color='gray')\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.title(' Human Contour Signal at level of individual trials (Human Percent Correct)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Beta-wise Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_participants_unique_beta={}\n",
    "human_unique_beta_acc=[]\n",
    "for b in np.unique(all_participants_beta):\n",
    "    all_participants_unique_beta[b]={}\n",
    "    all_participants_unique_beta[b]['mean']=np.mean(all_participants_correct[np.where(all_participants_beta==b)])\n",
    "    all_participants_unique_beta[b]['std']=np.std(all_participants_correct[np.where(all_participants_beta==b)])\n",
    "    human_unique_beta_acc.append([np.mean(all_participants_correct[np.where(all_participants_beta==b)]),np.std(all_participants_correct[np.where(all_participants_beta==b)])])\n",
    "human_unique_beta_acc=np.array(human_unique_beta_acc)\n",
    "\n",
    "print(all_participants_unique_beta)\n",
    "print(human_unique_beta_acc)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(unique_beta_vals,human_unique_beta_acc[:,0],color='gray',linewidth=4,label='human')\n",
    "plt.plot(unique_beta_vals,human_unique_beta_acc[:,0],'.',color='k',markersize=15)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.xticks(unique_beta_vals,fontsize=15)\n",
    "plt.yticks(fontsize=15) \n",
    "plt.ylabel('Average Accuracy',fontsize=15)\n",
    "plt.ylim(0.4,1.0)\n",
    "plt.title('Mean Accuracy for each beta condition')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding error bars (bootstrapped 95% CI) over each beta condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_image=[]\n",
    "beta_image=[]\n",
    "bootstrapped_accuracy_image=[]\n",
    "\n",
    "\n",
    "\n",
    "for img in tqdm(unique_images):\n",
    "    accuracy_image.append(np.mean(all_participants_correct[np.where(all_participants_filename==img)]))\n",
    "    beta_image.append(np.mean(all_participants_beta[np.where(all_participants_filename==img)]))\n",
    "    \n",
    "    t=np.mean(bootstrap_sample(all_participants_correct[np.where(all_participants_filename==img)],len(all_participants_correct[np.where(all_participants_filename==img)])),1)\n",
    "    bootstrapped_accuracy_image.append(t)\n",
    "\n",
    "    \n",
    "accuracy_image=np.array(accuracy_image)\n",
    "beta_image=np.array(beta_image)\n",
    "bootstrapped_accuracy_image=np.array(bootstrapped_accuracy_image)\n",
    "\n",
    "print(accuracy_image.shape)\n",
    "print(beta_image.shape)\n",
    "print(bootstrapped_accuracy_image.shape)\n",
    "\n",
    "condition_accuracy_image=np.empty((0,200))\n",
    "condition_bootstrapped_accuracy_image=np.empty((0,200,300))\n",
    "condition_beta=[]\n",
    "\n",
    "for b in np.unique(beta_image):\n",
    "    condition_beta.append(b)\n",
    "    condition_bootstrapped_accuracy_image=np.concatenate((condition_bootstrapped_accuracy_image,np.expand_dims(bootstrapped_accuracy_image[np.where(beta_image==b)[0],:],0)))\n",
    "    condition_accuracy_image=np.concatenate((condition_accuracy_image,np.expand_dims(accuracy_image[np.where(beta_image==b)[0]],0)))\n",
    "\n",
    "print(condition_accuracy_image.shape)\n",
    "print(condition_bootstrapped_accuracy_image.shape)\n",
    "\n",
    "condition_bootstrapped_accuracy_image=np.mean(condition_bootstrapped_accuracy_image,1)\n",
    "condition_accuracy_image=np.mean(condition_accuracy_image,1)\n",
    "\n",
    "print(condition_accuracy_image.shape)\n",
    "print(condition_bootstrapped_accuracy_image.shape)\n",
    "print(condition_beta)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Human Results\n",
    "plt.figure(figsize=(10,12))\n",
    "plt.errorbar(np.arange(len(condition_accuracy_image)), condition_accuracy_image, \n",
    "             yerr=[condition_accuracy_image - np.percentile(condition_bootstrapped_accuracy_image, 2.5, axis=1),np.percentile(condition_bootstrapped_accuracy_image, 97.5, axis=1) - condition_accuracy_image], \n",
    "             capsize=5,elinewidth=4,linestyle='-', marker='.', color='gray', markersize=10, \n",
    "             label='supervised regular finetune', alpha=0.8, linewidth=4)\n",
    "\n",
    "for i in range(len(condition_accuracy_image)):\n",
    "    plt.axvline(i,linestyle='--',alpha=0.1,color='k')\n",
    "\n",
    "plt.xticks(np.arange(len(condition_accuracy_image)),condition_beta,fontsize=20)\n",
    "plt.yticks(fontsize=20) \n",
    "\n",
    "plt.ylim(0.4,1.0)\n",
    "\n",
    "plt.xlabel('Beta Condition',fontsize=20)\n",
    "plt.ylabel('Percent Correct',fontsize=20)\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing percent correct for each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psychophysics_folder = psychophysics_visual_diet_config['root_directory']\n",
    "recorder_image=[]\n",
    "for img in unique_images:\n",
    "    recorder_image.append(torch.load(os.path.join(psychophysics_folder,img)))\n",
    "recorder_image=np.array(recorder_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_accuracy_image=[]\n",
    "sorted_beta_image=[]\n",
    "sorted_recorder_image=[]\n",
    "\n",
    "for b in np.unique(all_participants_beta[0,:],return_counts=True)[0]:\n",
    "    s_accuracy_image=accuracy_image[np.where(beta_image==b)]\n",
    "    s_beta_image=beta_image[np.where(beta_image==b)]\n",
    "    s_recorder_image=recorder_image[np.where(beta_image==b)]\n",
    "    \n",
    "    \n",
    "    s_indices=np.argsort(s_accuracy_image)\n",
    "    \n",
    "    \n",
    "    sorted_accuracy_image.append(s_accuracy_image[s_indices])\n",
    "    sorted_beta_image.append(s_beta_image[s_indices])\n",
    "    sorted_recorder_image.append(s_recorder_image[s_indices])\n",
    "    \n",
    "    \n",
    "sorted_accuracy_image=np.concatenate(sorted_accuracy_image)\n",
    "sorted_beta_image=np.concatenate(sorted_beta_image)\n",
    "sorted_recorder_image=np.concatenate(sorted_recorder_image)\n",
    "\n",
    "plt.figure(figsize=(24,8))  # Set the figure size\n",
    "deep_palette = sns.color_palette(\"deep\", 5)\n",
    "color_dict = {15: deep_palette[0], \n",
    "             30: deep_palette[1], \n",
    "             45: deep_palette[2], \n",
    "             60: deep_palette[3], \n",
    "             75: deep_palette[4]}\n",
    "\n",
    "\n",
    "colors = [color_dict[label] for label in sorted_beta_image]\n",
    "\n",
    "plt.bar(range(len(sorted_accuracy_image)), sorted_accuracy_image,color=colors)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"color:red\"> Figure : Fig. 5, SFig. 6, SFig. 7, SFig. 8, SFig. 9   </h1>\n",
    "\n",
    "## Description: Human Behavior, Model-Human Alignment, Model-Human Performance as fn of Curvature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Human Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_variables(filename):\n",
    "    \"\"\"\n",
    "    Load multiple Python variables from a file.\n",
    "    \n",
    "    Parameters:\n",
    "    - filename (str): the name of the file to load from.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing the loaded variables.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_percent_correct(all_participants_filename,all_participants_correct,all_participants_beta):\n",
    "    unique_images=np.unique(all_participants_filename)\n",
    "\n",
    "    accuracy_image=[]\n",
    "    beta_image=[]\n",
    "\n",
    "    for img in unique_images:\n",
    "        accuracy_image.append(np.mean(all_participants_correct[np.where(all_participants_filename==img)]))\n",
    "        beta_image.append(np.mean(all_participants_beta[np.where(all_participants_filename==img)]))\n",
    "        \n",
    "    return np.array(accuracy_image),np.array(beta_image),unique_images\n",
    "\n",
    "def bootstrap_sample(data, num_samples,num_simulations=300):\n",
    "    \"\"\"Creates a bootstrap sample from data with replacement.\n",
    "\n",
    "    Args:\n",
    "    data (np.array): The numpy array to sample from.\n",
    "    num_samples (int): The number of samples to draw.\n",
    "\n",
    "    Returns:\n",
    "    np.array: The bootstrap sample as a numpy array.\n",
    "    \"\"\"\n",
    "    bootstrapped_data=[]\n",
    "    for sim in range(num_simulations):\n",
    "        bootstrapped_data.append(np.random.choice(data, size=num_samples))\n",
    "    bootstrapped_data=np.array(bootstrapped_data)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return bootstrapped_data\n",
    "\n",
    "data = load_variables('../contour_integ_behavior/contour_exp1/analysis_data/analysis_data.pkl')\n",
    "\n",
    "all_participants_filename=data['all_participants_filename']\n",
    "all_participants_correct=data['all_participants_correct']\n",
    "all_participants_beta=data['all_participants_beta']\n",
    "\n",
    "print(all_participants_beta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Human Contour Signal at level of individual trials\n",
    "human_img_signal,beta_image,unique_images=get_percent_correct(all_participants_filename,all_participants_correct,all_participants_beta)\n",
    "unique_beta_vals = np.unique(beta_image)\n",
    "\n",
    "print(human_img_signal.shape)\n",
    "print(unique_beta_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Human Beta-wise Accuracy\n",
    "\n",
    "all_participants_unique_beta={}\n",
    "human_unique_beta_acc=[]\n",
    "for b in np.unique(all_participants_beta):\n",
    "    all_participants_unique_beta[b]={}\n",
    "    all_participants_unique_beta[b]['mean']=np.mean(all_participants_correct[np.where(all_participants_beta==b)])\n",
    "    all_participants_unique_beta[b]['std']=np.std(all_participants_correct[np.where(all_participants_beta==b)])\n",
    "    human_unique_beta_acc.append([np.mean(all_participants_correct[np.where(all_participants_beta==b)]),np.std(all_participants_correct[np.where(all_participants_beta==b)])])\n",
    "human_unique_beta_acc=np.array(human_unique_beta_acc)\n",
    "\n",
    "print(all_participants_unique_beta)\n",
    "print(human_unique_beta_acc)\n",
    "\n",
    "### Adding error bars (bootstrapped 95% CI) over each beta condition\n",
    "\n",
    "accuracy_image=[]\n",
    "beta_image=[]\n",
    "bootstrapped_accuracy_image=[]\n",
    "\n",
    "\n",
    "\n",
    "for img in tqdm(unique_images):\n",
    "    accuracy_image.append(np.mean(all_participants_correct[np.where(all_participants_filename==img)]))\n",
    "    beta_image.append(np.mean(all_participants_beta[np.where(all_participants_filename==img)]))\n",
    "    \n",
    "    t=np.mean(bootstrap_sample(all_participants_correct[np.where(all_participants_filename==img)],len(all_participants_correct[np.where(all_participants_filename==img)])),1)\n",
    "    bootstrapped_accuracy_image.append(t)\n",
    "\n",
    "    \n",
    "accuracy_image=np.array(accuracy_image)\n",
    "beta_image=np.array(beta_image)\n",
    "bootstrapped_accuracy_image=np.array(bootstrapped_accuracy_image)\n",
    "\n",
    "print(accuracy_image.shape)\n",
    "print(beta_image.shape)\n",
    "print(bootstrapped_accuracy_image.shape)\n",
    "\n",
    "condition_accuracy_image=np.empty((0,200))\n",
    "condition_bootstrapped_accuracy_image=np.empty((0,200,300))\n",
    "condition_beta=[]\n",
    "\n",
    "for b in np.unique(beta_image):\n",
    "    condition_beta.append(b)\n",
    "    condition_bootstrapped_accuracy_image=np.concatenate((condition_bootstrapped_accuracy_image,np.expand_dims(bootstrapped_accuracy_image[np.where(beta_image==b)[0],:],0)))\n",
    "    condition_accuracy_image=np.concatenate((condition_accuracy_image,np.expand_dims(accuracy_image[np.where(beta_image==b)[0]],0)))\n",
    "\n",
    "print(condition_accuracy_image.shape)\n",
    "print(condition_bootstrapped_accuracy_image.shape)\n",
    "\n",
    "condition_bootstrapped_accuracy_image=np.mean(condition_bootstrapped_accuracy_image,1)\n",
    "condition_accuracy_image=np.mean(condition_accuracy_image,1)\n",
    "\n",
    "print(condition_accuracy_image.shape)\n",
    "print(condition_bootstrapped_accuracy_image.shape)\n",
    "print(condition_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model-Human Alignment and Model-Human Performance as fn of Curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_signal_strength(model,human_img_signal):\n",
    "    contour_present=None\n",
    "    contour_absent=None\n",
    "\n",
    "    ### When contour is present\n",
    "    psychophysics_dataset_norm = Psychophysics_Dataset(root=os.path.expanduser('/home/jovyan/work/Datasets/contour_integration/model-psychophysics/experiment_1/'),transform=data_transform,get_B=[0,15,30,45,60,75],get_D=[32],get_A=[0],get_contour='contour')\n",
    "    psychophysics_loader_norm=torch.utils.data.DataLoader(dataset=psychophysics_dataset_norm, batch_size=15, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    all_betas=[]\n",
    "    all_labels=[]\n",
    "    all_outputs=[]\n",
    "    all_preds=[]\n",
    "    all_recorder_path=[]\n",
    "\n",
    "\n",
    "    for (inputs, b, d, a, nel, labels, record) in psychophysics_loader_norm:\n",
    "        inputs=inputs.to(device)\n",
    "        output=model.forward(inputs).detach().cpu()\n",
    "        _, preds = torch.max(output, 1)\n",
    "\n",
    "        all_preds.append(preds)\n",
    "\n",
    "        all_outputs.append(output)\n",
    "        all_betas.append(b)\n",
    "        all_labels.append(labels)\n",
    "        all_recorder_path.append(np.array(record))\n",
    "\n",
    "    all_betas=torch.cat(all_betas).numpy()\n",
    "    all_labels=torch.cat(all_labels).numpy()\n",
    "    all_preds=torch.cat(all_preds).numpy()\n",
    "    all_outputs=torch.cat(all_outputs).numpy()\n",
    "    all_recorder_path=np.concatenate(all_recorder_path)\n",
    "\n",
    "    contour_present=all_outputs\n",
    "\n",
    "\n",
    "    #####################################################################################################################\n",
    "\n",
    "    ### When contour is absent\n",
    "    psychophysics_dataset_norm = Psychophysics_Dataset(root=os.path.expanduser('/home/jovyan/work/Datasets/contour_integration/model-psychophysics/experiment_1/'),transform=data_transform,get_B=[0,15,30,45,60,75],get_D=[32],get_A=[0],get_contour='control')\n",
    "    psychophysics_loader_norm=torch.utils.data.DataLoader(dataset=psychophysics_dataset_norm, batch_size=15, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    all_betas=[]\n",
    "    all_labels=[]\n",
    "    all_outputs=[]\n",
    "    all_preds=[]\n",
    "    all_recorder_path=[]\n",
    "\n",
    "\n",
    "    for (inputs, b, d, a, nel, labels, record) in psychophysics_loader_norm:\n",
    "        inputs=inputs.to(device)\n",
    "        output=model.forward(inputs).detach().cpu()\n",
    "        _, preds = torch.max(output, 1)\n",
    "\n",
    "        all_preds.append(preds)\n",
    "\n",
    "        # all_outputs.append(softmax_layer(output))\n",
    "        all_outputs.append(output)\n",
    "\n",
    "        all_betas.append(b)\n",
    "        all_labels.append(labels)\n",
    "        all_recorder_path.append(np.array(record))\n",
    "\n",
    "    all_betas=torch.cat(all_betas).numpy()\n",
    "    all_labels=torch.cat(all_labels).numpy()\n",
    "    all_preds=torch.cat(all_preds).numpy()\n",
    "    all_outputs=torch.cat(all_outputs).numpy()\n",
    "    all_recorder_path=np.concatenate(all_recorder_path)\n",
    "\n",
    "    contour_absent=all_outputs\n",
    "    \n",
    "    \n",
    "    model_img_signal=((contour_present - contour_absent)/np.sqrt(2))[:,1]\n",
    "    model_human_corr=np.corrcoef(model_img_signal,human_img_signal)[0][1]\n",
    "    \n",
    "    \n",
    "    return model_human_corr, model_img_signal, contour_present, contour_absent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_beta_acc(model):\n",
    "    psychophysics_dataset_norm = Psychophysics_Dataset(root=os.path.expanduser('/home/jovyan/work/Datasets/contour_integration/model-psychophysics/experiment_1/'),transform=data_transform,get_B=[0,15,30,45,60,75],get_D=[32],get_A=[0],get_contour='all')\n",
    "    psychophysics_loader_norm=torch.utils.data.DataLoader(dataset=psychophysics_dataset_norm, batch_size=15, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    all_betas=[]\n",
    "    all_labels=[]\n",
    "    all_outputs=[]\n",
    "    all_preds=[]\n",
    "    all_recorder_path=[]\n",
    "\n",
    "\n",
    "    for (inputs, b, d, a, nel, labels, record) in tqdm(psychophysics_loader_norm,disable=True):\n",
    "        inputs=inputs.to(device)\n",
    "        output=model.forward(inputs).detach().cpu()\n",
    "        _, preds = torch.max(output, 1)\n",
    "\n",
    "        all_preds.append(preds)\n",
    "\n",
    "        all_outputs.append(output)\n",
    "        all_betas.append(b)\n",
    "        all_labels.append(labels)\n",
    "        all_recorder_path.append(np.array(record))\n",
    "\n",
    "    all_betas=torch.cat(all_betas).numpy()\n",
    "    all_labels=torch.cat(all_labels).numpy()\n",
    "    all_preds=torch.cat(all_preds).numpy()\n",
    "    all_outputs=torch.cat(all_outputs).numpy()\n",
    "    all_recorder_path=np.concatenate(all_recorder_path)\n",
    "\n",
    "    unique_beta_vals=np.unique(all_betas)\n",
    "    model_unique_beta_acc=[]\n",
    "\n",
    "\n",
    "    for i in unique_beta_vals:\n",
    "\n",
    "        acc=np.mean(all_labels[np.where(all_betas==i)[0]] == all_preds[np.where(all_betas==i)[0]])\n",
    "        model_unique_beta_acc.append(acc)\n",
    "    \n",
    "    return model_unique_beta_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fig. 5B, 5C, 5E, 5F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_files =['../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune.pt', \n",
    "                   '../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune_beta_020.pt']\n",
    "\n",
    "model_list_files=sorted(model_list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_human_corr_list=[]\n",
    "model_beta_acc_list=[]\n",
    "model_img_signal_list=[]\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    print(file)\n",
    "    checkpoint=torch.load(file)\n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    \n",
    "    model_human_corr,model_img_signal, _, _ = get_model_signal_strength(loaded_spliced_model,human_img_signal)\n",
    "    model_human_corr_list.append(model_human_corr)\n",
    "    model_beta_acc_list.append(get_model_beta_acc(loaded_spliced_model))\n",
    "    \n",
    "    model_img_signal_list.append(model_img_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_beta_acc_list = np.array(model_beta_acc_list)\n",
    "model_img_signal_list = np.array(model_img_signal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    print(i,'\\t',file.split('/')[-1],' \\t\\t Corr:',model_human_corr_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,18))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(model_img_signal_list[0],human_img_signal,'.',color='red')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title(model_human_corr_list[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(model_img_signal_list[1],human_img_signal,'.',color='green')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title(model_human_corr_list[1])\n",
    "\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.errorbar(np.arange(len(condition_accuracy_image)), condition_accuracy_image, \n",
    "             yerr=[condition_accuracy_image - np.percentile(condition_bootstrapped_accuracy_image, 2.5, axis=1),np.percentile(condition_bootstrapped_accuracy_image, 97.5, axis=1) - condition_accuracy_image], \n",
    "             capsize=5,elinewidth=4,linestyle='-', marker='.', color='gray', markersize=10, \n",
    "             label='Human', alpha=0.8, linewidth=4)\n",
    "plt.plot(np.arange(len(condition_accuracy_image)),model_beta_acc_list[0],linewidth=4,label='finetuned broad',color='r')\n",
    "plt.plot(np.arange(len(condition_accuracy_image)),model_beta_acc_list[0],'.',color='k',markersize=15)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(condition_accuracy_image)),condition_beta,fontsize=20)\n",
    "plt.yticks(fontsize=20) \n",
    "plt.ylim(0.4,1.0)\n",
    "plt.xlabel('Beta Condition',fontsize=20)\n",
    "plt.ylabel('Human Percent Correct/Model Accuracy',fontsize=20)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.errorbar(np.arange(len(condition_accuracy_image)), condition_accuracy_image, \n",
    "             yerr=[condition_accuracy_image - np.percentile(condition_bootstrapped_accuracy_image, 2.5, axis=1),np.percentile(condition_bootstrapped_accuracy_image, 97.5, axis=1) - condition_accuracy_image], \n",
    "             capsize=5,elinewidth=4,linestyle='-', marker='.', color='gray', markersize=10, \n",
    "             label='Human', alpha=0.8, linewidth=4)\n",
    "plt.plot(np.arange(len(condition_accuracy_image)),model_beta_acc_list[1],linewidth=4,label='finetuned narrow',color='green')\n",
    "plt.plot(np.arange(len(condition_accuracy_image)),model_beta_acc_list[1],'.',color='k',markersize=15)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(condition_accuracy_image)),condition_beta,fontsize=20)\n",
    "plt.yticks(fontsize=20) \n",
    "plt.ylim(0.4,1.0)\n",
    "plt.xlabel('Beta Condition',fontsize=20)\n",
    "plt.ylabel('Human Percent Correct/Model Accuracy',fontsize=20)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fig. 5D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_files = glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow/*.pt\")\n",
    "model_list_files=sorted(model_list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model_list_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_human_corr_list=[]\n",
    "model_beta_acc_list=[]\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    print(file)\n",
    "    checkpoint=torch.load(file)\n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    \n",
    "    model_human_corr,_, _, _ = get_model_signal_strength(loaded_spliced_model,human_img_signal)\n",
    "    model_human_corr_list.append(model_human_corr)\n",
    "    model_beta_acc_list.append(get_model_beta_acc(loaded_spliced_model))\n",
    "\n",
    "model_beta_acc_list=np.array(model_beta_acc_list)\n",
    "beta_experience=[]\n",
    "for m in model_list_files:\n",
    "    beta_experience.append(int(m.split('_')[-1][:-3]))\n",
    "\n",
    "print('Highest at',beta_experience[np.argmax(model_human_corr_list)], model_human_corr_list[np.argmax(model_human_corr_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = beta_experience[:8]+beta_experience[9:-4]\n",
    "height=model_human_corr_list[:8]+model_human_corr_list[9:-4]\n",
    "\n",
    "\n",
    "y_pos = np.arange(len(bars)) / 4.5\n",
    "color=['green'] * len(bars)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.fill_between(y_pos,0.7856633080741112 - 0.009460643283334128, 0.7856633080741112 + 0.009460643283334128,color='gray',alpha=0.6)\n",
    "\n",
    "plt.plot(y_pos,height,'-',color='green',linewidth=4)\n",
    "plt.plot(y_pos,height,'.',color='k',markersize=15)\n",
    "\n",
    "\n",
    "max_value = np.argmax(height)\n",
    "plt.plot(y_pos[max_value],height[max_value],'.',color='green',markersize=25)\n",
    "plt.axvline(y_pos[max_value],linestyle='--',alpha=0.5,color='green')\n",
    "\n",
    "\n",
    "for i in y_pos:\n",
    "    plt.axvline(i,linestyle='--',alpha=0.1,color='k')\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(y_pos, bars,fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylabel('Correlation with humans',fontsize=25,labelpad=20)\n",
    "plt.xlabel('Training Curvature',fontsize=25,labelpad=20)\n",
    "# plt.title('Alexnet Avgpool - Constrained Finetuning',fontsize=25)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(True)\n",
    "plt.gca().spines['bottom'].set_visible(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Supp. Fig. 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_files =['../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune.pt']\n",
    "\n",
    "model_list_files=sorted(model_list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_human_corr_list=[]\n",
    "model_beta_acc_list=[]\n",
    "model_img_signal_list=[]\n",
    "\n",
    "model_contour_present_list=[]\n",
    "model_contour_absent_list=[]\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    print(file)\n",
    "    checkpoint=torch.load(file)\n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    \n",
    "    model_human_corr,model_img_signal, contour_present, contour_absent  = get_model_signal_strength(loaded_spliced_model,human_img_signal)\n",
    "    model_human_corr_list.append(model_human_corr)\n",
    "    model_beta_acc_list.append(get_model_beta_acc(loaded_spliced_model))\n",
    "    \n",
    "    model_img_signal_list.append(model_img_signal)\n",
    "    \n",
    "    \n",
    "    model_contour_present_list.append(contour_present)\n",
    "    model_contour_absent_list.append(contour_absent)\n",
    "    \n",
    "\n",
    "model_beta_acc_list = np.array(model_beta_acc_list)\n",
    "model_img_signal_list = np.array(model_img_signal_list)\n",
    "model_contour_present_list = np.array(model_contour_present_list)\n",
    "model_contour_absent_list = np.array(model_contour_absent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "plt.figure(figsize=(24,8))\n",
    "# Create a GridSpec object\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[1, 1.5], height_ratios=[1, 1])\n",
    "# Adjust space between the rows\n",
    "gs.update(wspace=0.3, hspace=0.5)\n",
    "\n",
    "\n",
    "# First subplot: Bottom-Right\n",
    "ax1 = plt.subplot(gs[1, 1])\n",
    "\n",
    "# Second subplot: Top-Right\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "\n",
    "# Third subplot: Top-Left and Bottom-Left\n",
    "ax3 = plt.subplot(gs[:, 0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Top-left\n",
    "ax1.bar(range(len(human_img_signal)), human_img_signal,color='gray')\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.set_xlabel('Trials')\n",
    "ax1.set_ylabel('Human Percent Correct')\n",
    "ax1.set_title(' Human Contour Signal at level of individual trials')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Second subplot: Bottom-left\n",
    "ax2.bar(range(len(model_img_signal_list[0])), model_img_signal_list[0],color='gray')\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.set_xlabel('Trials')\n",
    "ax2.set_ylabel('Distance from Diagonal')\n",
    "ax2.set_title(' Model Contour Signal at level of individual trials')\n",
    "\n",
    "\n",
    "\n",
    "ax3.plot(model_contour_present_list[0,:,1],model_contour_absent_list[0,:,1],'.')\n",
    "ax3.plot(np.arange(-14,14),np.arange(-14,14),linestyle='--',color='k')\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "ax3.spines['left'].set_visible(True)\n",
    "ax3.spines['bottom'].set_visible(True)\n",
    "ax3.set_xlabel('Contour Present Image',fontsize=15,labelpad=10)\n",
    "ax3.set_ylabel('Contour Absent Image',fontsize=15,labelpad=10)\n",
    "\n",
    "ax3.set_ylim(-14,14)\n",
    "ax3.set_xlim(-14,14)\n",
    "ax3.set_title('Activity on the contour Present Node')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Supp. Fig. 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_files = glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/*.pt\")\n",
    "model_list_files=sorted(model_list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model_list_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_human_corr_list=[]\n",
    "model_beta_acc_list=[]\n",
    "\n",
    "\n",
    "model_details=[]\n",
    "\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    print(file)\n",
    "    checkpoint=torch.load(file)\n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    \n",
    "    model_human_corr,_, _, _ = get_model_signal_strength(loaded_spliced_model,human_img_signal)\n",
    "    model_human_corr_list.append(model_human_corr)\n",
    "    model_beta_acc_list.append(get_model_beta_acc(loaded_spliced_model))\n",
    "    \n",
    "    \n",
    "    model_details.append([checkpoint['training_config']['base_model_name'], checkpoint['training_config']['layer_name'], '-'.join(map(str, checkpoint['visual_diet_config']['get_B']))])\n",
    "\n",
    "model_beta_acc_list = np.array(model_beta_acc_list)\n",
    "model_details = np.array(model_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "df.append({'model_name':'human', \n",
    "           'model_human_corr': 0.7856633080741112, \n",
    "           'beta_acc_15_30_45_60_75': condition_accuracy_image, \n",
    "           'model_base_model_name': 'human', \n",
    "           'model_layer_name': 'human', \n",
    "           'model_contourtraining_diet': 'human'})\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    \n",
    "    \n",
    "    df.append({'model_name':file, \n",
    "           'model_human_corr': model_human_corr_list[i], \n",
    "           'beta_acc_15_30_45_60_75': model_beta_acc_list[i], \n",
    "           'model_base_model_name': model_details[i][0], \n",
    "           'model_layer_name': model_details[i][1], \n",
    "           'model_contourtraining_diet': model_details[i][2]})\n",
    "df = pd.DataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Supp. Fig. 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_files =['../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune_beta_000.pt',\n",
    "'../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune_beta_015.pt',\n",
    "'../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune_beta_030.pt',\n",
    "'../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune_beta_045.pt',\n",
    "'../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune_beta_060.pt',\n",
    "'../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune_beta_075.pt']\n",
    "\n",
    "model_list_files=sorted(model_list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model_list_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_human_corr_list=[]\n",
    "model_beta_acc_list=[]\n",
    "\n",
    "\n",
    "model_details=[]\n",
    "\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    print(file)\n",
    "    checkpoint=torch.load(file)\n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    \n",
    "    model_human_corr,_, _, _ = get_model_signal_strength(loaded_spliced_model,human_img_signal)\n",
    "    model_human_corr_list.append(model_human_corr)\n",
    "    model_beta_acc_list.append(get_model_beta_acc(loaded_spliced_model))\n",
    "    \n",
    "    \n",
    "    model_details.append([checkpoint['training_config']['base_model_name'], checkpoint['training_config']['layer_name'], '-'.join(map(str, checkpoint['visual_diet_config']['get_B']))])\n",
    "\n",
    "model_beta_acc_list = np.array(model_beta_acc_list)\n",
    "model_details = np.array(model_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "df.append({'model_name':'human', \n",
    "           'model_human_corr': 0.7856633080741112, \n",
    "           'beta_acc_15_30_45_60_75': condition_accuracy_image, \n",
    "           'model_base_model_name': 'human', \n",
    "           'model_layer_name': 'human', \n",
    "           'model_contourtraining_diet': 'human'})\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    \n",
    "    \n",
    "    df.append({'model_name':file, \n",
    "           'model_human_corr': model_human_corr_list[i], \n",
    "           'beta_acc_15_30_45_60_75': model_beta_acc_list[i], \n",
    "           'model_base_model_name': model_details[i][0], \n",
    "           'model_layer_name': model_details[i][1], \n",
    "           'model_contourtraining_diet': model_details[i][2]})\n",
    "df = pd.DataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Supp. Fig. 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_files = glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_0/*.pt\") \\\n",
    "+ glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_1/*.pt\") \\\n",
    "+ glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_2/*.pt\") \\\n",
    "+ glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_3/*.pt\") \\\n",
    "+ glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_4/*.pt\") \\\n",
    "+ glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_5/*.pt\") \\\n",
    "+ glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_6/*.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model_list_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_human_corr_list=[]\n",
    "model_beta_acc_list=[]\n",
    "\n",
    "\n",
    "model_details=[]\n",
    "\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    print(file)\n",
    "    checkpoint=torch.load(file)\n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    \n",
    "    model_human_corr,_, _, _ = get_model_signal_strength(loaded_spliced_model,human_img_signal)\n",
    "    model_human_corr_list.append(model_human_corr)\n",
    "    model_beta_acc_list.append(get_model_beta_acc(loaded_spliced_model))\n",
    "    \n",
    "    \n",
    "    model_details.append([checkpoint['training_config']['base_model_name'], checkpoint['training_config']['layer_name'], '-'.join(map(str, checkpoint['visual_diet_config']['get_B']))])\n",
    "\n",
    "model_beta_acc_list = np.array(model_beta_acc_list)\n",
    "model_details = np.array(model_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "df.append({'model_name':'human', \n",
    "           'model_human_corr': 0.7856633080741112, \n",
    "           'beta_acc_15_30_45_60_75': condition_accuracy_image, \n",
    "           'model_base_model_name': 'human', \n",
    "           'model_layer_name': 'human', \n",
    "           'model_contourtraining_diet': 'human'})\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    \n",
    "    \n",
    "    df.append({'model_name':file, \n",
    "           'model_human_corr': model_human_corr_list[i], \n",
    "           'beta_acc_15_30_45_60_75': model_beta_acc_list[i], \n",
    "           'model_base_model_name': model_details[i][0], \n",
    "           'model_layer_name': model_details[i][1], \n",
    "           'model_contourtraining_diet': model_details[i][2]})\n",
    "df = pd.DataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">Generate the results_contour_readout_models.csv file for all the provided models</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_list_files = glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_0/*.pt\") \\\n",
    "# + glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_1/*.pt\") \\\n",
    "# + glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_2/*.pt\") \\\n",
    "# + glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_3/*.pt\") \\\n",
    "# + glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_4/*.pt\") \\\n",
    "# + glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_5/*.pt\") \\\n",
    "# + glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_6/*.pt\") \\\n",
    "# + glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/*.pt\") \\\n",
    "# + glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_frozen_broad/*.pt\") \\\n",
    "# + glob.glob(\"../model_weights/contour_model_weights/alexnet-bagnet11_regimagenet_categ_finetune_broad/*.pt\") \\\n",
    "# + glob.glob(\"../model_weights/contour_model_weights/alexnet-bagnet17_regimagenet_categ_finetune_broad/*.pt\") \\\n",
    "# + glob.glob(\"../model_weights/contour_model_weights/alexnet-bagnet31_regimagenet_categ_finetune_broad/*.pt\") \\\n",
    "# + glob.glob(\"../model_weights/contour_model_weights/alexnet-bagnet33_regimagenet_categ_finetune_broad/*.pt\") \\\n",
    "# + glob.glob(\"../model_weights/contour_model_weights/alexnet-epoch100_regimagenet_categ_finetune_broad/*.pt\") \\\n",
    "# + glob.glob(\"../model_weights/contour_model_weights/alexnet-epoch50_regimagenet_categ_finetune_broad/*.pt\") \\\n",
    "# + glob.glob(\"../model_weights/contour_model_weights/alexnet-random-nodata-notask_finetune_broad/*.pt\") \\\n",
    "# + glob.glob(\"../model_weights/contour_model_weights/alexnet-random-nodata-notask_frozen_broad/*.pt\")\n",
    "\n",
    "\n",
    "# print(len(model_list_files))\n",
    "\n",
    "# model_human_corr_list=[]\n",
    "# model_beta_acc_list=[]\n",
    "# model_details=[]\n",
    "\n",
    "\n",
    "# for i,file in tqdm(enumerate(model_list_files)):\n",
    "#     print(file)\n",
    "#     checkpoint=torch.load(file)\n",
    "#     loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "#     loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    \n",
    "#     model_human_corr,_, _, _ = get_model_signal_strength(loaded_spliced_model,human_img_signal)\n",
    "#     model_human_corr_list.append(model_human_corr)\n",
    "#     model_beta_acc_list.append(get_model_beta_acc(loaded_spliced_model))\n",
    "    \n",
    "    \n",
    "#     model_details.append([checkpoint['training_config']['base_model_name'], checkpoint['training_config']['layer_name'], '-'.join(map(str, checkpoint['visual_diet_config']['get_B'])), str(checkpoint['training_config']['fine_tune'])])\n",
    "\n",
    "# model_beta_acc_list = np.array(model_beta_acc_list)\n",
    "# model_details = np.array(model_details)\n",
    "\n",
    "\n",
    "# df = []\n",
    "# df.append({'model_name':'human', \n",
    "#            'model_human_corr': 0.7856633080741112, \n",
    "#            'beta_acc_15_30_45_60_75': condition_accuracy_image, \n",
    "#            'model_base_model_name': 'human', \n",
    "#            'model_layer_name': 'human', \n",
    "#            'model_contourtraining_diet': 'human', \n",
    "#            'model_contourtraining_finetune': 'human'})\n",
    "\n",
    "# for i,file in tqdm(enumerate(model_list_files)):\n",
    "    \n",
    "    \n",
    "#     df.append({'model_name':file, \n",
    "#            'model_human_corr': model_human_corr_list[i], \n",
    "#            'beta_acc_15_30_45_60_75': model_beta_acc_list[i], \n",
    "#            'model_base_model_name': model_details[i][0], \n",
    "#            'model_layer_name': model_details[i][1], \n",
    "#            'model_contourtraining_diet': model_details[i][2], 'model_contourtraining_finetune': model_details[i][3]})\n",
    "# df = pd.DataFrame(df)\n",
    "# df\n",
    "\n",
    "# df = df.reset_index().rename(columns={'index': 'model_index'})\n",
    "# df.to_csv('results_contour_readout_models.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"color:red\"> Figure : SFig. 10   </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_for_additional_model_weights = '../relevant_files/additional_model_weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=get_B,get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuned Alexnet, Pretrained Alexnet and Pretrained Resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the accuracy data for all models over the validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915a7a98bc5b412fa1aa57c67a818eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alexnet_regular_finetune_files =  glob.glob(os.path.join(path_for_additional_model_weights, 'alexnet_finetune/*finetune.pt'))\n",
    "alexnet_regular_finetune_acc={'train':get_train_val_acc(alexnet_regular_finetune_files)[0],'val':get_train_val_acc(alexnet_regular_finetune_files)[1]}\n",
    "alexnet_regular_finetune_predictions=get_prediction(alexnet_regular_finetune_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06772e1666204591ad1fa6dbf7c2d7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alexnet_regular_frozen_files =  glob.glob(os.path.join(path_for_additional_model_weights, 'alexnet/*frozen.pt'))\n",
    "alexnet_regular_frozen_acc={'train':get_train_val_acc(alexnet_regular_frozen_files)[0],'val':get_train_val_acc(alexnet_regular_frozen_files)[1]}\n",
    "alexnet_regular_frozen_predictions=get_prediction(alexnet_regular_frozen_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183f8281985d4ed785437c9f614c73d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resnet50_regular_frozen_files =  glob.glob(os.path.join(path_for_additional_model_weights, 'resnet50/*frozen.pt'))\n",
    "resnet50_regular_frozen_acc={'train':get_train_val_acc(resnet50_regular_frozen_files)[0],'val':get_train_val_acc(resnet50_regular_frozen_files)[1]}\n",
    "resnet50_regular_frozen_predictions=get_prediction(resnet50_regular_frozen_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_layers = ['features_0',\n",
    " 'features_1',\n",
    " 'features_2',\n",
    " 'features_3',\n",
    " 'features_4',\n",
    " 'features_5',\n",
    " 'features_6',\n",
    " 'features_7',\n",
    " 'features_8',\n",
    " 'features_9',\n",
    " 'features_10',\n",
    " 'features_11',\n",
    " 'features_12',\n",
    " 'avgpool',\n",
    " 'classifier_0',\n",
    " 'classifier_1',\n",
    " 'classifier_2',\n",
    " 'classifier_3',\n",
    " 'classifier_4',\n",
    " 'classifier_5',\n",
    " 'classifier_6']\n",
    "\n",
    "for i in range(len(alexnet_layers)):\n",
    "    alexnet_layers[i]=alexnet_layers[i].replace('_','.')\n",
    "\n",
    "finetunealexnet_layers = alexnet_layers\n",
    "\n",
    "resnet50_layers = ['relu', 'layer2', 'layer3', 'layer4', 'avgpool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetunealexnet_means,_,finetunealexnet_int,_,_=get_list_acc_mean_std_ci(alexnet_regular_finetune_predictions,finetunealexnet_layers)\n",
    "\n",
    "alexnet_means,_,alexnet_int,_,_=get_list_acc_mean_std_ci(alexnet_regular_frozen_predictions,alexnet_layers)\n",
    "\n",
    "resnet50_means,_,resnet50_int,_,_=get_list_acc_mean_std_ci(resnet50_regular_frozen_predictions,resnet50_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "img_dim = 224\n",
    "\n",
    "\n",
    "data_transform = transforms.Compose([       \n",
    " transforms.Resize(img_dim, interpolation=transforms.InterpolationMode.BICUBIC),                   \n",
    " transforms.CenterCrop((img_dim,img_dim)),         \n",
    " transforms.ToTensor(),                    \n",
    " transforms.Normalize(                      \n",
    " mean=[0.5000, 0.5000, 0.5000],                \n",
    " std=[0.5000, 0.5000, 0.5000]                  \n",
    " )])\n",
    "\n",
    "data_transform_without_norm = transforms.Compose([       \n",
    " transforms.Resize(img_dim),                   \n",
    " transforms.CenterCrop((img_dim,img_dim)),         \n",
    " transforms.ToTensor()                    \n",
    " ])\n",
    "\n",
    "val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=get_B,get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "\n",
    "inputs, b, d, a, nel, labels, record =next(iter(val_loader_norm))\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ed2d27c23a47d48c64050dafcd9f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timmvit_regular_frozen_files =  glob.glob(os.path.join(path_for_additional_model_weights, 'vit_timm_bicubic_batch8/*frozen.pt'))\n",
    "timmvit_regular_frozen_acc={'train':get_train_val_acc(timmvit_regular_frozen_files)[0],'val':get_train_val_acc(timmvit_regular_frozen_files)[1]}\n",
    "timmvit_regular_frozen_predictions=get_prediction(timmvit_regular_frozen_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "timmvit_layers = ['blocks.0', 'blocks.1', 'blocks.2', 'blocks.3', 'blocks.4',\n",
    "               'blocks.5', 'blocks.6', 'blocks.7', 'blocks.8', 'blocks.9',\n",
    "               'blocks.10', 'blocks.11']\n",
    "\n",
    "\n",
    "timmvit_means,_,timmvit_int,_,_=get_list_acc_mean_std_ci(timmvit_regular_frozen_predictions,timmvit_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot with all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your specific base color\n",
    "color_or_finetune=(78/255,121/255,180/255)\n",
    "color_or_frozen = np.array([209/255, 111/255, 28/255])  # Example color in RGBRGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a gradient of colors\n",
    "def create_color_gradient(base_color, num_colors, darkening_factor=0.1):\n",
    "    # Convert the base_color to a NumPy array for element-wise operations\n",
    "    base_color = np.array(base_color)\n",
    "    # Create a darker shade of the base color\n",
    "    darker_color = base_color * darkening_factor\n",
    "    # Generate a gradient from base_color to darker_color\n",
    "    cmap = LinearSegmentedColormap.from_list(\"gradient\", [darker_color, base_color])\n",
    "    return cmap(np.linspace(0, 1, num_colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate color shades for AlexNet and ResNet50\n",
    "finetunealexnet_colors = create_color_gradient(color_or_finetune, len(finetunealexnet_layers))  \n",
    "alexnet_colors = create_color_gradient(color_or_frozen, len(alexnet_layers))  \n",
    "resnet50_colors = create_color_gradient(color_or_frozen, len(resnet50_layers))\n",
    "timmvit_colors = create_color_gradient(color_or_frozen, len(timmvit_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Bar width\n",
    "width = 0.7\n",
    "# Bar caosize\n",
    "capsize = 2\n",
    "\n",
    "# Positions for AlexNet and ResNet50\n",
    "finetunealexnet_positions = np.arange(len(finetunealexnet_layers))\n",
    "alexnet_positions = np.arange(len(alexnet_layers))\n",
    "resnet50_positions = np.arange(len(resnet50_layers))\n",
    "timmvit_positions = np.arange(len(timmvit_layers))\n",
    "\n",
    "\n",
    "# Offset values\n",
    "finetunealexnet_offset = 0\n",
    "alexnet_offset = len(finetunealexnet_layers) + 5\n",
    "resnet50_offset = len(finetunealexnet_layers) + len(alexnet_layers) + 10\n",
    "timmvit_offset = len(finetunealexnet_layers) + len(alexnet_layers) + len(resnet50_layers) + 15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot AlexNet-Finetune\n",
    "ax.bar(finetunealexnet_positions + finetunealexnet_offset, finetunealexnet_means, yerr=finetunealexnet_int, \n",
    "       width=width, capsize=capsize, color=finetunealexnet_colors, edgecolor='black', label='Finetune AlexNet', alpha=0.7)\n",
    "\n",
    "# Plot AlexNet\n",
    "ax.bar(alexnet_positions + alexnet_offset, alexnet_means, yerr=alexnet_int, \n",
    "       width=width, capsize=capsize, color=alexnet_colors, edgecolor='black', label='AlexNet', alpha=0.7)\n",
    "\n",
    "# Plot ResNet50\n",
    "ax.bar(resnet50_positions + resnet50_offset, resnet50_means, yerr=resnet50_int, \n",
    "       width=width, capsize=capsize, color=resnet50_colors, edgecolor='black', label='ResNet50', alpha=0.7)\n",
    "\n",
    "\n",
    "# Plot Timmvit\n",
    "ax.bar(timmvit_positions + timmvit_offset, timmvit_means, yerr=timmvit_int, \n",
    "       width=width, capsize=capsize, color=timmvit_colors, edgecolor='black', label='timmVit', alpha=0.7)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.axhline(y=0.5,linestyle='--',alpha=0.3,color='r',label='chance')\n",
    "plt.ylim(0.4,1.01)\n",
    "\n",
    "# Setting labels and titles\n",
    "ax.set_ylabel('Contour Readout Accuracy')\n",
    "ax.set_title('Performance for Finetuned Alexnet, Frozen Alexnet, Frozen Resnet50, Frozen TimmVit')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xticks(np.concatenate([finetunealexnet_positions + finetunealexnet_offset,\n",
    "                              alexnet_positions + alexnet_offset,\n",
    "                              resnet50_positions + resnet50_offset,\n",
    "                              timmvit_positions + timmvit_offset]))\n",
    "\n",
    "ax.set_xticklabels([f'Finetune AlexNet Layer {i+1}' for i in finetunealexnet_positions] +\n",
    "                   [f'AlexNet Layer {i+1}' for i in alexnet_positions] +\n",
    "                   [f'ResNet50 Layer {i+1}' for i in resnet50_positions] +\n",
    "                   [f'Timm Vit Layer {i+1}' for i in timmvit_positions], rotation = 90)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.legend()\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./supp_f10.png',bbox_inches = 'tight', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop_dev",
   "language": "python",
   "name": "workshop_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
