{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "sys.path.insert(1,'../contour_integ_models/')\n",
    "\n",
    "\n",
    "# Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Pytorch related\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils import data as dt\n",
    "from torchinfo import summary\n",
    "import torchvision.models as pretrained_models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils import model_zoo\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "# Numpy, Matplotlib, Pandas, Sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from sklearn import manifold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "# python utilities\n",
    "from itertools import combinations\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import copy\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "from IPython.display import Image\n",
    "from IPython.core.debugger import set_trace\n",
    "import collections\n",
    "from functools import partial\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "\n",
    "from PIL import Image, ImageStat\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "# Extra imports\n",
    "from lib.feature_extractor import FeatureExtractor\n",
    "from lib.custom_dataset import Contour_Dataset, Psychophysics_Dataset\n",
    "from lib.build_fe_ft_models import *\n",
    "from lib.misc_functions import *\n",
    "from lib.field_stim_functions import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dim=512\n",
    "\n",
    "batch_size=32\n",
    "num_workers=8\n",
    "device = torch.device('cuda:'+'3')\n",
    "# device = 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all the values from the config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdiet_savedmodel_config import *\n",
    "\n",
    "print('\\n Visual Diet config \\n')\n",
    "print(visual_diet_config)\n",
    "\n",
    "\n",
    "print('\\n Psychophysics Visual Diet config \\n')\n",
    "print(psychophysics_visual_diet_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory=visual_diet_config['root_directory']\n",
    "get_B=visual_diet_config['get_B']\n",
    "get_D=visual_diet_config['get_D']\n",
    "get_A=visual_diet_config['get_A']\n",
    "get_numElements=visual_diet_config['get_numElements']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize images using parameters from the training image set\n",
    "data_transform = transforms.Compose([       \n",
    " transforms.Resize(img_dim),                   \n",
    " transforms.CenterCrop((img_dim,img_dim)),         \n",
    " transforms.ToTensor(),                    \n",
    " transforms.Normalize(                      \n",
    " mean=[0.485, 0.456, 0.406],                \n",
    " std=[0.229, 0.224, 0.225]                  \n",
    " )])\n",
    "\n",
    "data_transform_without_norm = transforms.Compose([       \n",
    " transforms.Resize(img_dim),                   \n",
    " transforms.CenterCrop((img_dim,img_dim)),         \n",
    " transforms.ToTensor()                    \n",
    " ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diff_means(arr1, arr2):\n",
    "    mean1 = np.mean(arr1)\n",
    "    mean2 = np.mean(arr2)\n",
    "    \n",
    "    return (mean1 - mean2)\n",
    "\n",
    "def compute_dprime_means(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Compute d' based on the difference of means divided by the pooled standard deviation.\n",
    "    \"\"\"\n",
    "    mean1 = np.mean(arr1)\n",
    "    mean2 = np.mean(arr2)\n",
    "    \n",
    "    var1 = np.var(arr1, ddof=1)  # ddof=1 for sample variance\n",
    "    var2 = np.var(arr2, ddof=1)  # ddof=1 for sample variance\n",
    "    \n",
    "    # Calculate the pooled standard deviation\n",
    "    pooled_std = np.sqrt((var1 + var2) / 2)\n",
    "    \n",
    "    # Compute d'\n",
    "    d_prime = (mean1 - mean2) / pooled_std\n",
    "    \n",
    "    return d_prime\n",
    "\n",
    "def compute_aprime_means(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Compute A' as the area under the ROC curve based on two input arrays.\n",
    "    \"\"\"\n",
    "    # Combine the arrays and create corresponding labels\n",
    "    combined = np.concatenate((arr1, arr2))\n",
    "    labels = np.concatenate((np.ones(len(arr1)), np.zeros(len(arr2))))\n",
    "    \n",
    "    # Compute AUC\n",
    "    a_prime = roc_auc_score(labels, combined)\n",
    "    \n",
    "    return a_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_acc(files):\n",
    "    all_models_train_acc={}\n",
    "    all_models_val_acc={}\n",
    "\n",
    "    for f in files:\n",
    "        checkpoint=torch.load(f)\n",
    "        all_models_train_acc[checkpoint['training_config']['layer_name']]=checkpoint['metrics']['train_acc'][-1]\n",
    "        all_models_val_acc[checkpoint['training_config']['layer_name']]=checkpoint['metrics']['val_acc'][-1]\n",
    "\n",
    "\n",
    "\n",
    "    myKeys = list(all_models_train_acc.keys())\n",
    "    myKeys.sort()\n",
    "    all_models_train_acc = {i: all_models_train_acc[i] for i in myKeys}\n",
    "\n",
    "\n",
    "\n",
    "    myKeys = list(all_models_val_acc.keys())\n",
    "    myKeys.sort()\n",
    "    all_models_val_acc = {i: all_models_val_acc[i] for i in myKeys}\n",
    "    \n",
    "    \n",
    "    return all_models_train_acc,all_models_val_acc\n",
    "\n",
    "def get_list_acc(acc_dict,list_layers):\n",
    "    acc=[]\n",
    "    for i in range(len(list_layers)):\n",
    "        # layer=list_layers[i].replace('_','.')\n",
    "        layer=list_layers[i]\n",
    "        if layer in acc_dict.keys():\n",
    "            acc.append(acc_dict[layer])\n",
    "            \n",
    "    return acc\n",
    "\n",
    "\n",
    "def get_prediction(model_file_paths):\n",
    "    model_valacc_scores={}\n",
    "    for file in tqdm(model_file_paths):\n",
    "\n",
    "        # Model\n",
    "        checkpoint=torch.load(file)\n",
    "        # loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device='cpu')\n",
    "        loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "        loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "        \n",
    "        change_train_eval_mode(loaded_spliced_model,loaded_spliced_model.fine_tune,train_eval_mode='eval')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        model_valacc_scores[checkpoint['training_config']['layer_name']]=np.array([])\n",
    "        for (inputs, b, d, alpha, nel, labels, record) in tqdm(val_loader_norm,disable=True):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs= loaded_spliced_model.forward(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            model_valacc_scores[checkpoint['training_config']['layer_name']]=np.concatenate((model_valacc_scores[checkpoint['training_config']['layer_name']],(preds == labels).float().cpu().numpy()))\n",
    "    \n",
    "    \n",
    "    return model_valacc_scores\n",
    "\n",
    "def get_list_acc_mean_std_ci(sal_dict,list_layers):\n",
    "    mean_sal=[]\n",
    "    std_sal=[]\n",
    "    confint_sal=[]\n",
    "    \n",
    "    \n",
    "    confint_lower=[]\n",
    "    confint_upper=[]\n",
    "    \n",
    "    for i in range(len(list_layers)):\n",
    "        # layer=list_layers[i].replace('_','.')\n",
    "        layer=list_layers[i]\n",
    "        if layer in sal_dict.keys():\n",
    "            mean_sal.append(np.mean(sal_dict[layer]))\n",
    "            \n",
    "            std_sal.append(np.std(sal_dict[layer],ddof=1))\n",
    "            \n",
    "            confint_sal.append(1.96*(np.std(sal_dict[layer],ddof=1) / np.sqrt(len(sal_dict[layer]))))\n",
    "            \n",
    "            confint_lower.append(np.percentile(sal_dict[layer], 2.5))\n",
    "            confint_upper.append(np.percentile(sal_dict[layer], 97.5))\n",
    "            \n",
    "    return mean_sal,std_sal,confint_sal,confint_lower,confint_upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">All Figures</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"color:red\">Figure : Fig. 2A</h1>\n",
    "\n",
    "## Description: Contour Readout Accuracies for Finetune, Frozen and Random Alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the dataset and set of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=get_B,get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the accuracy data for all models over the validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_regular_finetune_files = glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/*finetune.pt\")\n",
    "sup_regular_finetune_acc={'train':get_train_val_acc(sup_regular_finetune_files)[0],'val':get_train_val_acc(sup_regular_finetune_files)[1]}\n",
    "sup_regular_finetune_predictions=get_prediction(sup_regular_finetune_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_regular_frozen_files =  glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_frozen_broad/*frozen.pt\")\n",
    "sup_regular_frozen_acc={'train':get_train_val_acc(sup_regular_frozen_files)[0],'val':get_train_val_acc(sup_regular_frozen_files)[1]}\n",
    "sup_regular_frozen_predictions=get_prediction(sup_regular_frozen_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_regular_frozen_files =  glob.glob(\"../model_weights/contour_model_weights/alexnet-random-nodata-notask_frozen_broad/*frozen.pt\")\n",
    "random_regular_frozen_acc={'train':get_train_val_acc(random_regular_frozen_files)[0],'val':get_train_val_acc(random_regular_frozen_files)[1]}\n",
    "random_regular_frozen_predictions=get_prediction(random_regular_frozen_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting all the accuracy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_layers=['features_0',\n",
    " 'features_1',\n",
    " 'features_2',\n",
    " 'features_3',\n",
    " 'features_4',\n",
    " 'features_5',\n",
    " 'features_6',\n",
    " 'features_7',\n",
    " 'features_8',\n",
    " 'features_9',\n",
    " 'features_10',\n",
    " 'features_11',\n",
    " 'features_12',\n",
    " 'avgpool',\n",
    " 'classifier_0',\n",
    " 'classifier_1',\n",
    " 'classifier_2',\n",
    " 'classifier_3',\n",
    " 'classifier_4',\n",
    " 'classifier_5',\n",
    " 'classifier_6']\n",
    "\n",
    "for i in range(len(list_layers)):\n",
    "    list_layers[i]=list_layers[i].replace('_','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "\n",
    "\n",
    "spacing_x=2\n",
    "jitter_stylized=0.4\n",
    "\n",
    "color_or_finetune=(78/255,121/255,180/255)\n",
    "color_or_frozen=(209/255,111/255,28/255)\n",
    "color_stylized_frozen=(231/255,168/255,34/255)\n",
    "\n",
    "error_bar_width=3\n",
    "error_bar_cap=4\n",
    "\n",
    "\n",
    "\n",
    "mean_sal,std_sal,confint_sal,confint_lower,confint_upper=get_list_acc_mean_std_ci(sup_regular_finetune_predictions,list_layers)\n",
    "plt.errorbar(np.arange(len(mean_sal)) * spacing_x, mean_sal, \n",
    "             yerr=confint_sal, \n",
    "             capsize=error_bar_cap,elinewidth=error_bar_width,fmt='o', marker='o', color=color_or_finetune, markersize=10, \n",
    "             label='or finetune', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mean_sal,std_sal,confint_sal,confint_lower,confint_upper=get_list_acc_mean_std_ci(sup_regular_frozen_predictions,list_layers)\n",
    "plt.errorbar(np.arange(len(mean_sal)) * spacing_x, mean_sal, \n",
    "             yerr=confint_sal, \n",
    "             capsize=error_bar_cap,elinewidth=error_bar_width,fmt='s', marker='o', color=color_or_frozen, markersize=10, \n",
    "             label='or frozen', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "mean_sal,std_sal,confint_sal,confint_lower,confint_upper=get_list_acc_mean_std_ci(random_regular_frozen_predictions,list_layers)\n",
    "plt.errorbar(np.arange(len(mean_sal)) * spacing_x, mean_sal, \n",
    "             yerr=confint_sal, \n",
    "             capsize=error_bar_cap,elinewidth=error_bar_width,fmt='d', marker='o', color='gray', markersize=10, \n",
    "             label='random frozen', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "plt.axhline(y=0.5,linestyle='--',alpha=0.3,color='r',label='chance')\n",
    "\n",
    "\n",
    "# for i in range(len(get_list_acc(sup_regular_finetune_acc['val'],list_layers))):\n",
    "#     plt.axvline(i*2,linestyle='--',alpha=0.1,color='k')\n",
    "\n",
    "plt.xticks(np.arange(len(get_list_acc(sup_regular_finetune_acc['val'],list_layers))) * spacing_x,list_layers,rotation=90,fontsize=15)\n",
    "plt.yticks(fontsize=15) \n",
    "plt.ylim(0.4,1.01)\n",
    "plt.ylabel('Validation Accuracy',fontsize=15)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"color:red\">Figure : Fig. 2B and SFig. 2A </h1>\n",
    "\n",
    "## Description: Location Sensitivity for avgpool and conv2 finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(model_file, beta_val=15,img_num=4):\n",
    "    \n",
    "    \n",
    "    ### Show the image\n",
    "    val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=[beta_val],get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "    val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    val_dataset_without_norm = Contour_Dataset(root=root_directory,transform=data_transform_without_norm,train=False,get_B=[beta_val],get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "    val_loader_without_norm = torch.utils.data.DataLoader(dataset=val_dataset_without_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    a, b, d, alpha, nel, labels, record =next(iter(val_loader_norm))\n",
    "    prep_img=torch.unsqueeze(a[img_num],0)\n",
    "    print('Beta Val: ',b[img_num])\n",
    "    print('Contour Present: ',labels[img_num])\n",
    "    prep_img=prep_img.to(device)\n",
    "    prep_img=Variable(prep_img,requires_grad=True)\n",
    "    \n",
    "    ## Plot the orginal image - this is the normalized one, the one that will actually be used\n",
    "    # original_img=np.transpose(prep_img[0].detach().cpu().numpy(),(1,2,0))\n",
    "    # plt.figure(figsize=(10,8))\n",
    "    # plt.imshow(original_img)\n",
    "    # plt.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Plot the orginal unnormalized image\n",
    "    plt.figure(figsize=(10,8))\n",
    "    a_without_norm,b, d, alpha, nel, labels, record=next(iter(val_loader_without_norm))\n",
    "    plt.imshow(np.transpose(torch.unsqueeze(a_without_norm[img_num],0)[0].numpy(),(1,2,0)))\n",
    "    plt.title('Original Image - visualizing unnormalized image here')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    # plt.savefig('./dev/ccn_figures/poster/original_image.png', bbox_inches='tight',dpi=300) \n",
    "    # plt.close()\n",
    "    \n",
    "\n",
    "    checkpoint=torch.load(model_file)\n",
    "\n",
    "    ####################################################################################################\n",
    "    ## Showing the Model Stats\n",
    "    print('Base Model is',checkpoint['training_config']['base_model_name'])\n",
    "    print('Layer Readout is',checkpoint['training_config']['layer_name'])\n",
    "    print('Final Val Accuracy is',checkpoint['metrics']['val_acc'][-1])\n",
    "    ####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ####################################################################################################\n",
    "    ## Loading the model and setting up the Guided Spliced Model for Attention Maps\n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    GBP = Spliced_GuidedBackprop(loaded_spliced_model,device)\n",
    "    ####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ####################################################################################################\n",
    "    ## Getting the Saliency Maps\n",
    "    # Get gradients\n",
    "    guided_grads = GBP.generate_gradients(prep_img, 1)\n",
    "\n",
    "\n",
    "    # Gradients\n",
    "    guided_grads_vis = guided_grads - guided_grads.min()\n",
    "    guided_grads_vis /= guided_grads_vis.max()\n",
    "\n",
    "#     plt.figure(figsize=(20,16))\n",
    "    \n",
    "    \n",
    "#     plt.subplot(2,2,3)\n",
    "#     plt.imshow(np.transpose(guided_grads_vis,(1,2,0)))\n",
    "#     plt.axis('off')\n",
    "\n",
    "\n",
    "    # Grayscale Gradients\n",
    "    grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
    "\n",
    "\n",
    "    grayscale_guided_grads_vis = grayscale_guided_grads - grayscale_guided_grads.min()\n",
    "    grayscale_guided_grads_vis /= grayscale_guided_grads_vis.max()\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(20,16))\n",
    "    \n",
    "    \n",
    "    plt.subplot(2,2,3)\n",
    "    plt.imshow(np.transpose(guided_grads_vis,(1,2,0)))\n",
    "    plt.title('Saliency Map overlayed on image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2,2,4)\n",
    "    plt.imshow(np.squeeze(np.transpose(grayscale_guided_grads_vis,(1,2,0))),cmap='gray')\n",
    "    plt.title('Saliency Map')\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "    ## Compute saliency maps\n",
    "    print(grayscale_guided_grads_vis.shape)\n",
    "    viz_image=grayscale_guided_grads_vis[0]\n",
    "    ####################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ####################################################################################################\n",
    "    ## Compute saliency scores\n",
    "    mask_img_path_fg, mask_img_path_bg, list_gauss=mask_renderer(torch.load(record[img_num]),140)\n",
    "    mask_img_path_fg=np.array(mask_img_path_fg)\n",
    "    mask_img_path_bg=np.array(mask_img_path_bg)\n",
    "\n",
    "    \n",
    "    plt.subplot(2,2,1)\n",
    "    plt.imshow(mask_img_path_fg,cmap='gray')\n",
    "    plt.title('Contour Element Locations')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.imshow(mask_img_path_bg,cmap='gray')\n",
    "    plt.title('Background Element Locations')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    saliency_score_diffmean=compute_diff_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    saliency_score_dprime=compute_dprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    saliency_score_aprime=compute_aprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "\n",
    "\n",
    "    print('saliency_score_diffmean: \\t', saliency_score_diffmean)\n",
    "    print('saliency_score_diffmean: \\t', saliency_score_dprime)\n",
    "    print('saliency_score_diffmean: \\t',saliency_score_aprime)\n",
    "    ####################################################################################################\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(model_file='../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-3_mode_finetune.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(model_file='../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"color:red\"> Figure : Fig. 2D and SFig. 2C-Inset  </h1>\n",
    "\n",
    "## Description: Alignment Sensitivity for avgpool finetuned model\n",
    "This analysis takes roughly **25 minutes** to complete on 600 validation images when run on the contour model reading out from avgpool layer of Alexnet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=get_B,get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "# val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=1, num_workers=num_workers, shuffle=False)\n",
    "print(len(val_dataset_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the model from the saved model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_file='../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune.pt'\n",
    "print(selected_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Saliency for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency(prep_img,prep_recorder,file):\n",
    "    prep_img=Variable(prep_img,requires_grad=True).to(device)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Loading the model and setting up the Guided Spliced Model for Attention Maps   \n",
    "    ## Model\n",
    "    checkpoint=torch.load(selected_file)\n",
    "    \n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    GBP = Spliced_GuidedBackprop(loaded_spliced_model,device)\n",
    "    ####################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ####################################################################################################\n",
    "    ## Getting the Saliency Maps\n",
    "    # Get gradients\n",
    "    guided_grads = GBP.generate_gradients(prep_img, 1)\n",
    "    # Gradients\n",
    "    guided_grads_vis = guided_grads - guided_grads.min()\n",
    "    guided_grads_vis /= guided_grads_vis.max()\n",
    "\n",
    "    # Grayscale Gradients\n",
    "    grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
    "    grayscale_guided_grads_vis = grayscale_guided_grads - grayscale_guided_grads.min()\n",
    "    grayscale_guided_grads_vis /= grayscale_guided_grads_vis.max()\n",
    "    \n",
    "    \n",
    "    viz_image=grayscale_guided_grads_vis[0]\n",
    "    ####################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Compute saliency scores\n",
    "    mask_img_path_fg, mask_img_path_bg, list_gauss=mask_renderer(torch.load(prep_recorder),140)\n",
    "    mask_img_path_fg=np.array(mask_img_path_fg)\n",
    "    mask_img_path_bg=np.array(mask_img_path_bg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    saliency_score_diffmean=compute_diff_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    saliency_score_dprime=compute_dprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    saliency_score_aprime=compute_aprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Making the run more memory efficient\n",
    "    loaded_spliced_model=loaded_spliced_model.to('cpu')\n",
    "    del GBP\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    return saliency_score_diffmean,saliency_score_dprime,saliency_score_aprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_saliency_score_diff=[]\n",
    "all_saliency_score_dprime=[]\n",
    "all_saliency_score_aprime=[]\n",
    "\n",
    "all_betas=[]\n",
    "all_labels=[]\n",
    "\n",
    "for (a, b, d, alpha, nel, labels, record) in tqdm(val_loader_norm):\n",
    "    for img_num in tqdm(range(a.shape[0]),disable=True):\n",
    "        \n",
    "        prep_img=torch.unsqueeze(a[img_num],0)\n",
    "        prep_recorder=record[img_num]\n",
    "        \n",
    "        saliency_score_diffmean,saliency_score_dprime,saliency_score_aprime=compute_saliency(prep_img,prep_recorder,selected_file)\n",
    "        \n",
    "        all_saliency_score_diff.append(saliency_score_diffmean)\n",
    "        all_saliency_score_dprime.append(saliency_score_dprime)\n",
    "        all_saliency_score_aprime.append(saliency_score_aprime)\n",
    "        \n",
    "        \n",
    "        \n",
    "    all_betas.append(b)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "all_betas=torch.cat(all_betas).numpy()\n",
    "all_labels=torch.cat(all_labels).numpy()\n",
    "all_saliency_score_diff=np.array(all_saliency_score_diff)\n",
    "all_saliency_score_dprime=np.array(all_saliency_score_dprime)\n",
    "all_saliency_score_aprime=np.array(all_saliency_score_aprime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_present_pos=np.where(all_labels==1)[0]\n",
    "contour_absent_pos=np.where(all_labels==0)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment Sensitivity via Diff. of Means Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "# Create a GridSpec object\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "ax1 = plt.subplot(gs[:, 0])\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "ax3 = plt.subplot(gs[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "# Scatter Plot with Connecting Lines\n",
    "for x1, x2 in zip(all_saliency_score_diff[contour_absent_pos], all_saliency_score_diff[contour_present_pos]):\n",
    "    ax1.plot([1, 4], [x1, x2], 'ko-', markersize=8,alpha=0.09)\n",
    "ax1.set_xticks([1, 4], ['Contour absent', 'Contour present'])\n",
    "ax1.set_xlim(0,5)\n",
    "ax1.set_ylabel('saliency_score_diffmean')\n",
    "ax1.set_title('Contour Sensitivity using Diff. Means')\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "sns.distplot(all_saliency_score_diff[contour_present_pos], ax=ax2, label='contour_present',color='green')\n",
    "sns.distplot(all_saliency_score_diff[contour_absent_pos], ax=ax2, label='contour_absent',color='gray')\n",
    "ax2.set_title('Contour Sensitivity using Diff. Means (Histogram)')\n",
    "ax2.legend()\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "sns.distplot(all_saliency_score_diff[contour_present_pos] - all_saliency_score_diff[contour_absent_pos], ax=ax3, label='difference',color='k')\n",
    "ax3.set_title('Alignment Sensitivity using Diff. Means')\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Perform the paired t-test\n",
    "t_statistic, p_value = stats.ttest_rel(all_saliency_score_diff[contour_present_pos], all_saliency_score_diff[contour_absent_pos])\n",
    "print(f\"t-statistic: {t_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment Sensitivity via DPrime Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "# Create a GridSpec object\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "ax1 = plt.subplot(gs[:, 0])\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "ax3 = plt.subplot(gs[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "# Scatter Plot with Connecting Lines\n",
    "for x1, x2 in zip(all_saliency_score_dprime[contour_absent_pos], all_saliency_score_dprime[contour_present_pos]):\n",
    "    ax1.plot([1, 4], [x1, x2], 'ko-', markersize=8,alpha=0.09)\n",
    "ax1.set_xticks([1, 4], ['Contour absent', 'Contour present'])\n",
    "ax1.set_xlim(0,5)\n",
    "ax1.set_ylabel('saliency_score_dprime')\n",
    "ax1.set_title('Contour Sensitivity using DPrime')\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "sns.distplot(all_saliency_score_dprime[contour_present_pos], ax=ax2, label='contour_present',color='green')\n",
    "sns.distplot(all_saliency_score_dprime[contour_absent_pos], ax=ax2, label='contour_absent',color='gray')\n",
    "ax2.set_title('Contour Sensitivity using DPrime (Histogram)')\n",
    "ax2.legend()\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "sns.distplot(all_saliency_score_dprime[contour_present_pos] - all_saliency_score_dprime[contour_absent_pos], ax=ax3, label='difference',color='k')\n",
    "ax3.set_title('Alignment Sensitivity using DPrime')\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Perform the paired t-test\n",
    "t_statistic, p_value = stats.ttest_rel(all_saliency_score_dprime[contour_present_pos], all_saliency_score_dprime[contour_absent_pos])\n",
    "print(f\"t-statistic: {t_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment Sensitivity via APrime Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "# Create a GridSpec object\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "ax1 = plt.subplot(gs[:, 0])\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "ax3 = plt.subplot(gs[1, 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Left column, spanning both rows\n",
    "# Scatter Plot with Connecting Lines\n",
    "for x1, x2 in zip(all_saliency_score_aprime[contour_absent_pos], all_saliency_score_aprime[contour_present_pos]):\n",
    "    ax1.plot([1, 4], [x1, x2], 'ko-', markersize=8,alpha=0.09)\n",
    "ax1.set_xticks([1, 4], ['Contour absent', 'Contour present'])\n",
    "ax1.set_xlim(0,5)\n",
    "ax1.set_ylabel('saliency_score_aprime')\n",
    "ax1.set_title('Contour Sensitivity using APrime')\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Second subplot: Top-right corner\n",
    "sns.distplot(all_saliency_score_aprime[contour_present_pos], ax=ax2, label='contour_present',color='green')\n",
    "sns.distplot(all_saliency_score_aprime[contour_absent_pos], ax=ax2, label='contour_absent',color='gray')\n",
    "ax2.set_title('Contour Sensitivity using APrime (Histogram)')\n",
    "ax2.legend()\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Third subplot: Bottom-right corner\n",
    "sns.distplot(all_saliency_score_aprime[contour_present_pos] - all_saliency_score_aprime[contour_absent_pos], ax=ax3, label='difference',color='k')\n",
    "ax3.set_title('Alignment Sensitivity using APrime')\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Perform the paired t-test\n",
    "t_statistic, p_value = stats.ttest_rel(all_saliency_score_aprime[contour_present_pos], all_saliency_score_aprime[contour_absent_pos])\n",
    "print(f\"t-statistic: {t_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"color:red\"> Figure : SFig. 2B and SFig. 2C   </h1>\n",
    "\n",
    "## Description: Location and Alignment Sensitivity for all layers of finetuned alexnet over broad curvatures\n",
    "This analysis takes roughly **8 hours** to complete on 600 validation images when run on all contour readout models (with object recognition alexnet finetuned backbones and trained on braod contour curvatures) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the dataset and set of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=get_B,get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "print(len(val_dataset_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_paths=sorted(glob.glob('../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/*.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,file in enumerate(model_file_paths):\n",
    "    print(i,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Saliency for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency(prep_img,prep_recorder,file):\n",
    "    prep_img=Variable(prep_img,requires_grad=True).to(device)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Loading the model and setting up the Guided Spliced Model for Attention Maps   \n",
    "    ## Model\n",
    "    checkpoint=torch.load(selected_file)\n",
    "    \n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    GBP = Spliced_GuidedBackprop(loaded_spliced_model,device)\n",
    "    ####################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ####################################################################################################\n",
    "    ## Getting the Saliency Maps\n",
    "    # Get gradients\n",
    "    guided_grads = GBP.generate_gradients(prep_img, 1)\n",
    "    # Gradients\n",
    "    guided_grads_vis = guided_grads - guided_grads.min()\n",
    "    guided_grads_vis /= guided_grads_vis.max()\n",
    "\n",
    "    # Grayscale Gradients\n",
    "    grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
    "    grayscale_guided_grads_vis = grayscale_guided_grads - grayscale_guided_grads.min()\n",
    "    grayscale_guided_grads_vis /= grayscale_guided_grads_vis.max()\n",
    "    \n",
    "    \n",
    "    viz_image=grayscale_guided_grads_vis[0]\n",
    "    ####################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Compute saliency scores\n",
    "    mask_img_path_fg, mask_img_path_bg, list_gauss=mask_renderer(torch.load(prep_recorder),140)\n",
    "    mask_img_path_fg=np.array(mask_img_path_fg)\n",
    "    mask_img_path_bg=np.array(mask_img_path_bg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    saliency_score_diffmean=compute_diff_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    saliency_score_dprime=compute_dprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    saliency_score_aprime=compute_aprime_means(viz_image[np.where(mask_img_path_fg==255)],viz_image[np.where(mask_img_path_bg==255)])\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    ## Making the run more memory efficient\n",
    "    loaded_spliced_model=loaded_spliced_model.to('cpu')\n",
    "    del GBP\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    return saliency_score_diffmean,saliency_score_dprime,saliency_score_aprime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the loop - All models, All images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saliency_score_diff={}\n",
    "model_saliency_score_dprime={}\n",
    "model_saliency_score_aprime={}\n",
    "\n",
    "\n",
    "for file in tqdm(model_file_paths):\n",
    "    print(file)\n",
    "    checkpoint=torch.load(file)\n",
    "    \n",
    "    model_saliency_score_diff[checkpoint['training_config']['layer_name']]=[]\n",
    "    model_saliency_score_dprime[checkpoint['training_config']['layer_name']]=[]\n",
    "    model_saliency_score_aprime[checkpoint['training_config']['layer_name']]=[]\n",
    "    \n",
    "    for (a, b, d, alpha, nel, labels, record) in tqdm(val_loader_norm,disable=True):\n",
    "        \n",
    "        \n",
    "        for img_num in tqdm(range(a.shape[0]),disable=True):\n",
    "            prep_img=torch.unsqueeze(a[img_num],0)\n",
    "            prep_recorder=record[img_num]\n",
    "            \n",
    "            saliency_score_diffmean,saliency_score_dprime,saliency_score_aprime=compute_saliency(prep_img,prep_recorder,file)\n",
    "\n",
    "            model_saliency_score_diff[checkpoint['training_config']['layer_name']].append(saliency_score_diffmean)\n",
    "            model_saliency_score_dprime[checkpoint['training_config']['layer_name']].append(saliency_score_dprime)\n",
    "            model_saliency_score_aprime[checkpoint['training_config']['layer_name']].append(saliency_score_aprime)\n",
    "            \n",
    "        \n",
    "        \n",
    "    model_saliency_score_diff[checkpoint['training_config']['layer_name']]=np.array(model_saliency_score_diff[checkpoint['training_config']['layer_name']])\n",
    "    model_saliency_score_dprime[checkpoint['training_config']['layer_name']]=np.array(model_saliency_score_dprime[checkpoint['training_config']['layer_name']])\n",
    "    model_saliency_score_aprime[checkpoint['training_config']['layer_name']]=np.array(model_saliency_score_aprime[checkpoint['training_config']['layer_name']])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_betas=[]\n",
    "all_labels=[]\n",
    "\n",
    "for (a, b, d, alpha, nel, labels, record) in tqdm(val_loader_norm,disable=True):\n",
    "    all_betas.append(b)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "    \n",
    "all_betas=torch.cat(all_betas).numpy()\n",
    "all_labels=torch.cat(all_labels).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_saliency_score_diff.keys())\n",
    "print(model_saliency_score_dprime.keys())\n",
    "print(model_saliency_score_aprime.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_present_pos=np.where(all_labels==1)[0]\n",
    "contour_absent_pos=np.where(all_labels==0)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting for all readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foreground_sensitivity_allmodels(sal_dict,list_layers,pos):\n",
    "    mean_sal=[]\n",
    "    std_sal=[]\n",
    "    confint_sal=[]\n",
    "    confint_lower=[]\n",
    "    confint_upper=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(list_layers)):\n",
    "        layer=list_layers[i]\n",
    "        \n",
    "        \n",
    "        if layer in sal_dict.keys():\n",
    "            \n",
    "            \n",
    "            mean_sal.append(np.mean(sal_dict[layer][pos]))\n",
    "            std_sal.append(np.std(sal_dict[layer][pos],ddof=1))\n",
    "            \n",
    "            \n",
    "            \n",
    "            confint_sal.append(1.96*(np.std(sal_dict[layer][pos],ddof=1) / np.sqrt(len(sal_dict[layer][pos]))))\n",
    "            confint_lower.append(np.percentile(sal_dict[layer][pos], 2.5))\n",
    "            confint_upper.append(np.percentile(sal_dict[layer][pos], 97.5))\n",
    "            \n",
    "    return mean_sal,std_sal,confint_sal,confint_lower,confint_upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alignment_sensitivity_allmodels(sal_dict,list_layers,present_pos,absent_pos):\n",
    "    \n",
    "    mean_sal=[]\n",
    "    std_sal=[]\n",
    "    confint_sal=[]\n",
    "    confint_lower=[]\n",
    "    confint_upper=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(list_layers)):\n",
    "        layer=list_layers[i]\n",
    "        \n",
    "        \n",
    "        if layer in sal_dict.keys():\n",
    "            \n",
    "            \n",
    "            diff_values=sal_dict[layer][present_pos] - sal_dict[layer][absent_pos]\n",
    "            \n",
    "            mean_sal.append(np.mean(diff_values))\n",
    "            std_sal.append(np.std(diff_values))\n",
    "            \n",
    "            \n",
    "            \n",
    "            confint_sal.append(1.96*(np.std(diff_values) / np.sqrt(len(diff_values))))\n",
    "            confint_lower.append(np.percentile(diff_values, 2.5))\n",
    "            confint_upper.append(np.percentile(diff_values, 97.5))\n",
    "            \n",
    "    return mean_sal,std_sal,confint_sal,confint_lower,confint_upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_layers=['features_0',\n",
    " 'features_1',\n",
    " 'features_2',\n",
    " 'features_3',\n",
    " 'features_4',\n",
    " 'features_5',\n",
    " 'features_6',\n",
    " 'features_7',\n",
    " 'features_8',\n",
    " 'features_9',\n",
    " 'features_10',\n",
    " 'features_11',\n",
    " 'features_12',\n",
    " 'avgpool',\n",
    " 'classifier_0',\n",
    " 'classifier_1',\n",
    " 'classifier_2',\n",
    " 'classifier_3',\n",
    " 'classifier_4',\n",
    " 'classifier_5',\n",
    " 'classifier_6']\n",
    "\n",
    "for i in range(len(list_layers)):\n",
    "    list_layers[i]=list_layers[i].replace('_','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_present_pos=np.where(all_labels==1)[0]\n",
    "contour_absent_pos=np.where(all_labels==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sal,std_sal,confint_sal,confint_lower,confint_upper=get_foreground_sensitivity_allmodels(model_saliency_score_diff,list_layers,contour_present_pos)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(22,10))\n",
    "color_or_finetune=(78/255,121/255,180/255)\n",
    "spacing_x=2\n",
    "error_bar_width=3\n",
    "error_bar_cap=4\n",
    "\n",
    "plt.errorbar(np.arange(len(mean_sal)) * spacing_x, mean_sal, yerr=confint_sal, \n",
    "             capsize=error_bar_cap,elinewidth=error_bar_width,fmt='o',marker='o', color=color_or_finetune, markersize=10, \n",
    "             label='or finetune', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(mean_sal)) * spacing_x,list_layers,rotation=90,fontsize=15)\n",
    "plt.yticks(fontsize=15) \n",
    "\n",
    "\n",
    "plt.ylabel('Diff of means',fontsize=15)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title('Location Sensitivity in Contour Images using Diff. of Means measure',fontsize=15)\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sal,std_sal,confint_sal,confint_lower,confint_upper=get_foreground_sensitivity_allmodels(model_saliency_score_aprime,list_layers,contour_present_pos)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(22,10))\n",
    "color_or_finetune=(78/255,121/255,180/255)\n",
    "spacing_x=2\n",
    "error_bar_width=3\n",
    "error_bar_cap=4\n",
    "\n",
    "plt.errorbar(np.arange(len(mean_sal)) * spacing_x, mean_sal, yerr=confint_sal, \n",
    "             capsize=error_bar_cap,elinewidth=error_bar_width,fmt='o',marker='o', color=color_or_finetune, markersize=10, \n",
    "             label='or finetune', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(mean_sal)) * spacing_x,list_layers,rotation=90,fontsize=15)\n",
    "plt.yticks(fontsize=15) \n",
    "\n",
    "\n",
    "plt.ylabel('Aprime',fontsize=15)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title('Location Sensitivity in Contour Images using APrime measure',fontsize=15)\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sal,std_sal,confint_sal,confint_lower,confint_upper=get_alignment_sensitivity_allmodels(model_saliency_score_aprime,list_layers,contour_present_pos,contour_absent_pos)\n",
    "\n",
    "plt.figure(figsize=(22,10))\n",
    "color_or_finetune=(78/255,121/255,180/255)\n",
    "spacing_x=2\n",
    "error_bar_width=3\n",
    "error_bar_cap=4\n",
    "\n",
    "plt.errorbar(np.arange(len(mean_sal)) * spacing_x, mean_sal, yerr=confint_sal, \n",
    "             capsize=error_bar_cap,elinewidth=error_bar_width,fmt='o',marker='o', color=color_or_finetune, markersize=10, \n",
    "             label='or finetune', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(mean_sal)) * spacing_x,list_layers,rotation=90,fontsize=15)\n",
    "plt.yticks(fontsize=15) \n",
    "\n",
    "\n",
    "plt.ylabel('Difference b/w misaligned and aligned pairs',fontsize=15)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title('Alignment Sensitivity in Contour Images using APrime measure',fontsize=15)\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"color:red\"> Figure : Fig. 3 and SFig. 3   </h1>\n",
    "\n",
    "## Description: Performance of PinholeNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_norm = Contour_Dataset(root=root_directory,transform=data_transform,train=False,get_B=get_B,get_D=get_D,get_A=get_A,get_numElements=get_numElements)\n",
    "val_loader_norm = torch.utils.data.DataLoader(dataset=val_dataset_norm, batch_size=1, num_workers=num_workers, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinholenet_get_prediction(model_file_paths):\n",
    "    model_valacc_scores={}\n",
    "    for file in tqdm(model_file_paths):\n",
    "\n",
    "        # Model\n",
    "        checkpoint=torch.load(file)\n",
    "        # loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device='cpu')\n",
    "        loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "        loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "        \n",
    "        change_train_eval_mode(loaded_spliced_model,loaded_spliced_model.fine_tune,train_eval_mode='eval')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        model_valacc_scores[file.split('/')[-1]]=np.array([])\n",
    "        for (inputs, b, d, alpha, nel, labels, record) in tqdm(val_loader_norm,disable=True):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs= loaded_spliced_model.forward(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            model_valacc_scores[file.split('/')[-1]]=np.concatenate((model_valacc_scores[file.split('/')[-1]],(preds == labels).float().cpu().numpy()))\n",
    "    \n",
    "    \n",
    "    return model_valacc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinholenet_get_list_acc_mean_std_ci(sal_dict):\n",
    "    mean_sal=[]\n",
    "    std_sal=[]\n",
    "    confint_sal=[]\n",
    "    \n",
    "    \n",
    "    confint_lower=[]\n",
    "    confint_upper=[]\n",
    "    \n",
    "    list_keys=[]\n",
    "    \n",
    "    for key in sal_dict.keys():\n",
    "        \n",
    "        \n",
    "        mean_sal.append(np.mean(sal_dict[key]))\n",
    "\n",
    "        std_sal.append(np.std(sal_dict[key],ddof=1))\n",
    "\n",
    "        confint_sal.append(1.96*(np.std(sal_dict[key],ddof=1) / np.sqrt(len(sal_dict[key]))))\n",
    "\n",
    "        confint_lower.append(np.percentile(sal_dict[key], 2.5))\n",
    "        confint_upper.append(np.percentile(sal_dict[key], 97.5))\n",
    "        \n",
    "        list_keys.append(key)\n",
    "            \n",
    "    return mean_sal,std_sal,confint_sal,confint_lower,confint_upper, list_keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting all the accuracy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagnet_conv5_finetune_files = ['../model_weights/contour_model_weights/alexnet-bagnet11_regimagenet_categ_finetune_broad/model_alexnet-bagnet11-regim-categ_layer_features-9_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet17_regimagenet_categ_finetune_broad/model_alexnet-bagnet17-regim-categ_layer_features-9_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet31_regimagenet_categ_finetune_broad/model_alexnet-bagnet31-regim-categ_layer_features-9_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet33_regimagenet_categ_finetune_broad/model_alexnet-bagnet33-regim-categ_layer_features-9_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-12_mode_finetune.pt']\n",
    "bagnet_conv5_finetune_predictions=pinholenet_get_prediction(bagnet_conv5_finetune_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagnet_ap_finetune_files = ['../model_weights/contour_model_weights/alexnet-bagnet11_regimagenet_categ_finetune_broad/model_alexnet-bagnet11-regim-categ_layer_avgpool_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet17_regimagenet_categ_finetune_broad/model_alexnet-bagnet17-regim-categ_layer_avgpool_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet31_regimagenet_categ_finetune_broad/model_alexnet-bagnet31-regim-categ_layer_avgpool_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet33_regimagenet_categ_finetune_broad/model_alexnet-bagnet33-regim-categ_layer_avgpool_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune.pt']\n",
    "bagnet_ap_finetune_predictions=pinholenet_get_prediction(bagnet_ap_finetune_files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bagnet_fc1_finetune_files = ['../model_weights/contour_model_weights/alexnet-bagnet11_regimagenet_categ_finetune_broad/model_alexnet-bagnet11-regim-categ_layer_classifier-2_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet17_regimagenet_categ_finetune_broad/model_alexnet-bagnet17-regim-categ_layer_classifier-2_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet31_regimagenet_categ_finetune_broad/model_alexnet-bagnet31-regim-categ_layer_classifier-2_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet33_regimagenet_categ_finetune_broad/model_alexnet-bagnet33-regim-categ_layer_classifier-2_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_classifier-2_mode_finetune.pt']\n",
    "bagnet_fc1_finetune_predictions=pinholenet_get_prediction(bagnet_fc1_finetune_files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bagnet_fc2_finetune_files = ['../model_weights/contour_model_weights/alexnet-bagnet11_regimagenet_categ_finetune_broad/model_alexnet-bagnet11-regim-categ_layer_classifier-5_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet17_regimagenet_categ_finetune_broad/model_alexnet-bagnet17-regim-categ_layer_classifier-5_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet31_regimagenet_categ_finetune_broad/model_alexnet-bagnet31-regim-categ_layer_classifier-5_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet-bagnet33_regimagenet_categ_finetune_broad/model_alexnet-bagnet33-regim-categ_layer_classifier-5_mode_finetune.pt',\n",
    "                        '../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_classifier-5_mode_finetune.pt']\n",
    "bagnet_fc2_finetune_predictions=pinholenet_get_prediction(bagnet_fc2_finetune_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagnet_conv5_mean_sal, _, bagnet_conv5_confint_sal, _, _, _ = pinholenet_get_list_acc_mean_std_ci(bagnet_conv5_finetune_predictions)\n",
    "\n",
    "\n",
    "bagnet_ap_mean_sal, _, bagnet_ap_confint_sal, _, _, _ = pinholenet_get_list_acc_mean_std_ci(bagnet_ap_finetune_predictions)\n",
    "\n",
    "\n",
    "bagnet_fc1_mean_sal, _, bagnet_fc1_confint_sal, _, _, _ = pinholenet_get_list_acc_mean_std_ci(bagnet_fc1_finetune_predictions)\n",
    "\n",
    "\n",
    "bagnet_fc2_mean_sal, _, bagnet_fc2_confint_sal, _, _, _ = pinholenet_get_list_acc_mean_std_ci(bagnet_fc2_finetune_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_shades_barplot=[(0.8229757785467128, 0.8898269896193771, 0.9527566320645905),\n",
    "                      (0.6965013456362937, 0.8248366013071895, 0.9092656670511342),\n",
    "                      (0.5168627450980392, 0.7357477893118032, 0.8601922337562476),\n",
    "                      (0.3363783160322953, 0.6255132641291811, 0.8067358708189158),\n",
    "                      (0.8275, 0.8275, 0.8275)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig. 3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'model_name': ['P 11*11', 'P 17*17', 'P 31*31', 'P 33*33','Standard Alexnet',\n",
    "                  'P 11*11', 'P 17*17', 'P 31*31', 'P 33*33','Standard Alexnet'],\n",
    "    \n",
    "    'layer_name': ['C5', 'C5', 'C5', 'C5', 'C5',\n",
    "                   'FC2','FC2','FC2','FC2','FC2'],\n",
    "    \n",
    "    'accuracy': [*bagnet_conv5_mean_sal,\n",
    "                *bagnet_fc2_mean_sal],\n",
    "    \n",
    "    'error': [*bagnet_conv5_confint_sal,\n",
    "                *bagnet_fc2_confint_sal],\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sample colors with RGB values\n",
    "color_dict = {\n",
    "    'P 11*11': (0.8229757785467128, 0.8898269896193771, 0.9527566320645905),\n",
    "    'P 17*17': (0.6965013456362937, 0.8248366013071895, 0.9092656670511342),\n",
    "    'P 31*31': (0.5168627450980392, 0.7357477893118032, 0.8601922337562476),\n",
    "    'P 33*33': (0.3363783160322953, 0.6255132641291811, 0.8067358708189158),\n",
    "    'Standard Alexnet':(0.827, 0.827, 0.827)\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "barplot = sns.barplot(data=df, x='layer_name', y='accuracy', hue='model_name', palette=color_dict, ci=None)\n",
    "\n",
    "# Adjust the ylim to make sure error bars are visible\n",
    "plt.ylim(0.4, 1.0)\n",
    "\n",
    "# Since we have grouped data, we need to adjust the way we access the error values.\n",
    "# We'll use groupby and get_group to get the correct subsets.\n",
    "for name, group in df.groupby(['layer_name', 'model_name']):\n",
    "    layer_name, model_name = name\n",
    "    group_data = df[(df['layer_name'] == layer_name) & (df['model_name'] == model_name)]\n",
    "    \n",
    "    # Get the positions of the bars for the current group\n",
    "    positions = [p.get_x() + p.get_width() / 2 for p in barplot.patches if p.get_height() == group_data['accuracy'].values[0]]\n",
    "    \n",
    "    # Plot the error bars for the current group\n",
    "    plt.errorbar(positions, group_data['accuracy'], yerr=group_data['error'], fmt='none', c='k', capsize=5)\n",
    "\n",
    "plt.title('Model Accuracy by Layer')\n",
    "plt.xlabel('Layer Name')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Remove the legend if not needed\n",
    "plt.legend(title='Model Name').remove()\n",
    "\n",
    "# Spine visibility settings as before\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "# plt.savefig('./dev/manuscript_figures/f3_bagnet_comparison.png', format='png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFig. 3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnetbase=torch.load('../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_features-12_mode_finetune.pt')['metrics']['val_acc']\n",
    "\n",
    "alexnetepoch50=torch.load('../model_weights/contour_model_weights/alexnet-epoch50_regimagenet_categ_finetune_broad/model_alexnet-epoch50-regim-categ_layer_features-12_mode_finetune.pt')['metrics']['val_acc']\n",
    "alexnetepoch100=torch.load('../model_weights/contour_model_weights/alexnet-epoch100_regimagenet_categ_finetune_broad/model_alexnet-epoch100-regim-categ_layer_features-12_mode_finetune.pt')['metrics']['val_acc']\n",
    "\n",
    "alexnet33=torch.load('../model_weights/contour_model_weights/alexnet-bagnet33_regimagenet_categ_finetune_broad/model_alexnet-bagnet33-regim-categ_layer_features-9_mode_finetune.pt')['metrics']['val_acc']\n",
    "alexnet31=torch.load('../model_weights/contour_model_weights/alexnet-bagnet31_regimagenet_categ_finetune_broad/model_alexnet-bagnet31-regim-categ_layer_features-9_mode_finetune.pt')['metrics']['val_acc']\n",
    "\n",
    "alexnet17=torch.load('../model_weights/contour_model_weights/alexnet-bagnet17_regimagenet_categ_finetune_broad/model_alexnet-bagnet17-regim-categ_layer_features-9_mode_finetune.pt')['metrics']['val_acc']\n",
    "alexnet11=torch.load('../model_weights/contour_model_weights/alexnet-bagnet11_regimagenet_categ_finetune_broad/model_alexnet-bagnet11-regim-categ_layer_features-9_mode_finetune.pt')['metrics']['val_acc']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_colors = ['#1f78b4', '#33a02c', '#e31a1c', '#ff7f00', '#6a3d9a']\n",
    "\n",
    "color_shades_barplot=[(0.8229757785467128, 0.8898269896193771, 0.9527566320645905),\n",
    "                      (0.6965013456362937, 0.8248366013071895, 0.9092656670511342),\n",
    "                      (0.5168627450980392, 0.7357477893118032, 0.8601922337562476),\n",
    "                      (0.3363783160322953, 0.6255132641291811, 0.8067358708189158),\n",
    "                      (0.8275, 0.8275, 0.8275)]\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "\n",
    "\n",
    "l1,=plt.plot(np.arange(len(alexnetbase)),alexnetbase,linewidth=3.0,color='lightgray',label='alexnet torchvision baseline (90 epochs)')\n",
    "# plt.plot(np.arange(len(alexnetepoch100)),alexnetepoch100,linewidth=1.0,color='k',label='alexnet-epoch100')\n",
    "plt.plot(np.arange(len(alexnetepoch50)),alexnetepoch50,linestyle='--',linewidth=3.0,color='k',label='alexnet-epoch50')\n",
    "\n",
    "\n",
    "plt.plot(np.arange(len(alexnet11)),alexnet11,linewidth=3.0,color=(0.8229757785467128, 0.8898269896193771, 0.9527566320645905),label='pinholenet-11')\n",
    "plt.plot(np.arange(len(alexnet17)),alexnet17,linewidth=3.0,color=(0.6965013456362937, 0.8248366013071895, 0.9092656670511342),label='pinholenet-17')\n",
    "\n",
    "plt.plot(np.arange(len(alexnet31)),alexnet31,linewidth=3.0,color=(0.5168627450980392, 0.7357477893118032, 0.8601922337562476),label='pinholenet-31')\n",
    "plt.plot(np.arange(len(alexnet33)),alexnet33,linewidth=3.0,color=(0.3363783160322953, 0.6255132641291811, 0.8067358708189158),label='pinholenet-33')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plt.axhline(y=0.5,linestyle='--',color='r',label='chance',alpha=0.1)\n",
    "\n",
    "plt.xlabel('Epochs',fontsize=15,labelpad=15)\n",
    "plt.ylabel('Contour Readout Accuracy',fontsize=15,labelpad=15)\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)   \n",
    "plt.ylim(0.4,1.01)\n",
    "\n",
    "    \n",
    "# plt.legend(fontsize=12,frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFig. 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_base_rf=[19,67,99,131,195,323,512,512]\n",
    "pinhole_11_rf=[7,9,11,11,11,101,512,512]\n",
    "pinhole_17_rf=[9,13,17,17,17,101,512,512]\n",
    "pinhole_31_rf=[11,19,23,27,31,111,512,512]\n",
    "pinhole_33_rf=[13,25,29,33,33,111,512,512]\n",
    "\n",
    "\n",
    "x_lab=['C1','C2','C3','C4','C5','AP','F1','F2']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(np.arange(len(alexnet_base_rf)),alexnet_base_rf,linewidth=3.0,color='lightgray',label='standarad alexnet')\n",
    "\n",
    "plt.plot(np.arange(len(pinhole_11_rf)),pinhole_11_rf,linewidth=3.0,color=(0.8229757785467128, 0.8898269896193771, 0.9527566320645905),label='pinholenet-11')\n",
    "plt.plot(np.arange(len(pinhole_17_rf)),pinhole_17_rf,linewidth=3.0,color=(0.6965013456362937, 0.8248366013071895, 0.9092656670511342),label='pinholenet-17')\n",
    "plt.plot(np.arange(len(pinhole_31_rf)),pinhole_31_rf,linewidth=3.0,color=(0.5168627450980392, 0.7357477893118032, 0.8601922337562476),label='pinholenet-31')\n",
    "plt.plot(np.arange(len(pinhole_33_rf)),pinhole_33_rf,linewidth=3.0,color=(0.3363783160322953, 0.6255132641291811, 0.8067358708189158),label='pinholenet-33')\n",
    "\n",
    "\n",
    "for i in range(len(np.arange(len(alexnet_base_rf)))):\n",
    "    plt.axvline(np.arange(len(alexnet_base_rf))[i],linestyle='--',alpha=0.09,color='k')\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(alexnet_base_rf)),x_lab,fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "plt.xlabel('Layer',fontsize=15,labelpad=15)\n",
    "plt.ylabel('Receptive Field Size',fontsize=15,labelpad=15)\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "plt.legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFig. 3E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'model_name': ['P 11*11', 'P 17*17', 'P 31*31', 'P 33*33','Standard Alexnet',\n",
    "                  'P 11*11', 'P 17*17', 'P 31*31', 'P 33*33','Standard Alexnet',\n",
    "                  'P 11*11', 'P 17*17', 'P 31*31', 'P 33*33','Standard Alexnet'],\n",
    "    \n",
    "    'layer_name': ['AP', 'AP', 'AP', 'AP', 'AP',\n",
    "                   'FC1','FC1','FC1','FC1','FC1',\n",
    "                   'FC2','FC2','FC2','FC2','FC2'],\n",
    "    \n",
    "    'accuracy': [*bagnet_ap_mean_sal,\n",
    "                *bagnet_fc1_mean_sal,\n",
    "                *bagnet_fc2_mean_sal]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sample colors with RGB values\n",
    "color_dict = {\n",
    "    'P 11*11': (0.8229757785467128, 0.8898269896193771, 0.9527566320645905),\n",
    "    'P 17*17': (0.6965013456362937, 0.8248366013071895, 0.9092656670511342),\n",
    "    'P 31*31': (0.5168627450980392, 0.7357477893118032, 0.8601922337562476),\n",
    "    'P 33*33': (0.3363783160322953, 0.6255132641291811, 0.8067358708189158),\n",
    "    'Standard Alexnet':(0.827, 0.827, 0.827)\n",
    "}\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.barplot(data=df, x='layer_name', y='accuracy', hue='model_name', palette=color_dict, ci=None)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Model Accuracy by Layer')\n",
    "plt.xlabel('Layer Name')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(title='Model Name').remove()\n",
    "\n",
    "\n",
    "plt.ylim(0.5, 1.0) \n",
    "\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "# plt.savefig('./dev/manuscript_figures/pinholenet_contour_training_classifier_head.png', bbox_inches='tight', format='png', dpi=600)\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"color:red\"> Figure : Fig. 4   </h1>\n",
    "\n",
    "## Description: Human Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_variables(filename):\n",
    "    \"\"\"\n",
    "    Load multiple Python variables from a file.\n",
    "    \n",
    "    Parameters:\n",
    "    - filename (str): the name of the file to load from.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing the loaded variables.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent_correct(all_participants_filename,all_participants_correct,all_participants_beta):\n",
    "    unique_images=np.unique(all_participants_filename)\n",
    "\n",
    "    accuracy_image=[]\n",
    "    beta_image=[]\n",
    "\n",
    "    for img in unique_images:\n",
    "        accuracy_image.append(np.mean(all_participants_correct[np.where(all_participants_filename==img)]))\n",
    "        beta_image.append(np.mean(all_participants_beta[np.where(all_participants_filename==img)]))\n",
    "        \n",
    "    return np.array(accuracy_image),np.array(beta_image),unique_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(data, num_samples,num_simulations=300):\n",
    "    \"\"\"Creates a bootstrap sample from data with replacement.\n",
    "\n",
    "    Args:\n",
    "    data (np.array): The numpy array to sample from.\n",
    "    num_samples (int): The number of samples to draw.\n",
    "\n",
    "    Returns:\n",
    "    np.array: The bootstrap sample as a numpy array.\n",
    "    \"\"\"\n",
    "    bootstrapped_data=[]\n",
    "    for sim in range(num_simulations):\n",
    "        bootstrapped_data.append(np.random.choice(data, size=num_samples))\n",
    "    bootstrapped_data=np.array(bootstrapped_data)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return bootstrapped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_variables('../contour_integ_behavior/contour_exp1/analysis_data/analysis_data.pkl')\n",
    "\n",
    "all_participants_filename=data['all_participants_filename']\n",
    "all_participants_correct=data['all_participants_correct']\n",
    "all_participants_beta=data['all_participants_beta']\n",
    "\n",
    "print(all_participants_beta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Contour Signal at level of individual trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_img_signal,beta_image,unique_images=get_percent_correct(all_participants_filename,all_participants_correct,all_participants_beta)\n",
    "unique_beta_vals = np.unique(beta_image)\n",
    "\n",
    "print(human_img_signal.shape)\n",
    "print(unique_beta_vals)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.bar(range(len(human_img_signal)), human_img_signal,color='gray')\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.title(' Human Contour Signal at level of individual trials (Human Percent Correct)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Beta-wise Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_participants_unique_beta={}\n",
    "human_unique_beta_acc=[]\n",
    "for b in np.unique(all_participants_beta):\n",
    "    all_participants_unique_beta[b]={}\n",
    "    all_participants_unique_beta[b]['mean']=np.mean(all_participants_correct[np.where(all_participants_beta==b)])\n",
    "    all_participants_unique_beta[b]['std']=np.std(all_participants_correct[np.where(all_participants_beta==b)])\n",
    "    human_unique_beta_acc.append([np.mean(all_participants_correct[np.where(all_participants_beta==b)]),np.std(all_participants_correct[np.where(all_participants_beta==b)])])\n",
    "human_unique_beta_acc=np.array(human_unique_beta_acc)\n",
    "\n",
    "print(all_participants_unique_beta)\n",
    "print(human_unique_beta_acc)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(unique_beta_vals,human_unique_beta_acc[:,0],color='gray',linewidth=4,label='human')\n",
    "plt.plot(unique_beta_vals,human_unique_beta_acc[:,0],'.',color='k',markersize=15)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.xticks(unique_beta_vals,fontsize=15)\n",
    "plt.yticks(fontsize=15) \n",
    "plt.ylabel('Average Accuracy',fontsize=15)\n",
    "plt.ylim(0.4,1.0)\n",
    "plt.title('Mean Accuracy for each beta condition')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding error bars (bootstrapped 95% CI) over each beta condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_image=[]\n",
    "beta_image=[]\n",
    "bootstrapped_accuracy_image=[]\n",
    "\n",
    "\n",
    "\n",
    "for img in tqdm(unique_images):\n",
    "    accuracy_image.append(np.mean(all_participants_correct[np.where(all_participants_filename==img)]))\n",
    "    beta_image.append(np.mean(all_participants_beta[np.where(all_participants_filename==img)]))\n",
    "    \n",
    "    t=np.mean(bootstrap_sample(all_participants_correct[np.where(all_participants_filename==img)],len(all_participants_correct[np.where(all_participants_filename==img)])),1)\n",
    "    bootstrapped_accuracy_image.append(t)\n",
    "\n",
    "    \n",
    "accuracy_image=np.array(accuracy_image)\n",
    "beta_image=np.array(beta_image)\n",
    "bootstrapped_accuracy_image=np.array(bootstrapped_accuracy_image)\n",
    "\n",
    "print(accuracy_image.shape)\n",
    "print(beta_image.shape)\n",
    "print(bootstrapped_accuracy_image.shape)\n",
    "\n",
    "condition_accuracy_image=np.empty((0,200))\n",
    "condition_bootstrapped_accuracy_image=np.empty((0,200,300))\n",
    "condition_beta=[]\n",
    "\n",
    "for b in np.unique(beta_image):\n",
    "    condition_beta.append(b)\n",
    "    condition_bootstrapped_accuracy_image=np.concatenate((condition_bootstrapped_accuracy_image,np.expand_dims(bootstrapped_accuracy_image[np.where(beta_image==b)[0],:],0)))\n",
    "    condition_accuracy_image=np.concatenate((condition_accuracy_image,np.expand_dims(accuracy_image[np.where(beta_image==b)[0]],0)))\n",
    "\n",
    "print(condition_accuracy_image.shape)\n",
    "print(condition_bootstrapped_accuracy_image.shape)\n",
    "\n",
    "condition_bootstrapped_accuracy_image=np.mean(condition_bootstrapped_accuracy_image,1)\n",
    "condition_accuracy_image=np.mean(condition_accuracy_image,1)\n",
    "\n",
    "print(condition_accuracy_image.shape)\n",
    "print(condition_bootstrapped_accuracy_image.shape)\n",
    "print(condition_beta)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Human Results\n",
    "plt.figure(figsize=(10,12))\n",
    "plt.errorbar(np.arange(len(condition_accuracy_image)), condition_accuracy_image, \n",
    "             yerr=[condition_accuracy_image - np.percentile(condition_bootstrapped_accuracy_image, 2.5, axis=1),np.percentile(condition_bootstrapped_accuracy_image, 97.5, axis=1) - condition_accuracy_image], \n",
    "             capsize=5,elinewidth=4,linestyle='-', marker='.', color='gray', markersize=10, \n",
    "             label='supervised regular finetune', alpha=0.8, linewidth=4)\n",
    "\n",
    "for i in range(len(condition_accuracy_image)):\n",
    "    plt.axvline(i,linestyle='--',alpha=0.1,color='k')\n",
    "\n",
    "plt.xticks(np.arange(len(condition_accuracy_image)),condition_beta,fontsize=20)\n",
    "plt.yticks(fontsize=20) \n",
    "\n",
    "plt.ylim(0.4,1.0)\n",
    "\n",
    "plt.xlabel('Beta Condition',fontsize=20)\n",
    "plt.ylabel('Percent Correct',fontsize=20)\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing percent correct for each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psychophysics_folder = psychophysics_visual_diet_config['root_directory']\n",
    "recorder_image=[]\n",
    "for img in unique_images:\n",
    "    recorder_image.append(torch.load(os.path.join(psychophysics_folder,img)))\n",
    "recorder_image=np.array(recorder_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_accuracy_image=[]\n",
    "sorted_beta_image=[]\n",
    "sorted_recorder_image=[]\n",
    "\n",
    "for b in np.unique(all_participants_beta[0,:],return_counts=True)[0]:\n",
    "    s_accuracy_image=accuracy_image[np.where(beta_image==b)]\n",
    "    s_beta_image=beta_image[np.where(beta_image==b)]\n",
    "    s_recorder_image=recorder_image[np.where(beta_image==b)]\n",
    "    \n",
    "    \n",
    "    s_indices=np.argsort(s_accuracy_image)\n",
    "    \n",
    "    \n",
    "    sorted_accuracy_image.append(s_accuracy_image[s_indices])\n",
    "    sorted_beta_image.append(s_beta_image[s_indices])\n",
    "    sorted_recorder_image.append(s_recorder_image[s_indices])\n",
    "    \n",
    "    \n",
    "sorted_accuracy_image=np.concatenate(sorted_accuracy_image)\n",
    "sorted_beta_image=np.concatenate(sorted_beta_image)\n",
    "sorted_recorder_image=np.concatenate(sorted_recorder_image)\n",
    "\n",
    "plt.figure(figsize=(24,8))  # Set the figure size\n",
    "deep_palette = sns.color_palette(\"deep\", 5)\n",
    "color_dict = {15: deep_palette[0], \n",
    "             30: deep_palette[1], \n",
    "             45: deep_palette[2], \n",
    "             60: deep_palette[3], \n",
    "             75: deep_palette[4]}\n",
    "\n",
    "\n",
    "colors = [color_dict[label] for label in sorted_beta_image]\n",
    "\n",
    "plt.bar(range(len(sorted_accuracy_image)), sorted_accuracy_image,color=colors)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"color:red\"> Figure : Fig. 5, SFig. 6, SFig. 7, SFig. 8, SFig. 9   </h1>\n",
    "\n",
    "## Description: Human Behavior, Model-Human Alignment, Model-Human Performance as fn of Curvature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Human Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_variables(filename):\n",
    "    \"\"\"\n",
    "    Load multiple Python variables from a file.\n",
    "    \n",
    "    Parameters:\n",
    "    - filename (str): the name of the file to load from.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing the loaded variables.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_percent_correct(all_participants_filename,all_participants_correct,all_participants_beta):\n",
    "    unique_images=np.unique(all_participants_filename)\n",
    "\n",
    "    accuracy_image=[]\n",
    "    beta_image=[]\n",
    "\n",
    "    for img in unique_images:\n",
    "        accuracy_image.append(np.mean(all_participants_correct[np.where(all_participants_filename==img)]))\n",
    "        beta_image.append(np.mean(all_participants_beta[np.where(all_participants_filename==img)]))\n",
    "        \n",
    "    return np.array(accuracy_image),np.array(beta_image),unique_images\n",
    "\n",
    "def bootstrap_sample(data, num_samples,num_simulations=300):\n",
    "    \"\"\"Creates a bootstrap sample from data with replacement.\n",
    "\n",
    "    Args:\n",
    "    data (np.array): The numpy array to sample from.\n",
    "    num_samples (int): The number of samples to draw.\n",
    "\n",
    "    Returns:\n",
    "    np.array: The bootstrap sample as a numpy array.\n",
    "    \"\"\"\n",
    "    bootstrapped_data=[]\n",
    "    for sim in range(num_simulations):\n",
    "        bootstrapped_data.append(np.random.choice(data, size=num_samples))\n",
    "    bootstrapped_data=np.array(bootstrapped_data)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return bootstrapped_data\n",
    "\n",
    "data = load_variables('../contour_integ_behavior/contour_exp1/analysis_data/analysis_data.pkl')\n",
    "\n",
    "all_participants_filename=data['all_participants_filename']\n",
    "all_participants_correct=data['all_participants_correct']\n",
    "all_participants_beta=data['all_participants_beta']\n",
    "\n",
    "print(all_participants_beta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Human Contour Signal at level of individual trials\n",
    "human_img_signal,beta_image,unique_images=get_percent_correct(all_participants_filename,all_participants_correct,all_participants_beta)\n",
    "unique_beta_vals = np.unique(beta_image)\n",
    "\n",
    "print(human_img_signal.shape)\n",
    "print(unique_beta_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Human Beta-wise Accuracy\n",
    "\n",
    "all_participants_unique_beta={}\n",
    "human_unique_beta_acc=[]\n",
    "for b in np.unique(all_participants_beta):\n",
    "    all_participants_unique_beta[b]={}\n",
    "    all_participants_unique_beta[b]['mean']=np.mean(all_participants_correct[np.where(all_participants_beta==b)])\n",
    "    all_participants_unique_beta[b]['std']=np.std(all_participants_correct[np.where(all_participants_beta==b)])\n",
    "    human_unique_beta_acc.append([np.mean(all_participants_correct[np.where(all_participants_beta==b)]),np.std(all_participants_correct[np.where(all_participants_beta==b)])])\n",
    "human_unique_beta_acc=np.array(human_unique_beta_acc)\n",
    "\n",
    "print(all_participants_unique_beta)\n",
    "print(human_unique_beta_acc)\n",
    "\n",
    "### Adding error bars (bootstrapped 95% CI) over each beta condition\n",
    "\n",
    "accuracy_image=[]\n",
    "beta_image=[]\n",
    "bootstrapped_accuracy_image=[]\n",
    "\n",
    "\n",
    "\n",
    "for img in tqdm(unique_images):\n",
    "    accuracy_image.append(np.mean(all_participants_correct[np.where(all_participants_filename==img)]))\n",
    "    beta_image.append(np.mean(all_participants_beta[np.where(all_participants_filename==img)]))\n",
    "    \n",
    "    t=np.mean(bootstrap_sample(all_participants_correct[np.where(all_participants_filename==img)],len(all_participants_correct[np.where(all_participants_filename==img)])),1)\n",
    "    bootstrapped_accuracy_image.append(t)\n",
    "\n",
    "    \n",
    "accuracy_image=np.array(accuracy_image)\n",
    "beta_image=np.array(beta_image)\n",
    "bootstrapped_accuracy_image=np.array(bootstrapped_accuracy_image)\n",
    "\n",
    "print(accuracy_image.shape)\n",
    "print(beta_image.shape)\n",
    "print(bootstrapped_accuracy_image.shape)\n",
    "\n",
    "condition_accuracy_image=np.empty((0,200))\n",
    "condition_bootstrapped_accuracy_image=np.empty((0,200,300))\n",
    "condition_beta=[]\n",
    "\n",
    "for b in np.unique(beta_image):\n",
    "    condition_beta.append(b)\n",
    "    condition_bootstrapped_accuracy_image=np.concatenate((condition_bootstrapped_accuracy_image,np.expand_dims(bootstrapped_accuracy_image[np.where(beta_image==b)[0],:],0)))\n",
    "    condition_accuracy_image=np.concatenate((condition_accuracy_image,np.expand_dims(accuracy_image[np.where(beta_image==b)[0]],0)))\n",
    "\n",
    "print(condition_accuracy_image.shape)\n",
    "print(condition_bootstrapped_accuracy_image.shape)\n",
    "\n",
    "condition_bootstrapped_accuracy_image=np.mean(condition_bootstrapped_accuracy_image,1)\n",
    "condition_accuracy_image=np.mean(condition_accuracy_image,1)\n",
    "\n",
    "print(condition_accuracy_image.shape)\n",
    "print(condition_bootstrapped_accuracy_image.shape)\n",
    "print(condition_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model-Human Alignment and Model-Human Performance as fn of Curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_signal_strength(model,human_img_signal):\n",
    "    contour_present=None\n",
    "    contour_absent=None\n",
    "\n",
    "    ### When contour is present\n",
    "    psychophysics_dataset_norm = Psychophysics_Dataset(root=os.path.expanduser('/home/jovyan/work/Datasets/contour_integration/model-psychophysics/experiment_1/'),transform=data_transform,get_B=[0,15,30,45,60,75],get_D=[32],get_A=[0],get_contour='contour')\n",
    "    psychophysics_loader_norm=torch.utils.data.DataLoader(dataset=psychophysics_dataset_norm, batch_size=15, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    all_betas=[]\n",
    "    all_labels=[]\n",
    "    all_outputs=[]\n",
    "    all_preds=[]\n",
    "    all_recorder_path=[]\n",
    "\n",
    "\n",
    "    for (inputs, b, d, a, nel, labels, record) in psychophysics_loader_norm:\n",
    "        inputs=inputs.to(device)\n",
    "        output=model.forward(inputs).detach().cpu()\n",
    "        _, preds = torch.max(output, 1)\n",
    "\n",
    "        all_preds.append(preds)\n",
    "\n",
    "        all_outputs.append(output)\n",
    "        all_betas.append(b)\n",
    "        all_labels.append(labels)\n",
    "        all_recorder_path.append(np.array(record))\n",
    "\n",
    "    all_betas=torch.cat(all_betas).numpy()\n",
    "    all_labels=torch.cat(all_labels).numpy()\n",
    "    all_preds=torch.cat(all_preds).numpy()\n",
    "    all_outputs=torch.cat(all_outputs).numpy()\n",
    "    all_recorder_path=np.concatenate(all_recorder_path)\n",
    "\n",
    "    contour_present=all_outputs\n",
    "\n",
    "\n",
    "    #####################################################################################################################\n",
    "\n",
    "    ### When contour is absent\n",
    "    psychophysics_dataset_norm = Psychophysics_Dataset(root=os.path.expanduser('/home/jovyan/work/Datasets/contour_integration/model-psychophysics/experiment_1/'),transform=data_transform,get_B=[0,15,30,45,60,75],get_D=[32],get_A=[0],get_contour='control')\n",
    "    psychophysics_loader_norm=torch.utils.data.DataLoader(dataset=psychophysics_dataset_norm, batch_size=15, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    all_betas=[]\n",
    "    all_labels=[]\n",
    "    all_outputs=[]\n",
    "    all_preds=[]\n",
    "    all_recorder_path=[]\n",
    "\n",
    "\n",
    "    for (inputs, b, d, a, nel, labels, record) in psychophysics_loader_norm:\n",
    "        inputs=inputs.to(device)\n",
    "        output=model.forward(inputs).detach().cpu()\n",
    "        _, preds = torch.max(output, 1)\n",
    "\n",
    "        all_preds.append(preds)\n",
    "\n",
    "        # all_outputs.append(softmax_layer(output))\n",
    "        all_outputs.append(output)\n",
    "\n",
    "        all_betas.append(b)\n",
    "        all_labels.append(labels)\n",
    "        all_recorder_path.append(np.array(record))\n",
    "\n",
    "    all_betas=torch.cat(all_betas).numpy()\n",
    "    all_labels=torch.cat(all_labels).numpy()\n",
    "    all_preds=torch.cat(all_preds).numpy()\n",
    "    all_outputs=torch.cat(all_outputs).numpy()\n",
    "    all_recorder_path=np.concatenate(all_recorder_path)\n",
    "\n",
    "    contour_absent=all_outputs\n",
    "    \n",
    "    \n",
    "    model_img_signal=((contour_present - contour_absent)/np.sqrt(2))[:,1]\n",
    "    model_human_corr=np.corrcoef(model_img_signal,human_img_signal)[0][1]\n",
    "    \n",
    "    \n",
    "    return model_human_corr, model_img_signal, contour_present, contour_absent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_beta_acc(model):\n",
    "    psychophysics_dataset_norm = Psychophysics_Dataset(root=os.path.expanduser('/home/jovyan/work/Datasets/contour_integration/model-psychophysics/experiment_1/'),transform=data_transform,get_B=[0,15,30,45,60,75],get_D=[32],get_A=[0],get_contour='all')\n",
    "    psychophysics_loader_norm=torch.utils.data.DataLoader(dataset=psychophysics_dataset_norm, batch_size=15, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    all_betas=[]\n",
    "    all_labels=[]\n",
    "    all_outputs=[]\n",
    "    all_preds=[]\n",
    "    all_recorder_path=[]\n",
    "\n",
    "\n",
    "    for (inputs, b, d, a, nel, labels, record) in tqdm(psychophysics_loader_norm,disable=True):\n",
    "        inputs=inputs.to(device)\n",
    "        output=model.forward(inputs).detach().cpu()\n",
    "        _, preds = torch.max(output, 1)\n",
    "\n",
    "        all_preds.append(preds)\n",
    "\n",
    "        all_outputs.append(output)\n",
    "        all_betas.append(b)\n",
    "        all_labels.append(labels)\n",
    "        all_recorder_path.append(np.array(record))\n",
    "\n",
    "    all_betas=torch.cat(all_betas).numpy()\n",
    "    all_labels=torch.cat(all_labels).numpy()\n",
    "    all_preds=torch.cat(all_preds).numpy()\n",
    "    all_outputs=torch.cat(all_outputs).numpy()\n",
    "    all_recorder_path=np.concatenate(all_recorder_path)\n",
    "\n",
    "    unique_beta_vals=np.unique(all_betas)\n",
    "    model_unique_beta_acc=[]\n",
    "\n",
    "\n",
    "    for i in unique_beta_vals:\n",
    "\n",
    "        acc=np.mean(all_labels[np.where(all_betas==i)[0]] == all_preds[np.where(all_betas==i)[0]])\n",
    "        model_unique_beta_acc.append(acc)\n",
    "    \n",
    "    return model_unique_beta_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fig. 5B, 5C, 5E, 5F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_files =['../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune.pt', \n",
    "                   '../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune_beta_020.pt']\n",
    "\n",
    "model_list_files=sorted(model_list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_human_corr_list=[]\n",
    "model_beta_acc_list=[]\n",
    "model_img_signal_list=[]\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    print(file)\n",
    "    checkpoint=torch.load(file)\n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    \n",
    "    model_human_corr,model_img_signal, _, _ = get_model_signal_strength(loaded_spliced_model,human_img_signal)\n",
    "    model_human_corr_list.append(model_human_corr)\n",
    "    model_beta_acc_list.append(get_model_beta_acc(loaded_spliced_model))\n",
    "    \n",
    "    model_img_signal_list.append(model_img_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_beta_acc_list = np.array(model_beta_acc_list)\n",
    "model_img_signal_list = np.array(model_img_signal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    print(i,'\\t',file.split('/')[-1],' \\t\\t Corr:',model_human_corr_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,18))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(model_img_signal_list[0],human_img_signal,'.',color='red')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title(model_human_corr_list[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(model_img_signal_list[1],human_img_signal,'.',color='green')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title(model_human_corr_list[1])\n",
    "\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.errorbar(np.arange(len(condition_accuracy_image)), condition_accuracy_image, \n",
    "             yerr=[condition_accuracy_image - np.percentile(condition_bootstrapped_accuracy_image, 2.5, axis=1),np.percentile(condition_bootstrapped_accuracy_image, 97.5, axis=1) - condition_accuracy_image], \n",
    "             capsize=5,elinewidth=4,linestyle='-', marker='.', color='gray', markersize=10, \n",
    "             label='Human', alpha=0.8, linewidth=4)\n",
    "plt.plot(np.arange(len(condition_accuracy_image)),model_beta_acc_list[0],linewidth=4,label='finetuned broad',color='r')\n",
    "plt.plot(np.arange(len(condition_accuracy_image)),model_beta_acc_list[0],'.',color='k',markersize=15)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(condition_accuracy_image)),condition_beta,fontsize=20)\n",
    "plt.yticks(fontsize=20) \n",
    "plt.ylim(0.4,1.0)\n",
    "plt.xlabel('Beta Condition',fontsize=20)\n",
    "plt.ylabel('Human Percent Correct/Model Accuracy',fontsize=20)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.errorbar(np.arange(len(condition_accuracy_image)), condition_accuracy_image, \n",
    "             yerr=[condition_accuracy_image - np.percentile(condition_bootstrapped_accuracy_image, 2.5, axis=1),np.percentile(condition_bootstrapped_accuracy_image, 97.5, axis=1) - condition_accuracy_image], \n",
    "             capsize=5,elinewidth=4,linestyle='-', marker='.', color='gray', markersize=10, \n",
    "             label='Human', alpha=0.8, linewidth=4)\n",
    "plt.plot(np.arange(len(condition_accuracy_image)),model_beta_acc_list[1],linewidth=4,label='finetuned narrow',color='green')\n",
    "plt.plot(np.arange(len(condition_accuracy_image)),model_beta_acc_list[1],'.',color='k',markersize=15)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(condition_accuracy_image)),condition_beta,fontsize=20)\n",
    "plt.yticks(fontsize=20) \n",
    "plt.ylim(0.4,1.0)\n",
    "plt.xlabel('Beta Condition',fontsize=20)\n",
    "plt.ylabel('Human Percent Correct/Model Accuracy',fontsize=20)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fig. 5D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_files = glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow/*.pt\")\n",
    "model_list_files=sorted(model_list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model_list_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_human_corr_list=[]\n",
    "model_beta_acc_list=[]\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    print(file)\n",
    "    checkpoint=torch.load(file)\n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    \n",
    "    model_human_corr,_, _, _ = get_model_signal_strength(loaded_spliced_model,human_img_signal)\n",
    "    model_human_corr_list.append(model_human_corr)\n",
    "    model_beta_acc_list.append(get_model_beta_acc(loaded_spliced_model))\n",
    "\n",
    "model_beta_acc_list=np.array(model_beta_acc_list)\n",
    "beta_experience=[]\n",
    "for m in model_list_files:\n",
    "    beta_experience.append(int(m.split('_')[-1][:-3]))\n",
    "\n",
    "print('Highest at',beta_experience[np.argmax(model_human_corr_list)], model_human_corr_list[np.argmax(model_human_corr_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = beta_experience[:8]+beta_experience[9:-4]\n",
    "height=model_human_corr_list[:8]+model_human_corr_list[9:-4]\n",
    "\n",
    "\n",
    "y_pos = np.arange(len(bars)) / 4.5\n",
    "color=['green'] * len(bars)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.fill_between(y_pos,0.7856633080741112 - 0.009460643283334128, 0.7856633080741112 + 0.009460643283334128,color='gray',alpha=0.6)\n",
    "\n",
    "plt.plot(y_pos,height,'-',color='green',linewidth=4)\n",
    "plt.plot(y_pos,height,'.',color='k',markersize=15)\n",
    "\n",
    "\n",
    "max_value = np.argmax(height)\n",
    "plt.plot(y_pos[max_value],height[max_value],'.',color='green',markersize=25)\n",
    "plt.axvline(y_pos[max_value],linestyle='--',alpha=0.5,color='green')\n",
    "\n",
    "\n",
    "for i in y_pos:\n",
    "    plt.axvline(i,linestyle='--',alpha=0.1,color='k')\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(y_pos, bars,fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylabel('Correlation with humans',fontsize=25,labelpad=20)\n",
    "plt.xlabel('Training Curvature',fontsize=25,labelpad=20)\n",
    "# plt.title('Alexnet Avgpool - Constrained Finetuning',fontsize=25)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(True)\n",
    "plt.gca().spines['bottom'].set_visible(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Supp. Fig. 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_files =['../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune.pt']\n",
    "\n",
    "model_list_files=sorted(model_list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_human_corr_list=[]\n",
    "model_beta_acc_list=[]\n",
    "model_img_signal_list=[]\n",
    "\n",
    "model_contour_present_list=[]\n",
    "model_contour_absent_list=[]\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    print(file)\n",
    "    checkpoint=torch.load(file)\n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    \n",
    "    model_human_corr,model_img_signal, contour_present, contour_absent  = get_model_signal_strength(loaded_spliced_model,human_img_signal)\n",
    "    model_human_corr_list.append(model_human_corr)\n",
    "    model_beta_acc_list.append(get_model_beta_acc(loaded_spliced_model))\n",
    "    \n",
    "    model_img_signal_list.append(model_img_signal)\n",
    "    \n",
    "    \n",
    "    model_contour_present_list.append(contour_present)\n",
    "    model_contour_absent_list.append(contour_absent)\n",
    "    \n",
    "\n",
    "model_beta_acc_list = np.array(model_beta_acc_list)\n",
    "model_img_signal_list = np.array(model_img_signal_list)\n",
    "model_contour_present_list = np.array(model_contour_present_list)\n",
    "model_contour_absent_list = np.array(model_contour_absent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "plt.figure(figsize=(24,8))\n",
    "# Create a GridSpec object\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[1, 1.5], height_ratios=[1, 1])\n",
    "# Adjust space between the rows\n",
    "gs.update(wspace=0.3, hspace=0.5)\n",
    "\n",
    "\n",
    "# First subplot: Bottom-Right\n",
    "ax1 = plt.subplot(gs[1, 1])\n",
    "\n",
    "# Second subplot: Top-Right\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "\n",
    "# Third subplot: Top-Left and Bottom-Left\n",
    "ax3 = plt.subplot(gs[:, 0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First subplot: Top-left\n",
    "ax1.bar(range(len(human_img_signal)), human_img_signal,color='gray')\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.set_xlabel('Trials')\n",
    "ax1.set_ylabel('Human Percent Correct')\n",
    "ax1.set_title(' Human Contour Signal at level of individual trials')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Second subplot: Bottom-left\n",
    "ax2.bar(range(len(model_img_signal_list[0])), model_img_signal_list[0],color='gray')\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.set_xlabel('Trials')\n",
    "ax2.set_ylabel('Distance from Diagonal')\n",
    "ax2.set_title(' Model Contour Signal at level of individual trials')\n",
    "\n",
    "\n",
    "\n",
    "ax3.plot(model_contour_present_list[0,:,1],model_contour_absent_list[0,:,1],'.')\n",
    "ax3.plot(np.arange(-14,14),np.arange(-14,14),linestyle='--',color='k')\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "ax3.spines['left'].set_visible(True)\n",
    "ax3.spines['bottom'].set_visible(True)\n",
    "ax3.set_xlabel('Contour Present Image',fontsize=15,labelpad=10)\n",
    "ax3.set_ylabel('Contour Absent Image',fontsize=15,labelpad=10)\n",
    "\n",
    "ax3.set_ylim(-14,14)\n",
    "ax3.set_xlim(-14,14)\n",
    "ax3.set_title('Activity on the contour Present Node')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Supp. Fig. 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_files = glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_broad/*.pt\")\n",
    "model_list_files=sorted(model_list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model_list_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_human_corr_list=[]\n",
    "model_beta_acc_list=[]\n",
    "\n",
    "\n",
    "model_details=[]\n",
    "\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    print(file)\n",
    "    checkpoint=torch.load(file)\n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    \n",
    "    model_human_corr,_, _, _ = get_model_signal_strength(loaded_spliced_model,human_img_signal)\n",
    "    model_human_corr_list.append(model_human_corr)\n",
    "    model_beta_acc_list.append(get_model_beta_acc(loaded_spliced_model))\n",
    "    \n",
    "    \n",
    "    model_details.append([checkpoint['training_config']['base_model_name'], checkpoint['training_config']['layer_name'], '-'.join(map(str, checkpoint['visual_diet_config']['get_B']))])\n",
    "\n",
    "model_beta_acc_list = np.array(model_beta_acc_list)\n",
    "model_details = np.array(model_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "df.append({'model_name':'human', \n",
    "           'model_human_corr': 0.7856633080741112, \n",
    "           'beta_acc_15_30_45_60_75': condition_accuracy_image, \n",
    "           'model_base_model_name': 'human', \n",
    "           'model_layer_name': 'human', \n",
    "           'model_contourtraining_diet': 'human'})\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    \n",
    "    \n",
    "    df.append({'model_name':file, \n",
    "           'model_human_corr': model_human_corr_list[i], \n",
    "           'beta_acc_15_30_45_60_75': model_beta_acc_list[i], \n",
    "           'model_base_model_name': model_details[i][0], \n",
    "           'model_layer_name': model_details[i][1], \n",
    "           'model_contourtraining_diet': model_details[i][2]})\n",
    "df = pd.DataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Supp. Fig. 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_files =['../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune_beta_000.pt',\n",
    "'../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune_beta_015.pt',\n",
    "'../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune_beta_030.pt',\n",
    "'../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune_beta_045.pt',\n",
    "'../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune_beta_060.pt',\n",
    "'../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow/model_alexnet-pytorch-regim-categ_layer_avgpool_mode_finetune_beta_075.pt']\n",
    "\n",
    "model_list_files=sorted(model_list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model_list_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_human_corr_list=[]\n",
    "model_beta_acc_list=[]\n",
    "\n",
    "\n",
    "model_details=[]\n",
    "\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    print(file)\n",
    "    checkpoint=torch.load(file)\n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    \n",
    "    model_human_corr,_, _, _ = get_model_signal_strength(loaded_spliced_model,human_img_signal)\n",
    "    model_human_corr_list.append(model_human_corr)\n",
    "    model_beta_acc_list.append(get_model_beta_acc(loaded_spliced_model))\n",
    "    \n",
    "    \n",
    "    model_details.append([checkpoint['training_config']['base_model_name'], checkpoint['training_config']['layer_name'], '-'.join(map(str, checkpoint['visual_diet_config']['get_B']))])\n",
    "\n",
    "model_beta_acc_list = np.array(model_beta_acc_list)\n",
    "model_details = np.array(model_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "df.append({'model_name':'human', \n",
    "           'model_human_corr': 0.7856633080741112, \n",
    "           'beta_acc_15_30_45_60_75': condition_accuracy_image, \n",
    "           'model_base_model_name': 'human', \n",
    "           'model_layer_name': 'human', \n",
    "           'model_contourtraining_diet': 'human'})\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    \n",
    "    \n",
    "    df.append({'model_name':file, \n",
    "           'model_human_corr': model_human_corr_list[i], \n",
    "           'beta_acc_15_30_45_60_75': model_beta_acc_list[i], \n",
    "           'model_base_model_name': model_details[i][0], \n",
    "           'model_layer_name': model_details[i][1], \n",
    "           'model_contourtraining_diet': model_details[i][2]})\n",
    "df = pd.DataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Supp. Fig. 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_files = glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_0/*.pt\") \\\n",
    "+ glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_1/*.pt\") \\\n",
    "+ glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_2/*.pt\") \\\n",
    "+ glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_3/*.pt\") \\\n",
    "+ glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_4/*.pt\") \\\n",
    "+ glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_5/*.pt\") \\\n",
    "+ glob.glob(\"../model_weights/contour_model_weights/alexnet_regimagenet_categ_finetune_narrow_classifier_6/*.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model_list_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_human_corr_list=[]\n",
    "model_beta_acc_list=[]\n",
    "\n",
    "\n",
    "model_details=[]\n",
    "\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    print(file)\n",
    "    checkpoint=torch.load(file)\n",
    "    loaded_spliced_model=SpliceModel(checkpoint['training_config']['base_model_name'],checkpoint['training_config']['layer_name'],fine_tune=checkpoint['training_config']['fine_tune'],device=device)\n",
    "    loaded_spliced_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    \n",
    "    model_human_corr,_, _, _ = get_model_signal_strength(loaded_spliced_model,human_img_signal)\n",
    "    model_human_corr_list.append(model_human_corr)\n",
    "    model_beta_acc_list.append(get_model_beta_acc(loaded_spliced_model))\n",
    "    \n",
    "    \n",
    "    model_details.append([checkpoint['training_config']['base_model_name'], checkpoint['training_config']['layer_name'], '-'.join(map(str, checkpoint['visual_diet_config']['get_B']))])\n",
    "\n",
    "model_beta_acc_list = np.array(model_beta_acc_list)\n",
    "model_details = np.array(model_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "df.append({'model_name':'human', \n",
    "           'model_human_corr': 0.7856633080741112, \n",
    "           'beta_acc_15_30_45_60_75': condition_accuracy_image, \n",
    "           'model_base_model_name': 'human', \n",
    "           'model_layer_name': 'human', \n",
    "           'model_contourtraining_diet': 'human'})\n",
    "\n",
    "for i,file in tqdm(enumerate(model_list_files)):\n",
    "    \n",
    "    \n",
    "    df.append({'model_name':file, \n",
    "           'model_human_corr': model_human_corr_list[i], \n",
    "           'beta_acc_15_30_45_60_75': model_beta_acc_list[i], \n",
    "           'model_base_model_name': model_details[i][0], \n",
    "           'model_layer_name': model_details[i][1], \n",
    "           'model_contourtraining_diet': model_details[i][2]})\n",
    "df = pd.DataFrame(df)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
